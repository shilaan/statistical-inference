# Exercises

## Likelihood-based tests and confidence regions

Consider a sample $X_1, ..., X_n$ from the Pareto family with unknown parameter $a > 2$ and known $b > 0$. The density function is given by 
$$
\begin{split}
f(x;a) &= \frac{ab^a}{x^{a + 1}}, ~x > b \\
E[X] &= \frac{ab}{a - 1} \\
\text{Var}[X] &= \frac{b^2a}{(a-1)^2(a-2)}
\end{split}
$$

Find the likelihood function, the log likelihood function, the Score function of $a$, and $\hat a$.

$$
\begin{split}
L(a) &= \prod_{i = 1}^n \frac{ab^a}{X_i^{a + 1}} \\
\ell(a) &= \sum_{i = 1}^n \ln a + a\ln b - (a + 1)\ln X_i \\
&= n\ln a + na\ln b   - (a + 1)\sum_{i = 1}^n\ln X_i \\
S_n(a) = \frac{\partial}{\partial a} \ell(a) &=  \frac{n}{a} + n\ln b -\sum_{i = 1}^n\ln X_i = 0 \\
&\Rightarrow \frac{n}{a} = \sum_{i = 1}^n\ln X_i -n\ln b \\
\hat a &=\frac{n}{\sum_{i = 1}^n\ln X_i -n\ln b} \\
&= \frac{n}{\sum_{i = 1}^n \ln(X_i/b)}
\end{split}
$$

\newpage

Calculate the total Fisher information matrix of $a$, compute the asymptotic variance of $\hat a$, and find the asymptotic distribution of $\hat a$.

$$
\begin{split}
S_i(a) &=   \frac{\partial}{\partial a} \ln a + a\ln b - (a + 1)\ln X_i\\
&= \frac{1}{a} + \ln b -\ln X_i \\
I_1(a) &= -E\Bigg[\frac{\partial}{\partial a} S_i(a)\Bigg] \\
&= -E\Bigg[\frac{\partial}{\partial a} \frac{1}{a} + \ln b -\ln X_i\Bigg] \\
&= -E\Big[-a^{-2}  \Big] \\
&=E\Big[\frac{1}{a^2} \Big] \\
&=\frac{1}{a^2} \\
I_T(a) = nI_1(a) &= \frac{n}{a^2} \\
\text{Var}(\hat a) = [I_T(a)]^{-1} &=\frac{a^2}{n}, \text{ the large-sample variance} \\
\text{For large n, }\hat a_n &\sim N(a, \frac{a^2}{n})\\
V(a) = [I_1(a)]^{-1} &={a^2}, \text{ the asymptotic variance} \\
\text{By the asymptotic} &\text{ normality of the MLE, } \\
\sqrt n(\hat a - a) &\xrightarrow D  N(0, [I_1(a)]^{-1}) \text{ as } n \rightarrow \infty \\
\sqrt n(\hat a - a) &\xrightarrow D  N(0, a^2) \text{ as } n \rightarrow \infty \\
\end{split}
$$

\newpage 

Find the MLE of the expected value $\displaystyle \frac{ab}{a - 1}$ and its asymptotic variance. 

$$
\begin{split}
\text{Let } g(a) &=  \frac{ab}{a - 1}\\
&\text{By invariance of the MLE, } \\
\widehat{g(a)} &= g(\hat a) \\
\widehat{E[X]}_{\text{MLE}} &= g(\hat a) \\
&= \frac{\hat ab}{\hat a - 1} \\
&\text{By the asymptotic normality of the MLE, } \\
\sqrt{n}(\hat a - a) &\xrightarrow{D} N(0, a^2) \\
&\text{By the Delta method, } \\
\sqrt{n}\Big(g(\hat a) - g(a)\Big) &\xrightarrow{D} N\Bigg(0, a^2\Big[g'(a) \Big]^2 \Bigg) \\
g(a) &=  \frac{ab}{a - 1}\\
g'(a) &=  \frac{b(a-1) - ab}{(a - 1)^2}\\
&= \frac{-b}{(a - 1)^2} \\
\Big[g'(a) \Big]^2 &= \frac{b^2}{(a - 1)^4} \\
\sqrt{n}\Big(g(\hat a) - g(a)\Big) &\xrightarrow{D} N\Bigg(0, \frac{(ab)^2}{(a - 1)^4} \Bigg) \\
V(\widehat{E[X]}_{\text{MLE}}) &=\frac{(ab)^2}{(a - 1)^4}, \text{ the asymptotic variance} \\
\text{Var}(\widehat{E[X]}_{\text{MLE}}) &=\frac{(ab)^2}{n(a - 1)^4}, \text{ the large-sample variance} \\
\end{split}
$$

\newpage

Find the asymptotic variance of the sample mean as an estimator of the expected value and compute the Asymptotic Relative Efficiency (ARE) of the two estimators. By the CLT,

$$
\begin{split}
\sqrt{n}\Big(\bar X_n - E[X]\Big) &\xrightarrow{D} N\Bigg(0,  \frac{b^2a}{(a-1)^2(a-2)}\Bigg) \\
V(\widehat{E[X]}_{\text{MLE}}) &=\frac{(ab)^2}{(a - 1)^4} \\
V(\bar X_n) &=\frac{b^2a}{(a-1)^2(a-2)}\\
ARE\Big(V(\widehat{E[X]}_{\text{MLE}}), V(\bar X_n)\Big) &= \frac{V(\bar X_n)}{V(\widehat{E[X]}} \\
\text{If } ARE > 1, \text{ then }&\widehat{E[X]}_{\text{MLE}} \text{ is more efficient} \\
\text{If } ARE < 1, \text{ then }&\bar X_n \text{ is more efficient}.\\
ARE\Big(V(\widehat{E[X]}_{\text{MLE}}), V(\bar X_n)\Big) &=  \frac{b^2a}{(a-1)^2(a-2)} \cdot \frac{(a-1)^4}{a^2b^2} \\
&= \frac{(a-1)^2}{a(a-2)} \\
&= \frac{a^2 -2a + 1}{a^2 - 2a} \\
&= 1 + \frac{1}{a(a-2)}
\end{split}
$$

We know that $a > 2$. We can see that for the lowest values of $a$, the $ARE$ is greater than 1 and thus $\widehat{E[X]}_{\text{MLE}}$ is more efficient than $\bar X_n$. $\bar X_n$ will be a relatively inefficient estimator for low values of $a$, e.g. $2 < a \le 4$. However, for larger values of $a$, the $ARE$ converges to 1, and the two estimators will be equally efficient. 

\newpage

Consider testing $H_0: a = a_0$ vs. $H_a: a \ne a_0$. Derive two different versions of the Wald test and their rejection region.

**Approach 1: Using $\hat a$ as a plug-estimator for $I_T(a)$**

$$
\begin{split}
I_T(a) &= \frac{n}{a^2} \\
Z_W &= \sqrt{I_T(\hat a)}(\hat a - a_0) \xrightarrow D N(0,1)  \\
&= \sqrt{\frac{n}{\hat a^2}}(\hat a - a_0) \\
&= \frac{\sqrt n}{\hat a}(\hat a - a_0) \\
&\text{We have obtained the following equivalent rejection regions:} \\
&\text{Reject } H_0 \text{ if } \\
|Z_W| &> \Phi^{-1}(1 -\alpha/2) \\
|Z_W| &> z_{\alpha/2} \\
Z_W &> z_{\alpha/2} \text{ or } Z_W < -z_{\alpha/2} \\
R(a_0) &= \Bigg[\pmb x: \Bigg|\frac{\sqrt n}{\hat a}(\hat a - a_0)\Bigg| > z_{\alpha/2} \Bigg]
\end{split}
$$



**Approach 2: Evaluating $I_T(a)$ at $a_0$**

$$
\begin{split}
I_T(a) &= \frac{n}{a^2} \\
Z'_W &= \sqrt{I_T(a_0)}(\hat a - a_0) \xrightarrow D N(0,1)  \\
&= \sqrt{\frac{n}{a_0^2}}(\hat a - a_0) \\
&= \frac{\sqrt n}{a_0}(\hat a - a_0) \\
&\text{We have obtained the following equivalent rejection regions:} \\
&\text{Reject } H_0 \text{ if } \\
|Z'_W| &> \Phi^{-1}(1 -\alpha/2) \\
|Z'_W| &> z_{\alpha/2} \\
 Z'_W &> z_{\alpha/2} \text{ or } Z'_W < -z_{\alpha/2} \\
R(a_0) &= \Bigg[\pmb x: \Bigg|\frac{\sqrt n}{a_0}(\hat a - a_0)\Bigg| > z_{\alpha/2} \Bigg]
\end{split}
$$

Construct a $(1-\alpha)100$% confidence interval for both estimators. Compute the confidence intervals for $a$ using $\alpha = 0.05$, $n = 100$, and $\hat a = 3.5$. 

**Approach 1: Using $\hat a$ as a plug-estimator for $I_T(a)$**

$$
\begin{split}
R(a_0) &= \Bigg[\pmb x: \Bigg|\frac{\sqrt n}{\hat a}(\hat a - a_0)\Bigg| > z_{\alpha/2} \Bigg] \\
C(\pmb x) &= \Big[a_0:\pmb x \notin R(a_0) \Big] \\
&= \Big[a_0: -z_{\alpha/2}\le Z_W \le z_{\alpha/2} \Big] \\
&= \Big[a_0: -z_{\alpha/2}\le \frac{\sqrt n}{\hat a}(\hat a - a_0) \le z_{\alpha/2} \Big] \\
&=  \Big[a_0: -\frac{\hat a}{\sqrt n}z_{\alpha/2}\le \hat a - a_0 \le \frac{\hat a}{\sqrt n}z_{\alpha/2} \Big] \\ 
&=  \Big[a_0:  -\hat a -\frac{\hat a}{\sqrt n}z_{\alpha/2}\le - a_0 \le  -\hat a +\frac{\hat a}{\sqrt n}z_{\alpha/2} \Big] \\ 
&=  \Big[a_0:  \hat a +\frac{\hat a}{\sqrt n}z_{\alpha/2}\ge a_0 \ge  \hat a -\frac{\hat a}{\sqrt n}z_{\alpha/2} \Big] \\ 
&=  \Big[a_0:  \hat a -\frac{\hat a}{\sqrt n}z_{\alpha/2}\le a_0 \le  \hat a +\frac{\hat a}{\sqrt n}z_{\alpha/2} \Big] \\ 
C(\pmb X) &=  \Big[\hat a -\frac{\hat a}{\sqrt n}z_{\alpha/2},~  \hat a +\frac{\hat a}{\sqrt n}z_{\alpha/2} \Big] \\ 
&= \Big[3.5 -\frac{3.5}{10}z_{\alpha/2},~ 3.5 +\frac{3.5}{10}z_{\alpha/2} \Big] \\
&= \Big[2.81,~4.19 \Big]
\end{split}
$$

\newpage

**Approach 2: Evaluating $I_T(a)$ at $a_0$**

$$
\begin{split}
R(a_0) &= \Bigg[\pmb x: \Bigg|\frac{\sqrt n}{a_0}(\hat a - a_0)\Bigg| > z_{\alpha/2} \Bigg] \\
C(\pmb x) &= \Big[a_0:\pmb x \notin R(a_0) \Big] \\
&= \Big[a_0: -z_{\alpha/2}\le Z'_W \le z_{\alpha/2} \Big] \\
&= \Big[a_0: -z_{\alpha/2}\le \frac{\sqrt n}{a_0}(\hat a - a_0) \le z_{\alpha/2} \Big] \\
&= \Big[a_0: -\frac{1}{\sqrt n}z_{\alpha/2}\le \frac{\hat a - a_0}{a_0} \le \frac{1}{\sqrt n}z_{\alpha/2} \Big] \\
&= \Big[a_0: -\frac{1}{\sqrt n}z_{\alpha/2}\le \frac{\hat a}{a_0} -1 \le \frac{1}{\sqrt n}z_{\alpha/2} \Big] \\
&= \Big[a_0: 1 -\frac{1}{\sqrt n}z_{\alpha/2}\le \frac{\hat a}{a_0} \le 1 + \frac{1}{\sqrt n}z_{\alpha/2} \Big] \\
&= \Big[a_0: \frac{1 -\frac{1}{\sqrt n}z_{\alpha/2}}{\hat a}\le \frac{1}{a_0} \le \frac{1 + \frac{1}{\sqrt n}z_{\alpha/2}}{\hat a} \Big] \\
&= \Big[a_0: \frac{\hat a}{1 -\frac{1}{\sqrt n}z_{\alpha/2}}\ge {a_0} \ge \frac{\hat a}{1 + \frac{1}{\sqrt n}z_{\alpha/2}} \Big] \\
&= \Big[a_0: \frac{\hat a}{1 +\frac{1}{\sqrt n}z_{\alpha/2}}\le {a_0} \le \frac{\hat a}{1 - \frac{1}{\sqrt n}z_{\alpha/2}} \Big] \\
C(\pmb X) &=\Big[\frac{\hat a}{1 +\frac{1}{\sqrt n}z_{\alpha/2}}, ~\frac{\hat a}{1 - \frac{1}{\sqrt n}z_{\alpha/2}} \Big] \\
&= \Big[\frac{3.5}{1 +\frac{1}{10}z_{\alpha/2}}, ~\frac{3.5}{1 - \frac{1}{10}z_{\alpha/2}} \Big] \\
&= \Big[2.93, ~4.35 \Big] \\
\end{split}
$$
\newpage

## M-estimators

Suppose that for each of $n$ independent subjects, the weekly number of alcoholic beverages $X$ is measured, along with two repeated blood pressure measurements $Y_1$ and $Y_2$. Interest lies in a regression analysis of blood pressure $Y$ on weekly number of alcoholic beverages $X$: 

$$
E(Y|X) = \theta_1 + \theta_2X
$$

Suppose that the data analyst ignores that the data from the same subject are dependent. The investigator thus solves the following estimation equations:
$$
0 = \sum_{i = 1}^n \sum_{j = 1}^2 \begin{pmatrix} 1 \\ X_i \end{pmatrix} (Y_{ij} - \theta_1 - \theta_2X_i) 
$$
Show that the obtained estimator is consistent, despite ignoring the dependence in the data.

We have
$$
\begin{split}
0 &= \sum_{i = 1}^n U_i (\hat \theta) \\
\text{Let }  U_i(\theta) &= \sum_{j = 1}^2 \begin{pmatrix} 1 \\ X_i \end{pmatrix} (Y_{ij} - \theta_1 - \theta_2X_i) \\
\text{To show } &\text{consistency, we need to show} \\
E\Big[U_i(\theta) \Big] &= 0 \\
E\Big[U_i(\theta) \Big] &= E\Bigg[ \sum_{j = 1}^2 \begin{pmatrix} 1 \\ X_i \end{pmatrix} (Y_{ij} - \theta_1 - \theta_2X_i)\Bigg] \\&= \sum_{j = 1}^2E\Bigg[  \begin{pmatrix} 1 \\ X_i \end{pmatrix} (Y_{ij} - \theta_1 - \theta_2X_i)\Bigg] \\
\text{Recall that } E\Big[Y_{ij} \Big] &= E\Big[E\Big(Y_{ij} | X_i \Big) \Big]. \text{ Thus, } \\
E\Big[U_i(\theta) \Big] &=\sum_{j = 1}^2E\Bigg[  E\Bigg(\begin{pmatrix} 1 \\ X_i \end{pmatrix} (Y_{ij} - \theta_1 - \theta_2X_i) \Bigg| X_i \Bigg)\Bigg] \\
&= \sum_{j = 1}^2E\Bigg[  \begin{pmatrix} 1 \\ X_i \end{pmatrix} \Big(E\Big[Y_{ij} \Big| X_i \Big] - \theta_1 - \theta_2X_i\Big) \Bigg] \\
&= \sum_{j = 1}^2E\Bigg[  \begin{pmatrix} 1 \\ X_i \end{pmatrix} \Big(\theta_1 + \theta_2X_i - \theta_1 - \theta_2X_i\Big) \Bigg] \\
&= \sum_{j = 1}^2E\Bigg[  \begin{pmatrix} 1 \\ X_i \end{pmatrix} \Big(0\Big) \Bigg] \\
&= 0
\end{split}
$$

What is the asymptotic variance of this estimator? When the data are iid, then the solution $\hat \theta$ to an unbiased estimating equation 
$$
0 = \sum_{i = 1}^n U_i (\hat \theta)
$$

is asymptotically normal with 
$$
\begin{split}
\sqrt n (\hat \theta - \theta) &\xrightarrow D N(0, \Sigma), \text{where} \\
\Sigma &= E\Bigg(\frac{\partial}{\partial \theta} U_i(\theta) \Bigg)^{-1} \text{Var}\Big[ U_i(\theta)  \Big] E\Bigg(\frac{\partial}{\partial \theta} U_i(\theta) \Bigg)^{-1,T} \\
U_i(\theta) &= \sum_{j = 1}^2 \begin{pmatrix} 1 \\ X_i \end{pmatrix} (Y_{ij} - \theta_1 - \theta_2X_i) \\
U_i(\theta_1) &= \sum_{j = 1}^2 Y_{ij} - \theta_1 - \theta_2X_i \\
\frac{\partial}{\partial \theta_1} U_i(\theta_1) &=\sum_{j = 1}^2-1 \\
&= -2 \\
\frac{\partial}{\partial \theta_2} U_i(\theta_1) = \frac{\partial}{\partial \theta_1} U_i(\theta_2) &= \sum_{j = 1}^2 -X_i \\
&=-2X_i \\
U_i(\theta_2) &= \sum_{j = 1}^2 Y_{ij}X_i - \theta_1X_i - \theta_2X_i^2 \\
\frac{\partial}{\partial \theta_2} U_i(\theta_2) &=  \sum_{j = 1}^2 -X_i^2 \\
&= -2X_i^2 \\
\frac{\partial}{\partial\theta} U_i(\theta) &= \begin{bmatrix} -2 & -2X_i \\ -2X_i & - 2X_i^2 \end{bmatrix} \\
&= -2 \begin{bmatrix} 1 & X_i \\ X_i & X_i^2 \end{bmatrix} \\
\Bigg(\frac{\partial}{\partial\theta} U_i(\theta)\Bigg)^{-1} &= -\frac{1}{2}\begin{bmatrix} 1 & X_i \\ X_i & X_i^2 \end{bmatrix}^{-1} \\
\end{split}
$$

$$
\begin{split}
\text{Var}\Big[U_i(\theta) \Big] &= E\Big [U_i(\theta)^2 \Big] - E\Big[U_i(\theta) \Big]^2 \\
&= E\Big [U_i(\theta)^2 \Big] \\
&= E\Big [U_i(\theta)U_i(\theta)^T \Big] \\
&= E\Bigg[\sum_{j = 1}^2 \begin{pmatrix} 1 \\ X_i \end{pmatrix} (Y_{ij} - \theta_1 - \theta_2X_i) \sum_{j = 1}^2 \begin{pmatrix} 1 & X_i \end{pmatrix} (Y_{ij} - \theta_1 - \theta_2X_i)  \Bigg] \\
&= E\Bigg[\Bigg(\sum_{j = 1}^2 (Y_{ij} - \theta_1 - \theta_2X_i) \Bigg)^2 \begin{pmatrix} 1 \\ X_i \end{pmatrix} \begin{pmatrix} 1 & X_i \end{pmatrix} \Bigg] \\
&\text{Recall that } \begin{pmatrix} a_{11} \\ a_{21} \end{pmatrix} \begin{pmatrix} b_{11} & b_{12} \end{pmatrix} =  \begin{pmatrix} a_{11}b_{11} & a_{11}b_{12} \\ a_{21}b_{11} & a_{21}b_{12}  \end{pmatrix}. \text{ Thus, } \\
\text{Var}\Big[U_i(\theta) \Big] &=E\Bigg[\Bigg(\sum_{j = 1}^2 (Y_{ij} - \theta_1 - \theta_2X_i) \Bigg)^2 \begin{pmatrix} a_{11}b_{11} & a_{11}b_{12} \\ a_{21}b_{11} & a_{21}b_{12}  \end{pmatrix} \Bigg] \\
&=E\Bigg[\Bigg(\sum_{j = 1}^2 (Y_{ij} - \theta_1 - \theta_2X_i) \Bigg)^2 \begin{pmatrix} 1 & X_i \\ X_i & X_i^2 \end{pmatrix} \Bigg] \\
\Sigma &= E\Bigg(\frac{\partial}{\partial \theta} U_i(\theta) \Bigg)^{-1} \text{Var}\Big[ U_i(\theta)  \Big] E\Bigg(\frac{\partial}{\partial \theta} U_i(\theta) \Bigg)^{-1,T} \\
&= -\frac{1}{2}E\begin{bmatrix} 1 & X_i \\ X_i & X_i^2 \end{bmatrix} ^{-1}E\Bigg[\Bigg(\sum_{j = 1}^2 (Y_{ij} - \theta_1 - \theta_2X_i) \Bigg)^2 \begin{pmatrix} 1 \\ X_i \end{pmatrix} \begin{pmatrix} 1 & X_i \end{pmatrix} \Bigg] \Bigg( -\frac{1}{2}E\begin{bmatrix} 1 & X_i \\ X_i & X_i^2 \end{bmatrix} ^{-1} \Bigg) \\
&= \frac{1}{4}E\begin{bmatrix} 1 & X_i \\ X_i & X_i^2 \end{bmatrix} ^{-1}E\Bigg[\Bigg(\sum_{j = 1}^2 (Y_{ij} - \theta_1 - \theta_2X_i) \Bigg)^2 \begin{pmatrix} 1 \\ X_i \end{pmatrix} \begin{pmatrix} 1 & X_i \end{pmatrix} \Bigg]  E\begin{bmatrix} 1 & X_i \\ X_i & X_i^2 \end{bmatrix} ^{-1} \\
\end{split}
$$



