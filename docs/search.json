[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Inference",
    "section": "",
    "text": "\\(\\alpha\\) = shape, \\(\\beta\\) = rate, where \\(\\beta = 1/\\theta\\)↩︎\n\\(k\\) = shape, \\(\\theta\\) = scale↩︎"
  },
  {
    "objectID": "week1.html",
    "href": "week1.html",
    "title": "1  Point estimation",
    "section": "",
    "text": "The likelihood function corresponds to the density of all observed data as a function of \\(\\theta\\), evaluated at the effective observations \\(x_1, ..., x_n\\).\n\\[\n\\begin{split}\nL(\\theta|x_1, ..., x_n) &= f_{X_1, X_2, ..., X_n}(x_1, x_2, ..., x_n | \\theta) \\\\\n\\text{Assuming that the data are} &\\text{ independent and have the same distribition function (iid),} \\\\\nf_{X_1, X_2, ..., X_n}(x_1, x_2, ..., x_n | \\theta) &= \\prod_{i = 1}^n f_{X_i}(x_i;\\theta)\n\\end{split}\n\\]\nA sensible choice for \\(\\theta\\) is the one for which the observed data are most likely,\n\\[\n\\hat \\theta = \\text{argmax}_\\theta L(\\theta|x_1, ..., x_n)\n\\]\nBecause maximizing \\(L(\\theta)\\) is not obvious, we often instead maximize the loglikelihood, which (assuming iid) simplifies to\n\\[\n\\begin{split}\n\\ell(\\theta | x_1 , ..., x_n) &= \\ln L(\\theta | x_1 , ..., x_n) \\\\\n&= \\ln \\prod_{i = 1}^n f_{X_i}(x_i;\\theta) \\\\\n&= \\sum_{i = 1}^n \\ln f_{X_i}(x_i;\\theta) \\\\\n\\hat \\theta &= \\text{argmax}_\\theta \\sum_{i = 1}^n \\ln f_{X_i}(x_i;\\theta)\n\\end{split}\n\\]\n\n\n\nHow likely is it to survive 120 days or more? We have\n\\[\n\\begin{split}\nX &\\sim \\text{Exp}(\\theta) \\\\\nf_X(x) &= \\theta\\exp(-\\theta x) \\\\\nF_X(x) &= 1 - \\exp(-\\theta x) \\\\\nP(X \\ge 120) &= 1 - P(X \\le 120) \\\\\n&= 1 - F_X(120)\\\\\n&=1- 1 + \\exp(-120\\theta) \\\\\n&= \\exp(-120\\theta) \\\\\nF_X(x) &= 1 - \\exp(-\\theta x) \\\\\nf_X(x) &= \\frac{d}{dx} F_X(x) \\\\\n&= -\\exp(-\\theta x)\\cdot-\\theta \\\\\n&= \\theta\\exp(-\\theta x) \\\\\nL(\\theta) &= \\prod_{i = 1}^n \\theta\\exp(-\\theta x_i) \\\\\n&= \\theta^n \\prod_{i = 1}^n \\exp(-\\theta x_i) \\\\\n&= \\theta^n \\exp\\Big(\\sum_{i = 1}^n -\\theta x_i\\Big) \\\\\n\\ell(\\theta) &= n\\ln\\theta + \\sum_{i = 1}^n -\\theta x_i \\\\\n&= n\\ln\\theta -\\theta\\sum_{i = 1}^n x_i \\\\\n\\frac{d}{d\\theta}\\ell(\\theta) &= \\frac{n}{\\theta} -\\sum_{i = 1}^n x_i =0 \\\\\n\\frac{n}{\\hat\\theta} &= \\sum_{i = 1}^n x_i \\\\\n\\hat \\theta &= \\frac{n}{\\displaystyle \\sum_{i = 1}^n x_i} = \\frac{1}{\\bar X_n}\n\\end{split}\n\\]\n\nNow, due to the invariance property of the MLE, we can estimate the probability to survive 120 days as \\(\\displaystyle P(Y \\ge 120) = \\exp(-120\\hat \\theta)\\).\n\\[\n\\begin{split}\n\\hat \\theta &= \\frac{1}{\\bar X_n} \\\\\ng(\\theta) &= \\exp(-\\theta y) \\\\\ng(\\theta)_\\text{MLE} &= g(\\hat \\theta) \\\\\n&= \\exp(-\\hat \\theta y)\n\\end{split}\n\\]\n\n\n\n\n\n\nNo. of movements\n0\n1\n2\n3\n4\n5\n6\n7\n\n\n\n\nCounts\n182\n41\n12\n2\n2\n0\n0\n1\n\n\n\nAssuming a Poisson distribution, show that the MLE equals 0.358.\nPlan of action\n\nDetermine the Distribution function \\[f_{X_i}(x_i;\\theta)\\]\nConstruct the Likelihood function \\[L(\\theta|x_1, ..., x_n) =  \\prod_{i = 1}^n f_{X_i}(x_i;\\theta)\\]\nConstruct the Loglikelihood function \\[LL(\\theta|x_1, ..., x_n) = \\ln \\prod_{i = 1}^n f_{X_i}(x_i;\\theta) =   \\sum_{i = 1}^n \\ln f_{X_i}(x_i;\\theta)\\]\nTake the derivative of the Loglikelihood function, and set to zero, to maximize it\n\nWe have\n\\[\n\\begin{split}\nf_{X_i}(x_i;\\lambda) &= e^{-\\lambda}\\frac{\\lambda^{x_i}}{x_i!}  \\\\\nL(\\lambda|x_1, ..., x_n) &=  \\prod_{i = 1}^n f_{X_i}(x_i;\\lambda) \\\\\n&=  \\prod_{i = 1}^n e^{-\\lambda}\\frac{\\lambda^{x_i}}{x_i!} \\\\\n\\ell(\\lambda) &= \\sum_{i = 1}^n -\\lambda + x_i\\ln\\lambda -\\ln(x_i!)\\\\\n\\frac{d}{d\\lambda} \\ell(\\lambda) &= \\sum_{i = 1}^n -1 + \\frac{x_i}{\\lambda} \\\\\n&= -n + \\frac{1}{\\lambda}\\sum_{i = 1}^n x_i =0 \\\\\n&\\Rightarrow  \\frac{1}{\\hat \\lambda}\\sum_{i = 1}^n x_i = n \\\\\n\\hat \\lambda &= \\frac{\\sum_{i = 1}^n x_i}{n} = \\bar X_n \\\\\n&= \\frac{41 + 24 + 6 + 8 + 7}{240} \\\\\n&= \\frac{86}{240} \\approx .358\n\\end{split}\n\\]\n\n\n\n\\[\n\\begin{split}\nf_X(x) &= pI(x = 0) + (1 - p)e^{-\\lambda} \\frac{\\lambda^x}{x!} \\\\\n&=\\begin{cases}\np + (1-p)e^{-\\lambda} & \\text{ for } x = 0 \\\\\n(1-p)e^{-\\lambda}\\frac{\\lambda^x}{x!} & \\text{ for } x > 0\n\\end{cases} \\\\\n&\\text{Combining this expression, we get} \\\\\nf_X(x) &= \\Big(p + (1-p)e^{-\\lambda}\\Big)^{I(x = 0)} \\Big((1-p)e^{-\\lambda}\\frac{\\lambda^x}{x!}\\Big)^{I(x > 0)} \\\\\nL(p, \\lambda) &= \\prod_{i = 1}^n f_{X_i}(x_i) \\\\\n&= \\prod_{i = 1}^n \\Big(p + (1-p)e^{-\\lambda}\\Big)^{I(x_i = 0)} \\Big((1-p)e^{-\\lambda}\\frac{\\lambda^{x_i}}{x_i!}\\Big)^{I(x_i > 0)} \\\\\n\\ell(p, \\lambda) &= \\sum_{i = 1}^n I(x_i=0)\\ln\\Big(p + (1-p)e^{-\\lambda}\\Big) \\\\ &~+ I(x_i > 0)\\Big[\\ln(1 - p) -\\lambda +x_i\\ln\\lambda -\\ln(x_i!) \\Big] \\\\\n\\frac{d}{dp} \\ell(p,\\lambda) &=\\sum_{i = 1}^n \\frac{I(x_i=0)(1 -e^{-\\lambda})}{\\Big(p + (1-p)e^{-\\lambda}\\Big)}\n- \\frac{I(x_i >0)}{(1 - p)} \\\\\n&= \\sum_{i: x_i = 0} \\frac{1 -e^{-\\lambda}}{p + (1-p)e^{-\\lambda}} + \\sum_{i: x_i > 0} \\frac{-1}{(1-p)} \\\\\n\\frac{d}{d\\lambda} \\ell(p,\\lambda) &=\\sum_{i = 1}^n \\frac{I(x_i =0)(1 - p)(-e^{-\\lambda})}{\\Big(p + (1-p)e^{-\\lambda}\\Big)}\n+ I(x_i > 0)\\Big[-1 + \\frac{x_i}{\\lambda}\\Big] \\\\\n&= \\sum_{i: x_i = 0}\\frac{(1 - p)(-e^{-\\lambda})}{p + (1-p)e^{-\\lambda}} + \\sum_{i: x_i > 0} -1 + \\frac{x_i}{\\lambda}\n\\end{split}\n\\]\n\n\n\n\\[\n\\begin{split}\nf_{X_i}(x_i; \\mu,\\sigma^2) &= \\frac{1}{(2\\pi\\sigma^2)^{1/2}} \\exp\\Big[-\\frac{(x_i - \\mu)^ 2}{2\\sigma^ 2}\\Big] \\\\\nL(\\mu, \\sigma^2 | x_1, ..., x_n) &= \\prod_{i = 1}^n f_{X_i}(x_i; \\mu,\\sigma^2) \\\\\n&= \\prod_{i = 1}^n \\frac{1}{(2\\pi\\sigma^2)^{1/2}} \\exp\\Big[-\\frac{(x_i - \\mu)^ 2}{2\\sigma^ 2}\\Big] \\\\\n&= \\frac{1}{(2\\pi\\sigma^2)^{n/2}} \\exp\\Big[\\sum_{i = 1}^n-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\Big] \\\\\n\\ell(\\mu, \\sigma^2 | x_1, ..., x_n) &= \\ln \\Big( (2\\pi\\sigma^2)^{-n/2}  \\exp\\Big[\\sum_{i = 1}^n-\\frac{(x_i - \\mu)^ 2}{2\\sigma^ 2}\\Big] \\Big) \\\\\n&= \\ln\\Big( (2\\pi\\sigma^2)^{-n/2} \\Big) + \\sum_{i = 1}^n-\\frac{(x_i - \\mu)^ 2}{2\\sigma^2} \\\\\n&= -n/2\\ln(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i = 1}^n{(x_i - \\mu)^ 2} \\\\\n\\frac{dl}{d\\mu} &= - \\frac{1}{2\\sigma^2} \\cdot 2 \\cdot -1 \\sum_{i = 1}^n (x_i - \\mu) \\\\\n&= \\frac{1}{\\sigma^2} \\sum_{i = 1}^n (x_i - \\mu) \\\\\n&=  \\frac{1}{\\sigma^2} \\Big[(\\sum_{i = 1}^n x_i) -n\\mu  \\Big] = 0 \\\\\n&\\Rightarrow \\Big(\\sum_{i = 1}^n x_i\\Big) -n\\hat\\mu = 0 \\\\\n&\\Rightarrow n\\hat\\mu = \\sum_{i = 1}^n x_i \\\\\n\\hat \\mu &= \\frac{1}{n} \\sum_{i = 1}^n x_i \\\\\n\\end{split}\n\\]\n\\[\n\\begin{split}\n\\frac{dl}{d\\sigma^2} &= -n/2\\frac{2\\pi}{2\\pi\\sigma^2} + \\frac{1}{2\\sigma^4} \\sum_{i = 1}^n{(x_i - \\mu)^ 2} \\\\\n&= \\frac{-n}{2\\sigma^2} + \\frac{1}{2\\sigma^4} \\sum_{i = 1}^n{(x_i - \\mu)^ 2} = 0 \\\\\n&\\Rightarrow \\frac{n}{2\\sigma^2} = \\frac{1}{2\\sigma^4} \\sum_{i = 1}^n{(x_i - \\mu)^ 2} \\\\\n&\\Rightarrow \\frac{n}{2} = \\frac{1}{2\\sigma^2}  \\sum_{i = 1}^n{(x_i - \\mu)^ 2} \\\\\n\\hat\\sigma^2 &= \\frac{1}{n}  \\sum_{i = 1}^n{(x_i - \\hat \\mu)^ 2}\n\\end{split}\n\\]\n\n\n\nSuppose we have the following days to failure:\n\\[Y = [2, > 72, 51, >60, 33, 27, 14, 24, 4, >21]\\]\nSuppose also that the data follow an exponential distribution with parameter \\(\\lambda\\). We have two groups: uncensored observations (\\(\\delta_i = 1\\)) and censored observations (\\(\\delta_i = 0\\)). For the first group (uncensored), we have \\(Y_i \\sim \\text{Exponential}(\\lambda)\\). For the second group (censored), we have \\(1 - F_{R_i}(R_i) = 1 - (1 - \\exp(-\\lambda R_i))\\). Thus, Y is a random variable with distribution function\n\\[\n\\begin{split}\nf_{Y_i}(y_i; \\lambda)\n&= \\begin{cases}\n\\lambda e^{-\\lambda y_i} & ~~~~~~~~~~~~\\text{ for } \\delta_i = 1 \\\\\ne^{-\\lambda R_i} &  ~~~~~~~~~~~~\\text{ for } \\delta_i = 0 \\text{ and censoring time } R_i\n\\end{cases}\n\\end{split}\n\\]\nCombining this expression, we get\n\\[\n\\begin{split}\nf_{Y_i }(y_i ; \\lambda)&= \\Big(\\lambda e^{-\\lambda y_i} \\Big)^{\\delta_i}\\Big(1 - (1 - e^{-\\lambda R_i})  \\Big)^{1 -\\delta_i} \\\\\n&= \\Big(\\lambda e^{-\\lambda y_i} \\Big)^{\\delta_i}\\Big(e^{-\\lambda R_i}  \\Big)^{1 -\\delta_i} \\\\\nL(\\lambda|y_i,..., y_n) &= \\prod_{i = 1}^n f_{Y_i}(y_i ; \\lambda) \\\\\n&= \\prod_{i = 1}^n \\Big(\\lambda e^{-\\lambda y_i} \\Big)^{\\delta_i}\\Big(e^{-\\lambda R_i}  \\Big)^{1 -\\delta_i} \\\\\n&= \\prod_{i = 1}^n \\lambda^{\\delta_i} \\exp\\Big({-\\lambda y_i\\delta_i}\\Big)\\exp\\Big({-\\lambda R_i(1-\\delta_i)}\\Big) \\\\\n&= \\prod_{i = 1}^n \\lambda^{\\delta_i}\\exp\\Big({-\\lambda(y_i\\delta_i + R_i(1 - \\delta_i)}\\Big) \\\\\n\\ell(\\lambda) &= \\sum_{i = 1}^n \\delta_i\\ln\\lambda -\\lambda\\Big(y_i\\delta_i + R_i(1 - \\delta_i)\\Big) \\\\\n\\frac{d}{d\\lambda}\\ell(\\lambda) &= \\sum_{i = 1}^n \\frac{\\delta_i}{\\lambda} -\\Big(y_i\\delta_i + R_i(1 - \\delta_i)\\Big) \\\\\n&= \\sum_{i = 1}^n \\frac{\\delta_i}{\\lambda} -y_i\\delta_i - R_i(1 - \\delta_i) = 0 \\\\\n\\sum_{i = 1}^n \\frac{\\delta_i}{\\lambda} &= \\sum_{i = 1}^n y_i\\delta_i + R_i(1 - \\delta_i) \\\\\n\\frac{1}{\\lambda} \\sum_{i = 1}^n \\delta_i &= \\sum_{i = 1}^n y_i\\delta_i + R_i(1 - \\delta_i) \\\\\n\\hat \\lambda &= \\frac{\\sum_{i = 1}^n\\delta_i}{\\sum_{i = 1}^ny_i\\delta_i + R_i(1 - \\delta_i)} \\\\\n&= \\frac{7}{308} \\approx 0.0227\n\\end{split}\n\\]\nThus, the maximum likelihood estimate for the censored data equals the number of uncensored observations divided by the sum of all \\(Y_i\\) (for \\(\\delta_i = 1\\)) and all censoring times \\(R_i\\) (for \\(\\delta_i = 0\\)).\n\n\nyi <- c(2, 51, 33, 27, 14, 24, 4)\nri <- c(72, 60, 21)\n\n#analytical solution\nlambda_hat <- length(yi)/sum(c(yi,ri))\nlambda_hat\n\n[1] 0.02272727\n\n#numerical solution\nnegloglik <- function(lambda){\n  -(sum(log(lambda) - lambda*yi) + sum(-lambda*ri))\n}\n\nnlm(negloglik, p = .03)$estimate\n\n[1] 0.0227273\n\n\n\n\n\n\nLet \\(X\\) be a continuous random variable with distribution function \\(F_X(x)\\) and density function \\(f_X(x)\\). Consider the following random variable \\(Y\\), and obtain its CDF and PDF\n\\[\n\\begin{split}\nY &= \\begin{cases}\nX & \\text{if } X < a \\\\\na & \\text{if } X \\ge a\n\\end{cases} \\\\\n\\end{split}\n\\]\nThis is a right-censored version of \\(X\\) at \\(x = a\\). We have\n\\[\n\\begin{split}\n\\text{For } X &< a, \\text{ or, equivalently, } Y < a, \\\\\nF_Y(y) &= P(Y \\le y) \\\\&= P(X \\le y) \\\\&= F_X(y) \\\\\nf_Y(y) &= f_X(x) \\\\&= f_X(y) \\\\\n\\text{For } X &\\ge a , \\text{ or, equivalently, } Y = a, \\\\\nF_Y(y) &= F_Y(a) \\\\ &= P(Y \\le a) \\\\&= 1 \\\\\nf_Y(y) &= P(Y = a) \\\\\n&= 1 - P(Y < a) \\\\\n&= 1 - P(X < a ) \\\\\n&= 1 - P(X \\le a ) \\\\\n&= 1 - F_X(a). \\text{ Thus, } \\\\\nf_Y(y) &= \\begin{cases}\nf_X(y) & \\text{if } y < a \\\\\n1 - F_X(a) & \\text{if } y = a\n\\end{cases} \\\\\n&= f_X(y)^{I(y < a)}\\Big[1 - F_X(a)\\Big]^{I(y = a)}\n\\\\\nF_Y(y)&= \\begin{cases}\nF_X(y) & ~~~~~~~\\text{if } y < a \\\\\n1 & ~~~~~~~\\text{if } y = a \\\\\n\\end{cases} \\\\\n&= F_X(y)^{I(y < a)} 1^{I(y = a)}\n\\end{split}\n\\]\n\nSuppose we have iid data \\(X_1, ..., X_n\\), with \\(X_i \\sim N(\\mu, 1)\\). Derive equations for the MLE \\(\\hat\\mu\\) of \\(\\mu\\) based on the censored data \\(Y_1, ..., Y_n\\). We have\n\\[\n\\begin{split}\nf_{Y_i}(y_i) &= f_{X_i}(y_i)^{I(y_i < a)}\\Big[1 - F_{X_i}(a)\\Big]^{I(y_i = a)} \\\\\nf_{X_i}(x_i) &=\\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\Bigg(-\\dfrac{(x_i - \\mu)^2}{2\\sigma^2}\\Bigg) \\\\\n&= \\frac{1}{\\sqrt{2\\pi}}\\exp\\Big[-\\frac{1}{2}(x - \\mu)^2\\Big] \\\\\nF_{X_i}(x_i) &=\\Phi\\Big(\\frac{x_i - \\mu}{\\sigma}\\Big)\\\\\n&= \\Phi(x_i - \\mu) \\\\\nL(\\mu) &= \\prod_{i = 1}^n f_{X_i}(y_i)^{I(y_i < a)}\\Big[1 - F_{X_i}(a)\\Big]^{I(y_i = a)} \\\\\n&=\\prod_{i = 1}^n \\Bigg(\\frac{1}{\\sqrt{2\\pi}}\\exp\\Big[-\\frac{1}{2}(y_i - \\mu)^2\\Big]\\Bigg)^{I(y_i < a)}\\Big[1 - \\Phi(a - \\mu)\\Big]^{I(y_i = a)} \\\\\n&\\text{Because } 1 - \\Phi(x) \\equiv \\Phi(-x), \\\\\n&=\\prod_{i = 1}^n \\Bigg(\\frac{1}{\\sqrt{2\\pi}}\\exp\\Big[-\\frac{1}{2}(y_i - \\mu)^2\\Big]\\Bigg)^{I(y_i < a)}\\Big[\\Phi(\\mu - a)\\Big]^{I(y_i = a)} \\\\\n\\ell(\\mu) &= \\sum_{i = 1}^n I(y_i < a) \\ln \\Bigg(\\frac{1}{\\sqrt{2\\pi}}\\exp\\Big[-\\frac{1}{2}(y_i - \\mu)^2\\Big] \\Bigg) + I(y_i = a)\\ln\\Big[\\Phi(\\mu - a)\\Big] \\\\\n&=\\sum_{i = 1}^n I(y_i < a)\\Bigg(\\ln\\Big[ \\frac{1}{\\sqrt{2\\pi}} \\Big] - \\frac{1}{2}(y_i - \\mu)^2 \\Bigg)+ I(y_i = a)\\ln\\Big[\\Phi(\\mu - a)\\Big] \\\\\n\\frac{d}{d\\mu} \\ell(\\mu) &=\\sum_{i = 1}^n I(y_i < a) (y_i - \\mu) + I(y_i = a) \\frac{f_X(\\mu - a)}{\\Phi(\\mu - a)}\n\\end{split}\n\\]\nNumerically calculate the MLe for a sample of size \\(n = 100\\) with \\(\\mu = 0\\) and \\(a = 1\\).\n\nxi <- rnorm(100)\nmean(xi) #the MLE based on the full data\n\n[1] 0.009724927\n\na <- 1\nyi <- ifelse(xi < a, xi, a)\n\n#numerical solution\nnegloglik <- function(mu){\n  #Negative log likelihood expression\n  -sum(I(yi < a)*(-0.5*(yi - mu)^2) + I(xi >= a)*log(pnorm(mu - a)))\n}\n\nnlm(negloglik, p = -.03)$estimate\n\n[1] 0.0205651\n\n\n\n\n\n\nSuppose we collect 2 measurements \\(Y_{i1}\\) and \\(Y_{i2}\\) for each of \\(n\\) subjects, with iid \\(Y_{ij}, i = 1, ..., n, j = 1,2, Y_{ij} \\sim N(\\mu_i, \\sigma^2)\\) (i.e., same variance \\(\\sigma^2\\) but individual-specific means \\(\\mu_i\\)). We are interested in estimating \\(\\sigma^2\\).\n\\[\n\\begin{split}\nf_{Y_{ij}}(y_{ij}) &=\\begin{cases}\nN(\\mu_i, \\sigma^2) & \\text{ for } j = 1 \\\\\nN(\\mu_i, \\sigma^2) & \\text{ for } j = 2\n\\end{cases} \\\\\n&= \\Big(\\frac{1}{(2\\pi\\sigma^2)^{1/2}} \\exp\\Big[-\\frac{(y_{ij} - \\mu_i)^ 2}{2\\sigma^ 2}\\Big]\\Big)^{I(j = 1)} \\\\\n&~~~~~\\Big(\\frac{1}{(2\\pi\\sigma^2)^{1/2}} \\exp\\Big[-\\frac{(y_{ij} - \\mu_i)^ 2}{2\\sigma^ 2}\\Big]\\Big)^{I(j = 2)} \\\\\n& = \\prod_{j = 1}^2 \\frac{1}{(2\\pi\\sigma^2)^{1/2}} \\exp\\Big[-\\frac{(y_{ij} - \\mu_i)^ 2}{2\\sigma^ 2}\\Big] \\\\\n&= \\frac{1}{2\\pi\\sigma^2} \\prod_{j = 1}^2 \\exp\\Big[-\\frac{(y_{ij} - \\mu_i)^ 2}{2\\sigma^ 2}\\Big] \\\\\n&= \\frac{1}{2\\pi\\sigma^2}\\exp\\Big[ \\sum_{j = 1}^2-\\frac{(y_{ij} - \\mu_i)^ 2}{2\\sigma^ 2}\\Big] \\\\\n&= \\frac{1}{2\\pi\\sigma^2}\\exp\\Big[- \\frac{1}{2\\sigma^ 2} \\sum_{j = 1}^2  {(y_{ij} - \\mu_i)^ 2}\\Big] \\\\\n&= \\frac{1}{2\\pi\\sigma^2}\\exp\\Big[- \\frac{1}{2\\sigma^ 2} \\Big({(y_{i1} - \\mu_i)^ 2}  + {(y_{i2} - \\mu_i)^ 2}\\Big)  \\Big] \\\\\nL(\\mu_i, \\sigma^2) &= \\prod_{i = 1}^n \\frac{1}{2\\pi\\sigma^2} \\exp\\Big[- \\frac{1}{2\\sigma^ 2} \\Big({(y_{i1} - \\mu_i)^ 2}  + {(y_{i2} - \\mu_i)^ 2}\\Big) \\Big] \\\\\n&= \\frac{1}{(2\\pi\\sigma^2)^n} \\exp\\Big[\\sum_{i = 1}^n- \\frac{1}{2\\sigma^ 2} \\Big({(y_{i1} - \\mu_i)^ 2}  + {(y_{i2} - \\mu_i)^ 2}\\Big)  \\Big] \\\\\n&= \\frac{1}{(2\\pi\\sigma^2)^n} \\exp\\Big[- \\frac{1}{2\\sigma^ 2}\\sum_{i = 1}^n {(y_{i1} - \\mu_i)^ 2}  + {(y_{i2} - \\mu_i)^ 2} \\Big] \\\\\nl (\\mu_i, \\sigma^2)&= \\ln\\Big((2\\pi\\sigma^2)^{-n}\\Big) - \\frac{1}{2\\sigma^ 2}\\sum_{i = 1}^n {(y_{i1} - \\mu_i)^ 2}  + {(y_{i2} - \\mu_i)^ 2} \\\\\n&= -n\\ln(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^ 2}\\sum_{i = 1}^n {(y_{i1} - \\mu_i)^ 2}  + {(y_{i2} - \\mu_i)^ 2} \\\\\n\\end{split}\n\\]\n\\[\n\\begin{split}\n&\\text{For a single individual, we have} \\\\\nl (\\mu_i, \\sigma^2)&= -n\\ln(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^ 2} \\Big( {(y_{i1} - \\mu_i)^ 2}  + {(y_{i2} - \\mu_i)^ 2} \\Big) \\\\\n\\frac{d}{d\\mu_i} l (\\mu_i, \\sigma)&= - \\frac{1}{2\\sigma^ 2}\\Big( -2{(y_{i1} - \\mu_i)}  - {2(y_{i2} - \\mu_i)} \\Big) \\\\\n&= \\frac{-\\Big( -2{(y_{i1} - \\mu_i)}  - {2(y_{i2} - \\mu_i)} \\Big)}{2\\sigma^ 2} \\\\\n&= \\frac{2(y_{i1} - \\mu_i) + 2(y_{i2} - \\mu_i)}{2\\sigma^ 2} \\\\\n&= \\frac{(y_{i1} - \\mu_i) + (y_{i2} - \\mu_i)}{\\sigma^ 2} \\\\\n&= \\frac{y_{i1} + y_{i2} - 2\\mu_i}{\\sigma^2} = 0 \\\\\n&\\Rightarrow 2\\mu_i = y_{i1} + y_{i2} \\\\\n\\hat{\\mu_i} &= \\frac{y_{i1} + y_{i2}}{2}\n\\end{split}\n\\]\nTo estimate \\(\\sigma^2\\), we plug in \\(\\hat \\mu_i\\):\n\\[\n\\begin{split}\n\\ell(\\mu_i, \\sigma^2)&= -n\\ln(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^ 2}\\sum_{i = 1}^n {(y_{i1} - \\mu_i)^ 2}  + {(y_{i2} - \\mu_i)^ 2} \\\\\n\\frac{d}{d\\sigma^2} \\ell(\\mu_i, \\sigma^2)&= \\frac{-n2\\pi}{2\\pi\\sigma^2} + \\frac{1}{2\\sigma^4}\\sum_{i = 1}^n {(y_{i1} - \\mu_i)^ 2}  + {(y_{i2} - \\mu_i)^ 2} \\\\\n&\\Rightarrow \\frac{1}{2\\sigma^4} \\sum_{i = 1}^n {(y_{i1} - \\mu_i)^ 2}  + {(y_{i2} - \\mu_i)^ 2} =\\frac{n}{\\sigma^2} \\\\\n&\\Rightarrow\\sum_{i = 1}^n {(y_{i1} - \\mu_i)^ 2}  + {(y_{i2} - \\mu_i)^ 2} =\\frac{2n\\sigma^4}{\\sigma^2} \\\\\n\\hat \\sigma^2 &= \\frac{1}{2n} \\sum_{i = 1}^n {(y_{i1} - \\mu_i)^ 2}  + {(y_{i2} - \\mu_i)^ 2} \\\\\n&=  \\frac{1}{2n} \\sum_{i = 1}^n {\\Big(\\frac{y_{i1} - y_{i2}}{2}\\Big)^ 2} + {\\Big(\\frac{y_{i1} - y_{i2}}{2}\\Big)^ 2} \\\\\n&= \\frac{1}{2n}\\sum_{i = 1}^n 2{\\Big(\\frac{y_{i1} - y_{i2}}{2}\\Big)^ 2} \\\\\n&= \\frac{1}{n}\\sum_{i = 1}^n{\\Big(\\frac{1}{2}({y_{i1} - y_{i2}})\\Big)^ 2} \\\\\n&= \\frac{1}{n}\\sum_{i = 1}^n\\frac{1}{4} \\Big({y_{i1} - y_{i2}}\\Big)^ 2 \\\\\n&= \\frac{1}{4n}\\sum_{i = 1}^n{\\Big({y_{i1} - y_{i2}}\\Big)^ 2}\n\\end{split}\n\\]\n\n\\(\\hat \\sigma^2\\) is a biased estimator for \\(\\sigma^2\\). We have\n\\[\n\\begin{split}\nE\\Big[\\hat \\sigma^2\\Big] &=E \\Bigg[\\frac{1}{4n}\\sum_{i = 1}^n\\Big({Y_{i1} - Y_{i2}}\\Big)^ 2 \\Bigg] \\\\\n&= \\frac{1}{4n} E \\Bigg[\\sum_{i = 1}^n\\Big({Y_{i1} - Y_{i2}}\\Big)^ 2 \\Bigg] \\\\\n&=\\frac{1}{4n} \\sum_{i = 1}^nE \\Bigg[\\Big({Y_{i1} - Y_{i2}}\\Big)^ 2 \\Bigg] \\\\\n\\text{Let } Z &= Y_{i1} - Y_{12}. \\text{ Then, }\\\\\nE[Z] &= E\\Big[Y_{i1} - Y_{12}\\Big] \\\\\n&= E[Y_{i1}] - E[Y_{i2}] \\\\\n&= \\mu_i - \\mu_i = 0. \\\\\n\\text{Var}[Z] &=  E[Z^2] - \\Big(E[Z]\\Big)^2 \\\\\n&= E[Z^2] \\\\\n&= E\\Big[(Y_{i1} - Y_{i2})^2\\Big] \\\\\n&= \\text{Var}\\Big[Y_{i1} - Y_{i2}\\Big] \\\\\n\\text{Because } Y_{i1} &\\perp \\!\\!\\! \\perp Y_{i2}, \\\\\n\\text{Var}\\Big[Y_{i1} - Y_{i2}\\Big] &= \\text{Var}[Y_{i1}] +  \\text{Var}[- Y_{i2}] \\\\\n&= \\text{Var}[Y_{i1}] +  \\text{Var}[Y_{i2}] \\\\\n&= 2\\sigma^2. \\text{ Thus, } \\\\\nE\\Big[\\hat \\sigma^2\\Big] &= \\frac{1}{4n} \\sum_{i = 1}^nE \\Bigg[\\Big({Y_{i1} - Y_{i2}}\\Big)^ 2 \\Bigg] \\\\\n&= \\frac{1}{4n} \\sum_{i = 1}^n 2\\sigma^2 \\\\\n&= \\frac{2n\\sigma^2}{4n} \\\\\n&= \\frac{\\sigma^2}{2}\n\\end{split}\n\\]\nThe bias remains even if \\(n\\) tends to infinity: the MLE of \\(\\sigma^2\\) is inconsistent. Solution: transform the data, such that their distribution is independent of the nuisance parameters. The likelihood of these transformed data is called a marginal likelihood, because it averages away some of the information in the data. It results in a marginal or restricted MLE.\n\\[\n\\begin{split}\n\\text{Let } V_i &= \\frac{Y_{i1}  - Y_{i2}}{\\sqrt 2}. \\text{ Then, } \\\\\nE[V_i] &= E\\Bigg[\\frac{Y_{i1}  - Y_{i2}}{\\sqrt 2}\\Bigg] \\\\\n&= \\frac{1}{\\sqrt 2} E\\Big[Y_{i1}  - Y_{i2}\\Big] = 0. \\\\\n\\text{Var}[V_i] &= E[V_i^2] - \\Big(E[V_i]\\Big)^2 \\\\\n&=E[V_i^2] \\\\\n&= E\\Bigg[\\Bigg(\\frac{Y_{i1}  - Y_{i2}}{\\sqrt 2}\\Bigg)^2\\Bigg] \\\\\n&= E\\Bigg[\\frac{(Y_{i1}  - Y_{i2})^2}{2}\\Bigg] \\\\\n&= \\frac{1}{2} E \\Big[({Y_{i1} - Y_{i2}})^ 2 \\Big] \\\\\n&= \\frac{2\\sigma^2}{2} \\\\\n&= \\sigma^2. \\\\\n\\text{Thus, } V_i &\\sim N(0, \\sigma^2) \\\\\n\\hat \\sigma^2 &= \\frac{1}{n} \\sum_{i = 1}^n \\Big(V_i - E[V_i]\\Big)^2 \\\\\n&= \\frac{1}{n} \\sum_{i = 1}^n V_i^2 \\\\\n&= \\frac{1}{n} \\sum_{i = 1}^n \\frac{(Y_{i1} - Y_{i2})^2}{2} \\\\\n&= \\frac{1}{2n} \\sum_{i = 1}^n (Y_{i1} - Y_{i2})^2 \\\\\nE[\\hat \\sigma^2] &= E\\Bigg[\\frac{1}{n} \\sum_{i = 1}^n V_i^2\\Bigg] \\\\\n&=  \\frac{1}{n} \\sum_{i = 1}^n E\\Big[V_i^2\\Big] \\\\\n&=  \\frac{1}{n} \\sum_{i = 1}^n \\sigma^2 \\\\\n&= \\sigma^2.\n\\end{split}\n\\]\n\nn <- 200; mu <- rnorm(n, mean = 0, sd = 1)\ny1 <- rnorm(n, mean = mu, sd = 1); y2 <- rnorm(n, mean = mu, sd = 1)\nsum((y1 - y2)^2)/(4*n) #biased mle of sigma^2\n\n[1] 0.4419836\n\nsum((y1 - y2)^2)/(2*n)  #restricted/marginal mle of sigma^2\n\n[1] 0.8839672"
  },
  {
    "objectID": "week1.html#distributions-of-transformed-data",
    "href": "week1.html#distributions-of-transformed-data",
    "title": "1  Point estimation",
    "section": "1.2 Distributions of transformed data",
    "text": "1.2 Distributions of transformed data\nLet X be a random variable with a known distribution and Y a function of X. Then, \\(Y = h(X)\\), \\(X = h^{-1}(Y)\\), and we can use the chain rule to obtain the PDF of \\(Y\\).\nFor \\(h(X)\\) strictly increasing, we have\n\\[\\displaystyle f_Y(y) = f_X\\Big(h^{-1}(y)\\Big) \\frac{d}{dy} \\Big[h^{-1}(y)\\Big]\\]\nDerivation\n\\[\n\\begin{split}\nF_Y(y) &= P\\Big(h(X) \\le y\\Big) \\\\\n&= P\\Big(X \\le h^{-1}(y)\\Big) \\\\\n&= F_X\\Big(h^{-1}(y) \\Big) \\\\\nf_Y(y) &= \\frac{d}{dy} F_X\\Big(h^{-1}(y) \\Big) \\\\\n&= f_X\\Big(h^{-1}(y) \\Big) \\frac{d}{dy} \\Big[h^{-1}(y)  \\Big] \\\\\n\\end{split}\n\\]\nFor example, let \\(X \\sim N(\\mu, \\sigma^2)\\) and \\(Y = e^X\\). Then,\n\\[\n\\begin{split}\nf_X(x) &= \\dfrac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\Bigg(-\\dfrac{(x - \\mu)^2}{2\\sigma^2}\\Bigg)\\\\\nY &= h(X) = e^X \\\\\nX &= h^{-1}(Y) = \\ln Y \\\\\nf_Y(y) &= f_X\\Big(h^{-1}(y)\\Big)  \\frac{d}{dy} \\Big[ h^{-1}(y) \\Big] \\\\\n&= f_X(\\ln y)  \\frac{d}{dy} \\Big[ \\ln y \\Big]\\\\\n&= f_X(\\ln y) \\cdot \\frac{1}{y} \\\\\n&= \\dfrac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\Bigg(-\\dfrac{(\\ln y - \\mu)^2}{2\\sigma^2}\\Bigg) \\cdot \\frac{1}{y} \\\\\n&= \\dfrac{1}{y\\sigma\\sqrt{2\\pi}}\\exp\\Bigg(-\\dfrac{(\\ln y  - \\mu)^2}{2\\sigma^2}\\Bigg) \\\\\n&\\text{This is } \\text{the lognormal distribution:} \\\\\n&Y \\sim \\text{LN}(\\mu, \\sigma^2)\n\\end{split}\n\\]\nTo obtain \\(h^{-1}(y)\\):\n\\[\n\\begin{split}\nF_Y(y) &= P(Y \\le y) \\\\\n&= P(e^X \\le y) \\\\\n&= P(X \\le \\ln y) \\\\\n&= F_X(\\ln y) \\\\\nh^{-1}(y) &= \\ln y\n\\end{split}\n\\]\n\nFor \\(h(X)\\) strictly decreasing, we note that if a continuous function is increasing on its interval, then its inverse is also increasing and continuous. Similarly, if a continuous function \\(h(X)\\) is decreasing on its interval, then its inverse \\(h^{-1}(Y)\\) is also decreasing and continuous (and, consequently, the derivative of both \\(h(X)\\) and \\(h^{-1}(Y)\\) will be negative). We have:\n\\[\n\\begin{split}\nf_Y(y) &= -f_X\\Big(h^{-1}(y)\\Big) \\frac{d}{dy} \\Big[ h^{-1}(y) \\Big] \\\\\n&= f_X\\Big(h^{-1}(y) \\Big)   \\Bigg| \\frac{d}{dy} \\Big[h^{-1}(y)  \\Big]\\Bigg|\n\\end{split}\n\\]\nDerivation\n\\[\n\\begin{split}\nF_Y(y) &= P(Y \\le y) \\\\\n&=P\\Big(h(X) \\le y\\Big) \\\\\n&= P\\Big(X \\ge h^{-1}(y)\\Big) \\\\\n&= 1 - P\\Big(X \\le h^{-1}(y)\\Big) \\\\\n&= 1 - F_X\\Big(h^{-1}(y) \\Big) \\\\\nf_Y(y) &= \\frac{d}{dy} \\Bigg[1 - F_X\\Big(h^{-1}(y) \\Big) \\Bigg] \\\\\n&= -\\frac{d}{dy} F_X\\Big(h^{-1}(y) \\Big) \\\\\n&= -f_X\\Big(h^{-1}(y) \\Big) \\cdot \\frac{d}{dy} \\Big[h^{-1}(y)  \\Big] \\\\\n\\text{Because } h(X) &\\text{ is strictly decreasing, } h^{-1}(y) \\text{ is also decreasing and thus } \\frac{d}{dy} \\Big[h^{-1}(y) \\Big]  < 0. \\\\\n\\Rightarrow f_Y(y) &=  f_X\\Big(h^{-1}(y) \\Big) \\cdot -\\frac{d}{dy} \\Big[h^{-1}(y)  \\Big] \\\\\n&= f_X\\Big(h^{-1}(y) \\Big) \\Bigg| \\frac{d}{dy} \\Big[h^{-1}(y)  \\Big] \\Bigg| \\\\\n\\end{split}\n\\]\nFor example, let \\(Y = h(X) = -e^X\\). Then,\n\\[\n\\begin{split}\nY &= -e^X = h(X) \\\\\n-Y &=e^X \\\\\nX &= \\ln(-Y) = h^{-1}(Y) \\\\\nf_Y(y) &= f_X\\Big(h^{-1}(y) \\Big)   \\Bigg| \\frac{d}{dy} \\Big[h^{-1}(y)  \\Big]\\Bigg| \\\\\n&= f_X\\Big(\\ln(-y) \\Big)   \\Bigg| \\frac{d}{dy} \\Big[\\ln(-y)  \\Big]\\Bigg| \\\\\n&= f_X\\Big(\\ln(-y) \\Big) \\Bigg| \\frac{-1}{-y} \\Bigg| \\\\\n&= \\Bigg|\\frac{1}{y} \\Bigg|f_X\\Big(\\ln(-y)\\Big) \\\\\n\\end{split}\n\\]"
  },
  {
    "objectID": "week1.html#score-vector",
    "href": "week1.html#score-vector",
    "title": "1  Point estimation",
    "section": "1.3 Score vector",
    "text": "1.3 Score vector\nWe have the loglikelihood based on data for a single subject \\(X_i\\):\n\\[\\ell_i(\\theta) = \\ln f_X(X_i;\\theta) = \\ln L_i(\\theta)\\]\nThe score vector is the derivative of the loglikelihood based on data for a single subject \\(X_i\\):\n\\[S_i(\\theta) \\equiv \\frac{\\partial \\ell_i(\\theta)}{\\partial \\theta} = \\frac{\\partial \\ln L_i(\\theta)}{\\partial \\theta}\\]\nThe MLE is obtained by solving \\(\\displaystyle \\frac{1}{n} \\sum_{i = 1}^n S_i(\\theta) = 0\\).\nTo obtain the MLE from the score vector, follow these steps:\n\\[\n\\begin{split}\n\\text{ For a } &\\text{single subject } X_i, \\\\\nL_i(\\theta | X_i) &= f_X(X_i; \\theta) \\\\\n\\ell_i(\\theta) &= \\ln f_X(X_i; \\theta) \\\\\nS_i(\\theta) &= \\frac{\\partial \\ell_i(\\theta)}{\\partial \\theta} \\\\\n\\text{To obtain} &\\text{ the MLE,  solve} \\\\\n\\frac{1}{n} \\sum_{i = 1}^n S_i(\\theta) &= 0 \\\\\n\\end{split}\n\\]\nA key property of the score statistic is, for any \\(\\theta\\), and assuming regularity conditions that allow interchanging the derivative and integral:\n\\[\nE_\\theta \\Big[S_i(\\theta)\\Big] = 0\n\\]\nThus, the solution \\(\\hat \\theta\\) to \\(\\displaystyle\\frac{1}{n} \\sum_{i = 1}^n S_i(\\theta) = 0\\) will be close to the population value, which solves \\(\\displaystyle E_\\theta \\Big[S_i(\\theta)\\Big] = 0\\). We use this fact later to show that:\n\nMLEs are unbiased in large samples\nMLEs converge to the truth as more data are collected\n\n\n1.3.1 Example: Score vector for Exponential distribution\n\\[\n\\begin{split}\nX &\\sim \\text{Exp}(\\theta) \\\\\nf_X(x) &= \\theta\\exp(-\\theta x) \\\\\n&\\text{ For a single subject } X_i, \\\\\nL_i(\\theta | X_i) &= f_X(X_i; \\theta) \\\\\n&= \\theta \\exp (-\\theta X_i) \\\\\n\\ell_i(\\theta) &= \\ln f_X(X_i; \\theta) \\\\\n&=\\ln \\theta -\\theta X_i \\\\\nS_i(\\theta) &= \\frac{\\partial \\ell_i(\\theta)}{\\partial \\theta} \\\\\n&= \\frac{1}{\\theta} - X_i \\\\\n\\text{To obtain the } &\\text{MLE, we solve} \\\\\n\\frac{1}{n} \\sum_{i = 1}^n S_i(\\theta) &= 0 \\\\\n\\frac{1}{n} \\sum_{i = 1}^n \\frac{1}{\\theta} - X_i &= 0 \\\\\n\\frac{1}{n} \\sum_{i = 1}^n \\frac{1}{\\theta} &= \\frac{1}{n} \\sum_{i = 1}^n X_i \\\\\n\\frac{1}{n} \\cdot \\frac{n}{\\theta}  &=\\frac{1}{n} \\sum_{i = 1}^n X_i \\\\\n\\frac{1}{\\theta}&=\\frac{1}{n} \\sum_{i = 1}^n X_i \\\\\n\\hat \\theta &= \\frac{1}{\\bar X_n}\n\\end{split}\n\\]\n\n\n1.3.2 Example: Score vector for Poisson distribution\n\\[\n\\begin{split}\nX &\\sim \\text{Po}(\\theta)\\\\\nf_X(x) &= \\frac{\\theta^x}{x!} e^{-\\theta}\\\\\n\\text{ For a } &\\text{single subject } X_i, \\\\\nL_i(\\theta | X_i) &= f_X(X_i; \\theta) \\\\\n&= \\frac{\\theta^{X_i}}{X_i!} e^{-\\theta} \\\\\n\\ell_i(\\theta) &= \\ln f_X(X_i; \\theta) \\\\\n&= X_i\\ln\\theta  -\\theta - \\ln X_i!  \\\\\nS_i(\\theta) &= \\frac{\\partial \\ell_i(\\theta)}{\\partial \\theta} \\\\\n&= \\frac{X_i}{\\theta} -1\\\\\n\\text{To obtain} &\\text{ the MLE,  solve} \\\\\n\\frac{1}{n} \\sum_{i = 1}^n S_i(\\theta) &= 0 \\\\\n\\frac{1}{n} \\sum_{i = 1}^n  \\frac{X_i}{\\theta} -1 &= 0 \\\\\n\\frac{1}{n} \\sum_{i = 1}^n  \\frac{X_i}{\\theta} - \\frac{1}{n} \\sum_{i = 1}^n 1 &= 0 \\\\\n\\frac{1}{n} \\sum_{i = 1}^n  \\frac{X_i}{\\theta} &= \\frac{1}{n} \\sum_{i = 1}^n 1 \\\\\n\\frac{1}{\\theta} \\frac{1}{n} \\sum_{i = 1}^n X_i &= \\frac{n}{n} \\\\\n\\frac{1}{\\theta} \\frac{1}{n} \\sum_{i = 1}^n X_i &= 1 \\\\\n\\hat \\theta &=  \\frac{1}{n} \\sum_{i = 1}^n X_i \\\\\n&= \\bar X_n\n\\end{split}\n\\]\n\n\n1.3.3 Example: Score vector for Normal distribution\n\\[\n\\begin{split}\nX &\\sim N(\\mu, \\sigma^2)\\\\\n\\text{ For a } &\\text{single subject } X_i, \\\\\nL_i(\\theta | X_i) &= f_X(X_i; \\theta) \\\\\n&= \\dfrac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\Bigg(-\\dfrac{(X_i - \\mu)^2}{2\\sigma^2}\\Bigg) \\\\\n\\ell_i(\\theta) &= \\ln f_X(X_i; \\theta) \\\\\n&= -\\ln\\sigma-\\ln\\sqrt{2\\pi} - \\frac{(X_i - \\mu)^2}{2\\sigma^2}\\\\\nS_i(\\mu) &= \\frac{\\partial \\ell_i(\\theta)}{\\partial \\mu} \\\\\n&= -\\frac{1}{2\\sigma^2} \\cdot 2(X_i-\\mu) \\cdot -1 \\\\\n&= \\frac{2(X_i - \\mu)}{2\\sigma^2}\\\\\n&= \\frac{X_i - \\mu}{\\sigma^2}\\\\\n\\text{To obtain} &\\text{ the MLE for } \\mu, \\text{ solve} \\\\\n\\frac{1}{n} \\sum_{i = 1}^n S_i(\\mu) &= 0 \\\\\n\\frac{1}{n} \\sum_{i = 1}^n \\frac{X_i - \\mu}{\\sigma^2} &= 0   \\\\\n\\frac{1}{\\sigma^2} \\frac{1}{n} \\sum_{i = 1}^n X_i - \\mu &= 0 \\\\\n\\frac{1}{n} \\sum_{i = 1}^n X_i - \\mu &= 0 \\\\\n\\frac{1}{n} \\sum_{i = 1}^n X_i &= \\frac{1}{n} \\sum_{i = 1}^n \\mu \\\\\n\\frac{1}{n} \\sum_{i = 1}^n X_i &= \\frac{n\\mu}{\\mu} = \\mu \\\\\n\\Rightarrow \\hat \\mu &= \\bar X_n \\\\\n\\end{split}\n\\]\n\\[\n\\begin{split}\n\\text{Rewrite } \\ell_i(\\theta) &= -\\ln\\sigma-\\ln\\sqrt{2\\pi} - \\frac{(X_i - \\mu)^2}{2} \\cdot \\frac{1}{\\sigma^2} \\\\\n&=  -\\ln\\sigma-\\ln\\sqrt{2\\pi} - \\frac{(X_i - \\mu)^2}{2} \\cdot \\sigma^{-2} \\\\\nS_i(\\sigma) &= \\frac{\\partial \\ell_i(\\theta)}{\\partial \\sigma} \\\\\n&= -\\frac{1}{\\sigma} - \\frac{(X_i - \\mu)^2}{2} \\cdot -2\\sigma^{-3} \\\\\n&= -\\frac{1}{\\sigma} + \\frac{(X_i - \\mu)^2}{\\sigma^3} \\\\\n&= \\frac{1}{\\sigma} \\Bigg(-1 +  \\frac{(X_i - \\mu)^2}{\\sigma^2} \\Bigg) \\\\\n\\text{Equivalently, } S_i(\\sigma^2) &= -\\frac{1}{2\\sigma^2} + \\frac{(X_i - \\mu)^2}{2\\sigma^4}  \\\\\n\\text{To obtain} &\\text{ the MLE for } \\sigma \\text{ and } \\sigma^2, \\text{ solve} \\\\\n\\frac{1}{n} \\sum_{i = 1}^n S_i(\\sigma) &= 0 \\\\\n\\frac{1}{n} \\sum_{i = 1}^n \\frac{1}{\\sigma} \\Bigg(-1 +  \\frac{(X_i - \\mu)^2}{\\sigma^2} \\Bigg) &= 0 \\\\\n\\frac{1}{\\sigma}  \\frac{1}{n} \\sum_{i = 1}^n -1 +  \\frac{(X_i - \\mu)^2}{\\sigma^2} &= 0 \\\\\n\\frac{1}{n} \\sum_{i = 1}^n -1 +  \\frac{(X_i - \\mu)^2}{\\sigma^2} &= 0 \\\\\n\\frac{1}{n} \\sum_{i = 1}^n \\frac{(X_i - \\mu)^2}{\\sigma^2} &=  \\frac{1}{n} \\sum_{i = 1}^n 1 \\\\\n&= \\frac{n}{n} = 1\\\\\n\\frac{1}{\\sigma^2} \\frac{1}{n} \\sum_{i = 1}^n  (X_i - \\mu)^2 &= 1 \\\\\n\\hat{\\sigma^2} &=\\frac{1}{n} \\sum_{i = 1}^n  (X_i - \\mu)^2\n\\end{split}\n\\]\nThus, we have obtained the score vector for a single individual and the (total) score:\n\\[\n\\begin{split}\nS_i(\\mu,\\sigma) &=\n\\begin{Bmatrix}\n\\dfrac{X_i - \\mu}{\\sigma^2}  \\\\\n-\\dfrac{1}{\\sigma} + \\dfrac{(X_i - \\mu)^2}{\\sigma^3} \\\\\n\\end{Bmatrix} \\\\\nS(\\mu,\\sigma) &=\n\\begin{Bmatrix}\n\\displaystyle \\sum_{i = 1}^n \\dfrac{X_i - \\mu}{\\sigma^2}  \\\\\n\\displaystyle \\sum_{i = 1}^n  -\\dfrac{1}{\\sigma} + \\dfrac{(X_i - \\mu)^2}{\\sigma^3} \\\\\n\\end{Bmatrix} \\\\\n\\end{split}\n\\]"
  },
  {
    "objectID": "week1.html#fisher-information-matrix-and-large-sample-variance",
    "href": "week1.html#fisher-information-matrix-and-large-sample-variance",
    "title": "1  Point estimation",
    "section": "1.4 Fisher information matrix and large-sample variance",
    "text": "1.4 Fisher information matrix and large-sample variance\nThere are two equivalent definitions/ways of obtaining the Fisher information matrix\n\\[\n\\begin{split}\nI_1(\\theta) &= - E\\Bigg[\\frac{\\partial}{\\partial \\theta} S_i(\\theta) \\Bigg] \\\\\n&= - E\\Bigg[\\frac{\\partial^2}{\\partial \\theta^2}  \\ln f_X(X_i;\\theta) \\Bigg] \\\\\nI_1(\\theta) &= E\\Bigg[ \\Big\\{S_i(\\theta)\\Big\\}^2 \\Bigg] \\\\\n&=E\\Bigg[ \\Big\\{  \\frac{\\partial}{\\partial \\theta} \\ln f_X(X_i;\\theta)\\Big\\}^2 \\Bigg]\n\\end{split}\n\\]\nWhen the model is correct, assuming regularity conditions that allow interchanging the derivatives and integral, then\n\\[\n\\text{Var}_\\theta \\Big[ S_i(\\theta) \\Big] = - E_\\theta\\Bigg[\\frac{\\partial}{\\partial \\theta} S_i (\\theta) \\Bigg] \\equiv I(\\theta)\n\\]\nDistribution of the MLE in large samples\nIn large samples, the MLE is normal with large-sample variance (in the univariate case) or large-sample covariance matrix (in the multivariate case)\n\\[\n\\text{Var}(\\hat \\theta) = \\frac{1}{nI(\\theta)}\n\\]\nThe total Fisher information equals \\(\\displaystyle nI(\\theta)\\).\n\n1.4.1 Multivariate Fisher information\nIn the multivariate parameter case, the Fisher information matrix \\(I(\\theta)\\) is the matrix of second derivatives of the log likelihood for a single individual, with elements\n\\[\nI_{jk}(\\theta) = - E_\\theta \\Bigg[\\frac{\\partial^2}{\\partial\\theta_j\\partial\\theta_k} \\ell_i(\\theta) \\Bigg]\n\\]\nThe large sample variance of the MLE for a parameter \\(\\theta\\) when the other parameter \\(\\eta\\) is known equals \\(\\displaystyle \\Big(n I_{\\theta\\theta}\\Big)^{-1}\\).\nWhen the other parameter \\(\\eta\\) is unknown, we have the Fisher information\n\\[\\displaystyle I^*_{\\theta\\theta} = I_{\\theta\\theta} - \\Big[ I_{\\theta\\eta} \\cdot I^{-1}_{\\eta\\eta} \\cdot I_{\\eta\\theta} \\Big] = I_{\\theta\\theta} - \\dfrac{I_{\\theta\\eta}^2}{I_{\\eta\\eta}}\\]\nThen, we have the large sample variance of the MLE for \\(\\theta\\) with the other parameter unknown: \\(\\displaystyle \\Big(n I^*_{\\theta\\theta}\\Big)^{-1}\\). And we have the total information \\(nI^*_{\\theta\\theta}\\).\nNote that, when the off-diagonal elements (\\(I_{\\theta\\eta}\\) and \\(I_{\\eta\\theta}\\)) equal zero, then the large sample variance of the MLe for \\(\\theta\\) and \\(\\eta\\) with the other parameter unknown will necessarily equal the large sample variance of the MLE for \\(\\theta\\) and \\(\\eta\\) with the other parameter known. That is:\n\\[\\displaystyle I^*_{\\theta\\theta} = I_{\\theta\\theta} - \\dfrac{I_{\\theta\\eta}^2}{I_{\\eta\\eta}} = I_{\\theta\\theta} - \\dfrac{0}{I_{\\eta\\eta}} = I_{\\theta\\theta} \\]\n\n\n1.4.2 Example: Fisher information matrix for Exponential distribution\n\n1.4.2.1 Approach 1\n\\[\n\\begin{split}\nX &\\sim \\text{Exp}(\\theta) \\\\\nS_i(\\theta) &= \\frac{1}{\\theta} - X_i \\\\\n&= \\theta^{-1} - X_i \\\\\nI_\\theta &= - E_\\theta\\Bigg[\\frac{\\partial}{\\partial \\theta} S_i (\\theta) \\Bigg] \\\\\n&= - E_\\theta \\Big[-\\theta^{-2} \\Big]\\\\\n&= -E_\\theta \\Big[-\\frac{1}{\\theta^2}\\Big] \\\\\n&= E_\\theta \\Big[\\frac{1}{\\theta^2}\\Big] \\\\\n&= \\frac{1}{\\theta^2} \\\\\nnI_\\theta &= \\frac{n}{\\theta^2}, \\text{the total Fisher information }\\\\\n\\text{We have the MLE } \\hat \\theta &= \\frac{1}{\\bar X_n} \\text{ and } \\\\\n\\text{Var}(\\hat \\theta) &= \\frac{1}{nI(\\theta)}, \\text{the large-sample variance} \\\\\n&= \\frac{1}{n} \\cdot \\theta^2 \\\\\n&= \\frac{\\theta^2}{n}\n\\end{split}\n\\]\n\n\n1.4.2.2 Approach 2\nWe have\n\\[\n\\begin{split}\nE[X] &= \\frac{1}{\\theta} \\\\\n\\text{Var}[X] &= \\frac{1}{\\theta^2} \\\\\nI_1(\\theta) &= E\\Big[ \\{S_i(\\theta)\\}^2 \\Big] \\\\\n&= E\\Big[\\Big(\\frac{1}{\\theta} - X \\Big)^2 \\Big] \\\\\n&= E\\Big[\\frac{1}{\\theta^2} - 2\\frac{X}{\\theta} + X^2 \\Big] \\\\\n&= \\frac{1}{\\theta^2} - \\frac{2}{\\theta}E[X] + E[X^2] \\\\\n&= \\frac{1}{\\theta^2} - \\frac{2}{\\theta^2} + \\text{Var}[X] + (E[X])^2\\\\\n&= \\frac{1}{\\theta^2} - \\frac{2}{\\theta^2} + \\frac{1}{\\theta^2} + \\frac{1}{\\theta^2} \\\\\n&= \\frac{1}{\\theta^2}\\\\\n\\end{split}\n\\]\n\n\n\n\n1.4.3 Example: Fisher information matrix for Poisson distribution\n\n1.4.3.1 Approach 1\n\\[\n\\begin{split}\nX &\\sim \\text{Poisson}(\\theta) \\\\\nS_i(\\theta) &=  \\frac{X_i}{\\theta} -1\\\\\nI_\\theta &= - E_\\theta\\Bigg[\\frac{\\partial}{\\partial \\theta} S_i (\\theta) \\Bigg] \\\\\n&= - E_\\theta\\Bigg[\\frac{\\partial}{\\partial \\theta} \\frac{X_i}{\\theta} -1\\Bigg] \\\\\n&= - E_\\theta\\Bigg[\\frac{\\partial}{\\partial \\theta} X_i \\cdot \\theta^{-1} -1\\Bigg] \\\\\n&= - E_\\theta \\Big[X_i \\cdot -\\theta^{-2} \\Big]\\\\\n&= -E_\\theta \\Big[-\\frac{X_i}{\\theta^2}\\Big] \\\\\n&= E_\\theta \\Big[\\frac{X_i}{\\theta^2}\\Big] \\\\\n&= \\frac{E_\\theta[X_i]}{\\theta^2} \\\\\n&= \\frac{\\theta}{\\theta^2}\\\\\n&= \\frac{1}{\\theta} \\\\\nnI(\\theta) &= \\frac{n}{\\theta}, \\text{ the total Fisher information}\\\\\n\\text{We have the MLE } \\hat \\theta &= \\bar X_n \\text{ and } \\\\\n\\text{Var}(\\hat \\theta) &= \\frac{1}{nI(\\theta)}, \\text{ the large-sample variance} \\\\\n&= \\frac{\\theta}{n}\n\\end{split}\n\\]\n\n\n1.4.3.2 Approach 2\n\\[\n\\begin{split}\nE[X] &= \\text{Var}[X] = \\theta \\\\\nI_1(\\theta) &= E\\Big[ \\{S_i(\\theta)\\}^2 \\Big] \\\\\n&= E\\Big[ \\Big(\\frac{X}{\\theta} - 1 \\Big)^2 \\Big] \\\\\n&= E\\Big[\\frac{X^2}{\\theta^2} - 2\\frac{X}{\\theta} + 1 \\Big] \\\\\n&= \\frac{1}{\\theta^2}E[X^2] -\\frac{2}{\\theta}E[X] + 1\\\\\n&= \\frac{1}{\\theta^2} \\Big(\\text{Var}[X] + \\{E[X]\\}^2 \\Big) -\\frac{2\\theta}{\\theta} + 1 \\\\\n&= \\frac{1}{\\theta^2} \\Big(\\theta + \\theta^2 \\Big) -2+1 \\\\\n&=\\frac{1}{\\theta} + 1- 1 \\\\\n&= \\frac{1}{\\theta}\n\\end{split}\n\\]\n\n\n\n1.4.4 Example: Fisher information for Normal Distribution\n\\[\n\\begin{split}\nS_i(\\mu,\\sigma) &=\n\\begin{Bmatrix}\n\\displaystyle\\frac{\\partial}{\\partial \\mu} \\ell_i(\\mu, \\sigma)  \\\\\n\\displaystyle\\frac{\\partial}{\\partial \\sigma} \\ell_i(\\mu, \\sigma) \\\\\n\\end{Bmatrix} \\\\\n&=\n\\begin{Bmatrix}\n\\dfrac{X_i - \\mu}{\\sigma^2}  \\\\\n-\\dfrac{1}{\\sigma} + \\dfrac{(X_i - \\mu)^2}{\\sigma^3} \\\\\n\\end{Bmatrix} \\\\\nI_{jk}(\\theta) &= - E_\\theta \\Bigg[\\frac{\\partial^2}{\\partial\\theta_j\\partial\\theta_k} \\ell_i(\\theta) \\Bigg] \\\\\nI_{11}(\\theta) &= - E_\\theta \\Bigg[\\frac{\\partial}{\\partial\\mu}\\frac{\\partial}{\\partial\\mu} \\ell_i(\\theta) \\Bigg]\\\\\n&=- E_\\theta \\Bigg[\\frac{\\partial}{\\partial\\mu} \\dfrac{X_i - \\mu}{\\sigma^2} \\Bigg]\\\\\n&=- E_\\theta \\Bigg[\\frac{\\partial}{\\partial\\mu} \\dfrac{X_i}{\\sigma^2} - \\dfrac{\\mu}{\\sigma^2}  \\Bigg]\\\\\n&=- E_\\theta \\Bigg[-\\frac{1}{\\sigma^2}  \\Bigg]  \\\\\n&= E_\\theta \\Bigg[\\frac{1}{\\sigma^2}  \\Bigg]  \\\\\n&= \\frac{1}{\\sigma^2} \\\\\nI_{21}(\\theta) = I_{12}(\\theta) &= - E_\\theta \\Bigg[\\frac{\\partial}{\\partial\\sigma}\\frac{\\partial}{\\partial\\mu} \\ell_i(\\theta) \\Bigg]\\\\\n&= - E_\\theta \\Bigg[\\frac{\\partial}{\\partial\\sigma}\\dfrac{X_i - \\mu}{\\sigma^2} \\Bigg] \\\\\n&= - E_\\theta \\Bigg[\\frac{\\partial}{\\partial\\sigma}(X_i - \\mu) \\cdot \\sigma^{-2} \\Bigg] \\\\\n&= - E_\\theta \\Bigg[(X_i - \\mu) \\cdot -2\\sigma^{-3} \\Bigg] \\\\\n&= - E_\\theta \\Bigg[-\\frac{2(X_i - \\mu)}{\\sigma^3}  \\Bigg] \\\\\n&=  E_\\theta \\Bigg[\\frac{2(X_i - \\mu)}{\\sigma^3}  \\Bigg] \\\\\n&=  \\frac{2}{\\sigma^3}E_\\theta \\Big[X_i - \\mu \\Big] \\\\\n&= \\frac{2}{\\sigma^3} \\Big[E_\\theta[X_i] - \\mu \\Big] \\\\\n&= \\frac{2}{\\sigma^3} \\Big[\\mu - \\mu \\Big] = 0 \\\\\nI_{22}(\\theta) &= - E_\\theta \\Bigg[\\frac{\\partial}{\\partial\\sigma}\\frac{\\partial}{\\partial\\sigma} \\ell_i(\\theta) \\Bigg]\\\\\n&= - E_\\theta \\Bigg[\\frac{\\partial}{\\partial\\sigma}-\\dfrac{1}{\\sigma} + \\dfrac{(X_i - \\mu)^2}{\\sigma^3} \\Bigg]\\\\\n&= - E_\\theta \\Bigg[\\frac{\\partial}{\\partial\\sigma}-\\sigma^{-1} + (X_i - \\mu)^2 \\cdot \\sigma^{-3} \\Bigg]\\\\\n&= - E_\\theta \\Bigg[\\frac{1}{\\sigma^2} + (X_i - \\mu)^2 \\cdot -3\\sigma^{-4} \\Bigg]\\\\\n&= - E_\\theta \\Bigg[\\frac{1}{\\sigma^2} + (X_i - \\mu)^2 \\cdot \\frac{-3}{\\sigma^4}  \\Bigg]\\\\\n&= -\\frac{1}{\\sigma^2} - E_\\theta \\Bigg[ (X_i - \\mu)^2 \\cdot \\frac{-3}{\\sigma^4}  \\Bigg]\\\\\n&= -\\frac{1}{\\sigma^2} + \\frac{3}{\\sigma^4}E_\\theta \\Big[ (X_i - \\mu)^2   \\Big]\\\\\n&= -\\frac{1}{\\sigma^2} + \\frac{3}{\\sigma^4}E_\\theta \\Bigg[ \\Big(X_i - E[X_i]\\Big)^2   \\Bigg]\\\\\n&= -\\frac{1}{\\sigma^2} + \\frac{3}{\\sigma^4}\\text{Var}[X]\\\\\n&= -\\frac{1}{\\sigma^2} + \\frac{3\\sigma^2}{\\sigma^4}\\\\\n&= \\frac{3}{\\sigma^2} - \\frac{1}{\\sigma^2} \\\\\n&= \\frac{2}{\\sigma^2} \\\\\n\\end{split}\n\\]\nThus, we have obtained\n\\[\n\\begin{split}\n\\text{The Fisher information matrix } I(\\theta) &=\n\\begin{bmatrix}\n\\displaystyle \\frac{1}{\\sigma^2} & 0 \\\\\n0 & \\displaystyle \\frac{2}{\\sigma^2}\\\\\n\\end{bmatrix} \\\\\n\\text{The total Fisher information matrix } nI(\\theta) &=\n\\begin{bmatrix}\n\\displaystyle \\frac{n}{\\sigma^2} & 0 \\\\\n0 & \\displaystyle \\frac{2n}{\\sigma^2}\\\\\n\\end{bmatrix} \\\\\n\\text{The large-sample covariance matrix } \\frac{1}{nI(\\theta)} &=\n\\begin{bmatrix}\n\\displaystyle \\frac{\\sigma^2}{n} & 0 \\\\\n0 & \\displaystyle \\frac{\\sigma^2}{2n}\\\\\n\\end{bmatrix} \\\\\n\\end{split}\n\\]\nBecause the off-diagonal elements of the Fisher information matrix equal \\(0\\), we can immediately infer that the large sample variance of \\(\\hat \\mu\\) equals \\(\\dfrac{\\sigma^2}{n}\\) regardless of whether \\(\\sigma\\) is known; the large sample variance of \\(\\hat \\sigma^2\\) equals \\(\\dfrac{\\sigma^2}{2n}\\) regardless of whether \\(\\sigma\\) is known.\n\n\n1.4.5 Example: Fisher information for Poisson regression\n\\[\n\\begin{split}\nS_i(\\beta_0,\\beta_1) &=\n\\begin{Bmatrix}\n\\displaystyle\\frac{\\partial}{\\partial \\beta_0} \\ell_i(\\mu, \\sigma)  \\\\\n\\displaystyle\\frac{\\partial}{\\partial \\beta_1} \\ell_i(\\mu, \\sigma) \\\\\n\\end{Bmatrix} \\\\\n&=\n\\begin{Bmatrix}\nY - \\exp(\\beta_0 + \\beta_1X)  \\\\\nXY - X\\exp(\\beta_0 + \\beta_1X) \\\\\n\\end{Bmatrix} \\\\\nI_{11}(\\theta)&= -E\\Bigg[ \\frac{\\partial}{\\partial \\beta_0} Y - \\exp(\\beta_0 + \\beta_1X) \\Bigg] \\\\\n&= - E\\Big[-\\exp(\\beta_0 + \\beta_1X) \\Big] \\\\\n&= E\\Big[\\exp(\\beta_0 + \\beta_1X) \\Big] \\\\\nI_{22}(\\theta)&= -E\\Bigg[ \\frac{\\partial}{\\partial \\beta_1} XY - X\\exp(\\beta_0 + \\beta_1X) \\Bigg] \\\\\n&= - E\\Big[-X^2\\exp(\\beta_0 + \\beta_1X) \\Big] \\\\\n&= E\\Big[X^2\\exp(\\beta_0 + \\beta_1X) \\Big] \\\\\nI_{12}(\\theta) = I_{21}(\\theta)&= -E\\Bigg[ \\frac{\\partial}{\\partial \\beta_1} Y - \\exp(\\beta_0 + \\beta_1X) \\Bigg] \\\\\n&= -E\\Big[-X\\exp(\\beta_0 + \\beta_1X) \\Big] \\\\\n&= E\\Big[X\\exp(\\beta_0 + \\beta_1X) \\Big] \\\\\n\\end{split}\n\\]\nWe have obtained the Fisher information matrix,\n\\[\n\\begin{split}\nI(\\theta) &=\n\\begin{bmatrix}\nE\\Big[\\exp(\\beta_0 + \\beta_1X) \\Big] & E\\Big[X\\exp(\\beta_0 + \\beta_1X) \\Big] \\\\\nE\\Big[X\\exp(\\beta_0 + \\beta_1X) \\Big] & E\\Big[X^2\\exp(\\beta_0 + \\beta_1X) \\Big]\\\\\n\\end{bmatrix} \\\\\n\\text{if } \\beta_0 \\text{ is known, } I_{\\beta_1\\beta_1} &=E\\Big[X^2\\exp(\\beta_0 + \\beta_1X) \\Big]\\\\\n\\text{Var}(\\hat \\beta_1) &= \\frac{1}{nE\\Big[X^2\\exp(\\beta_0 + \\beta_1X) \\Big]} \\\\\n\\text{if } \\beta_0 \\text{ is unknown, } I^*_{\\beta_1\\beta_1} &= I_{\\beta_1\\beta_1} - \\frac{(I_{\\beta_0\\beta_1})^2}{I_{\\beta_0\\beta_0}}\\\\\n&= E\\Big[X^2\\exp(\\beta_0 + \\beta_1X) \\Big] - \\frac{E[X\\exp(\\beta_0 + \\beta_1X)]^2}{E[\\exp(\\beta_0 + \\beta_1X)]}\n\\\\\n\\text{Var}(\\hat \\beta_1) &= \\frac{1}{n} \\Bigg(E\\Big[X^2\\exp(\\beta_0 + \\beta_1X) \\Big] - \\frac{E[X\\exp(\\beta_0 + \\beta_1X)]^2}{E[\\exp(\\beta_0 + \\beta_1X)]}\\Bigg)^{-1}\n\\end{split}\n\\]\n\n\n\n1.4.6 Example: Fisher information for Gamma distribution\nIf both the shape \\(\\alpha\\) and rate \\(\\lambda\\) are unknown for iid gamma variates \\(X_i \\sim\\) Ga(\\(\\alpha, \\lambda\\)), the Fisher information for \\(\\theta = (\\alpha, \\lambda)\\) is\n\\[\n\\begin{split}\nI(\\theta) &= \\begin{bmatrix}\n\\psi'(\\alpha) & -\\lambda^{-1} \\\\\n-\\lambda^{-1} & \\alpha\\lambda^{-2}\n\\end{bmatrix} \\text{ where }\\\\\n\\ell_i(\\theta) &= \\alpha \\ln(\\lambda) + (\\alpha - 1)\\ln X-\\lambda X -\\ln \\Gamma(\\alpha)\n\\end{split}\n\\]\nNB: \\(\\psi'(\\alpha)\\) is the trigamma function (the second derivative of the log of \\(\\Gamma(\\alpha)\\)).\nAbove, we used the shape-rate form of the Gamma distribution. If, instead, we use the shape-scale form(\\(\\alpha, \\beta\\)) with \\(\\alpha\\) known, we have\n\\[\n\\begin{split}\n\\ell(\\beta) &= -n\\ln\\Gamma(\\alpha) -n\\alpha\\ln\\beta + (a - 1)\\sum_{i = 1}^n \\ln x_i - \\frac{\\sum_{i = 1}^n x_i}{\\beta} \\\\\nS(\\beta) &= \\frac{n\\alpha}{\\beta^2}\\Bigg(\\frac{\\sum_{i = 1}^n x_i}{n\\alpha} -\\beta \\Bigg)\n\\end{split}\n\\]"
  },
  {
    "objectID": "week1.html#cramer-rao-inequality",
    "href": "week1.html#cramer-rao-inequality",
    "title": "1  Point estimation",
    "section": "1.5 Cramer-Rao inequality",
    "text": "1.5 Cramer-Rao inequality\nMinimum Variance Unbiased Estimators (MVUE) for \\(\\theta\\): An unbiased estimator (for every \\(\\theta\\)) whose variance is no larger than that of any other unbiased estimator.\nCramer-Rao information inequality for univariate parameters\nLet \\(W(\\pmb X)\\) be an unbiased estimator of a scalar parameter \\(\\tau(\\theta)\\). Then\n\\[\n\\begin{split}\n\\text{Var}\\{W(\\pmb X)\\} &\\ge \\frac{\\{\\frac{\\partial}{\\partial\\theta} \\tau(\\theta)\\}^2}{nI(\\theta)} \\\\\n\\text{Var}\\{W(\\pmb X)\\} &\\ge {\\Big\\{\\frac{\\partial}{\\partial\\theta} \\tau(\\theta)\\Big\\}^2} \\frac{1}{nI(\\theta)}\n\\end{split}\n\\]\nCramer-Rao information inequality for multivariate parameters\nLet \\(W(\\pmb X)\\) be an unbiased estimator of a multivariate parameter \\(\\tau(\\theta)\\).\n\\[\n\\text{Var}\\{W(\\pmb X)\\} \\ge {\\Big\\{\\frac{\\partial}{\\partial\\theta} \\tau(\\theta)\\Big\\}^t} {\\Big\\{\\frac{\\partial}{\\partial\\theta} \\tau(\\theta)\\Big\\}}  \\frac{1}{nI(\\theta)}\n\\]\nFor unbiased estimators of \\(\\tau(\\theta)\\), the Cramer-Rao inequality provides a lower bound for the variance.\nFor unbiased estimators \\(W(\\pmb X)\\) of \\(\\tau(\\theta)\\) (i.e., a function of \\(\\theta\\)), we have\n\\[\n\\text{Var}\\{W(\\pmb X)\\} \\ge \\underbrace{{\\Big\\{\\frac{\\partial}{\\partial\\theta} \\tau(\\theta)\\Big\\}^2} \\frac{1}{nI(\\theta)}}_{\\text{Cramer-Rao Lower Bound}}\n\\]\nFor unbiased estimators \\(W(\\pmb X)\\) of \\(\\theta\\), we have\n\\[\\text{Var}\\{W(\\pmb X)\\} \\ge \\underbrace{\\frac{1}{nI(\\theta)}}_{\\text{CRLB}}\\]\nMLEs are Best Asymptotically Normal (i.e., asymptotically efficient): The MLE has a limiting normal distribution with the Cramer-Rao lower bound as variance. That is,\n\\[\\text{Var}(\\hat \\theta)= \\frac{1}{nI(\\theta)}\\]\n\n1.5.1 Example: Cramer-Rao Lower bound for Poisson Distribution\nGiven a sample \\(X_1, ..., X_n\\) from the Poisson distribution with density\n\\[\nf(x; \\theta) = e^{-\\theta} \\frac{\\theta^x}{x!}\n\\]\nwith \\(\\theta\\) unknown and \\(x \\in \\mathbb{R}\\). We know that \\(E[X] = \\text{Var}[X] = \\theta\\). Find the MLE of \\(\\theta\\), the Fisher information and the corresponding large-sample variance of the MLE, and the Cramer-Rao lower bound for any unbiased estimator of \\(e^{-\\theta}\\).\n\\[\n\\begin{split}\n\\hat \\theta &= \\bar X_n \\\\\nI(\\theta) &= \\frac{1}{\\theta}  \\\\\nnI(\\theta) &= \\frac{n}{\\theta}  \\\\\n\\frac{1}{nI(\\theta)} &= \\frac{\\theta}{n} = \\text{Var}(\\hat \\theta) \\\\\n\\text{Var}\\{W(\\pmb X)\\} &\\ge {\\Big\\{\\frac{\\partial}{\\partial\\theta} \\tau(\\theta)\\Big\\}^2} \\frac{1}{nI(\\theta)} \\\\\n\\text{Var}\\{W(\\pmb X)\\} &\\ge {\\Big\\{\\frac{\\partial}{\\partial\\theta} e^{-\\theta}\\Big\\}^2} \\frac{\\theta}{n} \\\\\n\\text{Var}\\{W(\\pmb X)\\} &\\ge {\\Big\\{-e^{-\\theta}\\Big\\}^2} \\frac{\\theta}{n} \\\\\n\\text{Var}\\{W(\\pmb X)\\} &\\ge e^{-2\\theta} \\frac{\\theta}{n} \\\\\n\\text{Var}\\{W(\\pmb X)\\} &\\ge \\frac{\\theta e^{-2\\theta}}{n} \\\\\n\\end{split}\n\\]"
  },
  {
    "objectID": "week1.html#newton-raphson-method",
    "href": "week1.html#newton-raphson-method",
    "title": "1  Point estimation",
    "section": "1.6 Newton-Raphson method",
    "text": "1.6 Newton-Raphson method\nThe NR method is an iterative application of\n\\[x_{n + 1} = x_n - f(x_n)/f'(x_n)\\]\nFor example, let \\(f(x) = x^2\\). Our goal is to solve \\(f(x) = 0\\). Then,\n\\[\n\\begin{split}\nf(x) &= x^2 \\\\\nf'x &= 2x\\\\\nx_{n + 1} &= x_n - f(x_n)/f'(x_n) \\\\\n&= x_n - \\frac{x_n^2}{2x_n} \\\\\n&= x_n - \\frac{1}{2}x_n \\\\\n&= \\frac{x_n}{2}\n\\end{split}\n\\]\nThus, we see that the iteration converges towards 0, the solution to \\(f(x) = x^2 = 0\\).\nOur goal is to find the solution to \\(\\displaystyle \\frac{1}{n} \\sum_{i = 1}^n S_i(\\theta) = 0\\). We have\n\\[\n\\begin{split}\nf(\\theta) &= \\frac{1}{n} \\sum_{i = 1}^n S_i(\\theta) \\\\\nf'(\\theta) &= \\frac{1}{n} \\sum_{i = 1}^n \\frac{\\partial}{\\partial \\theta} S_i(\\theta)\\\\\n\\theta_{n + 1} &= \\theta_n - f(\\theta_n)/f'(\\theta_n) \\\\\n&= \\theta_n - \\dfrac{\\frac{1}{n} \\sum_{i = 1}^n S_i(\\theta_n)}{\\frac{1}{n} \\sum_{i = 1}^n \\frac{\\partial}{\\partial \\theta} S_i(\\theta_n)}\n\\end{split}\n\\]"
  },
  {
    "objectID": "week2.html",
    "href": "week2.html",
    "title": "2  Large sample theory",
    "section": "",
    "text": "The maximum likelihood estimator \\(\\hat\\theta_n\\) is distributed \\(N(\\theta, (nI_\\theta)^{-1})\\) for every \\(\\theta\\), for large n.\nAn estimator \\(\\pmb{\\hat \\theta}\\) of \\(\\pmb \\theta\\) is weakly consistent if the estimator \\(\\pmb{\\hat \\theta}\\) converges in probability to the true value \\(\\pmb \\theta\\). An estimator \\(\\pmb{\\hat \\theta}\\) of \\(\\pmb \\theta\\) is strongly consistent if the estimator \\(\\pmb{\\hat \\theta}\\) converges with probability one to the true value (i.e., converges almost surely to \\(\\pmb \\theta\\)). An estimator \\(\\pmb{\\hat \\theta}\\) is asymptotically normal if \\(\\sqrt n(\\pmb{\\hat \\theta} - \\pmb\\theta)\\) converges in distribution to a normal distribution with mean \\(\\pmb 0\\) and covariance matrix \\(\\pmb \\Sigma\\).\n\\[\n\\begin{split}\n\\text{Weak consistency: }& \\pmb{\\hat \\theta} \\xrightarrow{P} \\pmb\\theta \\text{ as }n \\rightarrow \\infty \\\\\n\\text{Strong consistency: }& \\text{ For any } \\epsilon > 1,  P(\\lim_{n \\rightarrow \\infty}|\\pmb{\\hat \\theta} - \\pmb\\theta| < \\epsilon) = 1. \\text{ That is,}\\\\\n&\\pmb{\\hat \\theta} \\xrightarrow{\\text{a.s.}} \\pmb\\theta\\\\\n\\text{Asymptotic normality: }& \\sqrt n(\\pmb{\\hat \\theta} - \\pmb\\theta) \\xrightarrow{d} N(0, \\pmb\\Sigma)\n\\end{split}\n\\]"
  },
  {
    "objectID": "week2.html#central-limit-theorem-clt",
    "href": "week2.html#central-limit-theorem-clt",
    "title": "2  Large sample theory",
    "section": "2.2 Central limit theorem (CLT)",
    "text": "2.2 Central limit theorem (CLT)\nLet \\(X_1, X_2, ...\\) be a sequence of iid random variables with \\(E(X_i) = \\mu\\) and \\(Var(X_i) = \\sigma^2\\). Then\n\\[\n\\begin{split}\n\\frac{\\sqrt n (\\bar X_n - \\mu)}{\\sigma} &\\xrightarrow{D} N(0,1)\\\\\n\\frac{\\sqrt n (\\bar X_n - \\mu)}{\\sigma} &\\approx N(0,1) \\\\\n\\sqrt n (\\bar X_n - \\mu) &\\xrightarrow{D} N(0, \\sigma^2)\n\\end{split}\n\\]"
  },
  {
    "objectID": "week2.html#delta-method",
    "href": "week2.html#delta-method",
    "title": "2  Large sample theory",
    "section": "2.3 Delta method",
    "text": "2.3 Delta method\n\\[\n\\begin{split}\n\\text{Let } X_n &\\text{ be a sequence of RVs s.t.} \\\\\n\\sqrt n(X_n - \\theta) &\\xrightarrow{D} N(0, \\sigma^2) \\\\\n&\\text{Then, } \\\\\n\\sqrt n\\Big(g(X_n) - g(\\theta)\\Big) &\\xrightarrow{D} N\\Big(0, \\sigma^2\\Big[g'(\\theta)\\Big]^2\\Big)\n\\end{split}\n\\]\nSome useful identities to obtain \\(N(0,1)\\):\n\\[\n\\begin{split}\nN(0, k) &= \\sqrt k N(0,1) \\\\\nN(0, k^2) &= kN(0,1) \\\\\nN(\\mu, k^2) &= kN(\\frac{\\mu}{k}, 1)\n\\end{split}\n\\]\nConsider iid random observations \\(X_1, X_2,...\\) with \\(E(X_i) = \\mu\\) and \\(Var(X_i) = \\sigma^2 < \\infty\\). We want to estimate \\(1/\\mu\\). Use the Delta Method to obtain the asymptotic distribution of \\(\\sqrt n\\Big(\\frac{1}{\\bar X_n} - \\frac{1}{\\mu}\\Big)\\). We have (from the CLT):\n\\[\n\\begin{split}\n\\sqrt n\\Big({\\bar X_n} - {\\mu}\\Big) &\\xrightarrow{D} N(0, \\sigma^2) \\\\\n\\sqrt n\\Big(g(X_n) - g(\\theta)\\Big) &\\xrightarrow{D} N\\Big(0, \\sigma^2\\Big[g'(\\theta)\\Big]^2\\Big) \\\\\n\\text{where } g(\\theta) = \\frac{1}{\\theta},~ g'(\\theta) &= -\\frac{1}{\\theta^2}, g'(\\theta)^2 = \\frac{1}{\\theta^4}, \\text{ and } \\theta = \\mu \\\\\n\\sqrt n\\Big(\\frac{1}{\\bar X_n} - \\frac{1}{\\mu}\\Big) &\\xrightarrow{D} N\\Big(0, \\frac{\\sigma^2}{\\mu^4}\\Big) \\\\\n\\sqrt n\\Big(\\frac{1}{\\bar X_n} - \\frac{1}{\\mu}\\Big) &\\xrightarrow{D} \\frac{\\sigma}{\\mu^2}N\\Big(0, 1\\Big) \\\\\n\\text{Using Slutsky's} &\\text{ lemma}, \\\\\n\\frac{\\sqrt n\\Big(\\frac{1}{\\bar X_n} - \\frac{1}{\\mu}\\Big)}{\\sigma / \\mu^2} &\\xrightarrow{D} N\\Big(0, 1\\Big) \\\\\n\\text{Plugging in consistent} &\\text{ estimators}, \\\\\n\\frac{\\sqrt n\\Big(\\frac{1}{\\bar X_n} - \\frac{1}{\\mu}\\Big)}{\\sqrt{S^2_n} / \\bar X^2_n} &\\xrightarrow{D} N\\Big(0, 1\\Big)\n\\end{split}\n\\]"
  },
  {
    "objectID": "week2.html#variance-stabilizing-transformations",
    "href": "week2.html#variance-stabilizing-transformations",
    "title": "2  Large sample theory",
    "section": "2.4 Variance stabilizing transformations",
    "text": "2.4 Variance stabilizing transformations\nWe have \\(X_1, X_2, ...\\) iid Poisson(\\(\\lambda\\)) random variables. Find an appropriate variance stabilizing transformation. We have\n\\[\n\\begin{split}\nE[X_i] = \\text{Var}[X_i] &= \\lambda. \\text{From the CLT, } \\\\\n\\sqrt n (\\bar X_n - \\lambda) &\\xrightarrow{D}N(0,\\lambda) \\\\\n\\text{We wish to find a function } &g(\\lambda) \\text{ such that} \\\\\n\\lambda\\Big[g'(\\lambda)\\Big]^2 &= c. \\\\\n\\text{A great candidate would be } c &= 1, \\text{such that} \\\\\n[g'(\\lambda)\\Big]^2 &= \\frac{1}{\\lambda} \\\\\ng'(\\lambda) &= \\sqrt{\\frac{1}{\\lambda}} = \\lambda^{-1/2} \\\\\ng(\\lambda) &= \\int \\lambda^{-1/2} ~d\\lambda \\\\\n&= \\frac{1}{1/2} \\lambda^{1/2} \\\\\n&= 2\\sqrt\\lambda \\\\\n\\text{Using the Delta method to } &\\text{confirm, we have} \\\\\n\\sqrt n\\Big(g(X_n) - g(\\lambda)\\Big) &\\xrightarrow{D} N\\Big(0, \\lambda\\Big[g'(\\lambda)\\Big]^2\\Big) \\\\\n\\sqrt n\\Big(2\\sqrt{X_n} - 2\\sqrt{\\lambda}\\Big) &\\xrightarrow{D} N\\Big(0, \\lambda\\Big[g'(\\lambda)\\Big]^2\\Big) \\\\\n&\\xrightarrow{D} N\\Big(0, \\lambda\\Big[\\frac{1}{\\sqrt \\lambda}\\Big]^2\\Big) \\\\\n&\\xrightarrow{D} N\\Big(0, \\lambda\\frac{1}{\\lambda}\\Big) \\\\\n&\\xrightarrow{D} N\\Big(0, 1\\Big) \\\\\n\\end{split}\n\\]"
  },
  {
    "objectID": "week3.html",
    "href": "week3.html",
    "title": "3  Asymptotics of maximum likelihood estimators",
    "section": "",
    "text": "As \\(n \\rightarrow \\infty\\), \\[\\sqrt{n}(\\hat \\theta - \\theta_0) \\xrightarrow{D} N\\Big(0, I_1(\\theta_0)^{-1}\\Big)\\]"
  },
  {
    "objectID": "week3.html#asymptotic-variance",
    "href": "week3.html#asymptotic-variance",
    "title": "3  Asymptotics of maximum likelihood estimators",
    "section": "3.2 Asymptotic variance",
    "text": "3.2 Asymptotic variance\nIf a sequence of estimators \\(\\pmb W_n\\) of \\(\\pmb \\tau(\\pmb \\theta)\\) satisfies\n\\[\\sqrt{n}(\\pmb W_n - \\pmb\\tau(\\pmb \\theta)) \\xrightarrow{D} N(\\pmb 0, \\pmb V(\\pmb \\theta)),\\]\nthen \\(\\pmb V(\\pmb\\theta)\\) is called the asymptotic variance of the sequence \\(\\pmb W_n\\) in \\(\\pmb\\tau(\\pmb \\theta)\\)\n\\[\n\\text{Var}(\\pmb W_n) \\approx \\pmb V(\\pmb \\theta)/n\n\\]\nFor scalar \\(W_n\\) and \\(\\tau\\) this becomes\n\\[\n\\begin{split}\n\\text{if } \\sqrt{n}( W_n - \\tau(\\pmb \\theta)) \\xrightarrow{D} N( 0,  V(\\pmb \\theta)), &\\text{ then, for any } \\epsilon > 0 \\\\\n\\lim_{n \\rightarrow \\infty} P\\Bigg( \\sqrt{n}\\Big( W_n - \\tau(\\pmb \\theta)\\Big) \\le \\epsilon \\Bigg) &= \\Phi\\Bigg(\\frac{\\epsilon}{\\sqrt{V(\\pmb \\theta)}}\\Bigg)\n\\end{split}\n\\]"
  },
  {
    "objectID": "week3.html#asymptotic-efficiency",
    "href": "week3.html#asymptotic-efficiency",
    "title": "3  Asymptotics of maximum likelihood estimators",
    "section": "3.3 Asymptotic efficiency",
    "text": "3.3 Asymptotic efficiency\nA sequence of estimators \\(\\pmb W_n\\) is called asymptotically efficient for a parameter \\(\\pmb \\tau(\\pmb \\theta)\\) if the asymptotic variance of \\(\\pmb W_n\\) achieves the Cramer-Rao bound. That is,\n\\[\n\\begin{split}\n\\sqrt{n}(\\pmb W_n - \\pmb\\tau(\\pmb \\theta)) &\\xrightarrow{D} N(\\pmb 0, \\pmb V(\\pmb \\theta)) \\\\\n\\text{ with } \\pmb V(\\pmb \\theta) = \\Big(\\pmb \\tau'&(\\pmb \\theta)\\Big)^T \\Big( \\pmb I_1(\\pmb\\theta) \\Big)^{-1} \\pmb \\tau'(\\pmb \\theta)\n\\end{split}\n\\]\nThrough the Delta method,\n\\[\n\\sqrt{n} \\Big(\\tau(\\hat \\theta_n) - \\tau(\\theta) \\Big) \\xrightarrow{D} N\\Big(0, I_1(\\theta)^{-1} [\\tau'(\\theta)]^2 \\Big)\n\\]"
  },
  {
    "objectID": "week3.html#asymptotic-relative-efficiency",
    "href": "week3.html#asymptotic-relative-efficiency",
    "title": "3  Asymptotics of maximum likelihood estimators",
    "section": "3.4 Asymptotic relative efficiency",
    "text": "3.4 Asymptotic relative efficiency\nLet \\(\\tau(\\pmb \\theta)\\) be a scalar function of \\(\\pmb \\theta\\). If two sequences of estimators \\(W_n\\) and \\(U_n\\) satisfy\n\\[\n\\begin{split}\n\\sqrt{n}( W_n - \\tau(\\pmb \\theta)) \\xrightarrow{D} N( 0,  V_W(\\pmb \\theta)) \\\\\n\\sqrt{n}( U_n - \\tau(\\pmb \\theta)) \\xrightarrow{D} N( 0,  V_U(\\pmb \\theta)),\n\\end{split}\n\\]\nthen the asymptotic relative efficiency (ARE) of \\(U_n\\) with respect to \\(W_n\\) is\n\\[\n\\text{ARE}(U_n, W_n) = \\frac{V_W(\\pmb \\theta)}{V_U(\\pmb \\theta)}\n\\]"
  },
  {
    "objectID": "week3.html#example-ntheta-theta",
    "href": "week3.html#example-ntheta-theta",
    "title": "3  Asymptotics of maximum likelihood estimators",
    "section": "3.5 Example: N(\\(\\theta, \\theta\\))",
    "text": "3.5 Example: N(\\(\\theta, \\theta\\))\nThe MLE of \\(\\theta\\) based on a set of \\(n\\) iid \\(X_i \\sim N(\\theta,\\theta)\\) measurements is given by\n\\[\\displaystyle \\hat \\theta_{\\text{MLE}} = \\frac{1}{2}\\Big(-1 + \\sqrt{1 + 4\\bar Y_n}\\Big), \\text{ with } Y_i = X_i^2\\]\nShow that \\(\\hat \\theta_{\\text{MLE}}\\) is consistent for \\(\\theta\\). By the weak law of large numbers,\n\\[\n\\begin{split}\n\\bar Y_n &\\xrightarrow{P} E[Y_i] \\\\\nE[Y_i] &= E[X_i^2] \\\\\n&= \\text{Var}[X_i] + (E[X_i])^2 \\\\\n&= \\theta + \\theta^2. \\text{ Thus, } \\\\\n\\bar Y_n &\\xrightarrow{P} \\theta + \\theta^2 \\\\\nh(\\bar Y_n) &\\xrightarrow{P} h(\\theta + \\theta^2) \\\\\n\\frac{1}{2}\\Big(-1 + \\sqrt{1 + 4\\bar Y_n}\\Big)  &\\xrightarrow{P} \\frac{1}{2}\\Big(-1 + \\sqrt{1 + 4(\\theta + \\theta^2)}\\Big) \\\\\n&= \\frac{1}{2}\\Big(-1 + \\sqrt{4\\theta^2 + 4\\theta + 1}\\Big) \\\\\n&= \\frac{1}{2}\\Bigg(-1 + \\sqrt{4\\Big(\\theta^2 + \\theta + \\frac{1}{4}\\Big)}\\Bigg) \\\\\n&= \\frac{1}{2}\\Bigg(-1 + \\sqrt{4\\Big(\\theta + \\frac{1}{2}\\Big)^2}\\Bigg) \\\\\n&= \\frac{1}{2}\\Big(-1 + 2 \\Big[\\theta + \\frac{1}{2}\\Big]\\Big) \\\\\n&=\\frac{1}{2} (-1 + 2\\theta + 1)\\\\\n&= \\theta \\\\\n\\text{Thus, } \\hat \\theta_{\\text{MLE}}  &\\xrightarrow{P}  \\theta\n\\end{split}\n\\]\nShow that \\(\\hat \\theta_{\\text{MLE}}\\) is asymptotically normal. Use the following fact:\n\\[\n\\begin{split}\n\\text{When } X \\sim N(\\mu, \\sigma^2), E[X^4] &= \\mu^4 + 6\\mu^2\\sigma^2 + 3\\sigma^4. \\text{ Thus, } \\\\\n\\text{When } X \\sim N(\\theta, \\theta), E[X^4] &= \\theta^4 + 6\\theta^2\\theta + 3\\theta^2 \\\\\n&= \\theta^4 + 6\\theta^3 + 3\\theta^2\n\\end{split}\n\\]\n\\[\n\\begin{split}\n\\text{Var}[Y_i]\n&=E[Y_i^2] - \\Big(E[Y_i]\\Big)^2 \\\\\n&= E[X_i^4] - (\\theta + \\theta^2)^2 \\\\\n&=\\theta^4 + 6\\theta^2\\theta + 3\\theta^2 - (\\theta + \\theta^2)^2\\\\\n&= \\theta^4 + 6\\theta^3 + 3\\theta^2 - [\\theta^2 +2\\theta^3 + \\theta^4 ] \\\\\n&= 4\\theta^3+2\\theta^2 \\\\\n&= 2\\theta^2(2\\theta + 1) \\\\\n&\\text{Using the CLT, } \\\\\n\\sqrt{n}(\\bar Y_n - \\mu) &\\xrightarrow{D} N\\Big(0, 2\\theta^2(2\\theta + 1)\\Big) \\\\\n&\\text{Using the Delta method, } \\\\\n\\sqrt{n}\\Big(g(\\bar Y_n) - g(\\mu)\\Big) &\\xrightarrow{D} N\\Big(0, 2\\theta^2(2\\theta + 1)\\Big[g'(\\mu) \\Big]^2 \\Big) \\\\\n\\text{Let } g(\\mu) &=\\frac{1}{2}\\Big(-1 + \\sqrt{1 + 4\\mu}\\Big). \\text{ Then, } \\\\\ng'(\\mu) &=\\frac{1}{2} \\cdot \\frac{1}{2}(1 + 4\\mu)^{-1/2} \\cdot 4 \\\\\n&=(1 + 4\\mu)^{-1/2} \\\\\n&= \\frac{1}{\\sqrt{1 + 4\\mu}} \\\\\n&=\\frac{1}{\\sqrt{1 + 4 (\\theta + \\theta^2)}} \\\\\n&=\\frac{1}{\\sqrt{4\\theta^2 + 4\\theta + 1}}  \\\\\n&=\\frac{1}{\\sqrt{4 (\\theta^2+ \\theta + \\frac{1}{4}) }} \\\\\n&=\\frac{1}{\\sqrt{4 (\\theta + \\frac{1}{2})^2 }} \\\\\n&=\\frac{1}{{2 (\\theta + \\frac{1}{2}) }} \\\\\n&=\\frac{1}{{2\\theta + 1 }} \\\\\n[g'(\\mu)]^2 &=\\frac{1}{{(2\\theta + 1)^2 }}. \\text{ Thus, } \\\\\n\\sqrt{n}\\Big(g(\\bar Y_n) - g(\\mu)\\Big) &\\xrightarrow{D} N\\Bigg(0, \\frac{2\\theta^2(2\\theta + 1)}{(2\\theta + 1)^2} \\Bigg)  \\\\\n&= N\\Bigg(0, \\frac{2\\theta^2}{2\\theta + 1} \\Bigg)\n\\end{split}\n\\]\nConfirm the asymptotic variance by obtaining the Fisher information matrix. We have\n\\[\n\\begin{split}\nX_i &\\sim N(\\theta,\\theta)\\\\\nf_{X_i}(x_i) &= \\frac{1}{\\sqrt{2\\pi}} \\theta^{-1/2} \\exp\\Bigg( - \\frac{(x_i - \\theta)^2}{2\\theta} \\Bigg) \\\\\n\\ln f_{X_i}(X_i;\\theta) &= \\ln\\frac{1}{\\sqrt{2\\pi}} -\\frac{1}{2}\\ln\\theta - \\frac{1}{2}\\Bigg[\\frac{(X_i - \\theta)^2}{\\theta}\\Bigg]  \\\\\n&=\\ln\\frac{1}{\\sqrt{2\\pi}} -\\frac{1}{2}\\ln\\theta - \\frac{1}{2}\\Bigg[\\frac{X_i^2 - 2X_i\\theta + \\theta^2}{\\theta}\\Bigg]\\\\\n&=\\ln\\frac{1}{\\sqrt{2\\pi}} -\\frac{1}{2}\\ln\\theta - \\frac{1}{2}\\Bigg[\\frac{X_i^2 }{\\theta} - 2X_i+ \\theta \\Bigg]  \\\\\nS_i(\\theta) = \\frac{\\partial}{\\partial \\theta} \\ln f_{X_i}(X_i;\\theta) &= -\\frac{1}{2\\theta} - \\frac{1}{2}\\Big[ -\\frac{X_i^2}{\\theta^2} +1\\Big] \\\\\n&=-\\frac{1}{2\\theta} +  \\frac{X_i^2}{2\\theta^2} -\\frac{1}{2}\\\\\nI_1(\\theta) &= -E\\Big[\\frac{\\partial}{\\partial \\theta}S_i(\\theta)\\Big] \\\\\n&= -E\\Big[\\frac{\\partial}{\\partial \\theta} -\\frac{1}{2}\\theta^{-1} + \\frac{1}{2}X_i^2 \\cdot\\theta^{-2} - \\frac{1}{2} \\Big] \\\\\n&= -E\\Big[-\\frac{1}{2} \\cdot-\\theta^{-2} +\\frac{1}{2}X_i^2 \\cdot -2\\theta^{-3}\\Big] \\\\\n&=-E\\Big[\\frac{1}{2\\theta^2} - \\frac{X_i^2}{\\theta^3} \\Big] \\\\\n&=E\\Big[ \\frac{X_i^2}{\\theta^3} -\\frac{1}{2\\theta^2} \\Big] \\\\\n&=\\frac{1}{\\theta^3}E[{X_i^2}] -\\frac{1}{2\\theta^2}  \\\\\n&= \\frac{\\theta + \\theta^2}{\\theta^3} -\\frac{1}{2\\theta^2}  \\\\\n&= \\frac{1}{\\theta^2} + \\frac{1}{\\theta} -\\frac{1}{2\\theta^2}  \\\\\n&= \\frac{1}{2\\theta^2}  + \\frac{1}{\\theta} \\\\\n&= \\frac{1}{2\\theta^2}  + \\frac{2\\theta}{\\theta \\cdot 2\\theta} \\\\\n&= \\frac{1 + 2\\theta}{2\\theta^2}  \\\\\n\\text{Var}[\\hat \\theta_{\\text{MLE}}] &\\approx \\frac{1}{n} \\cdot \\frac{2\\theta^2}{1 + 2\\theta} \\\\\n\\end{split}\n\\]\n\\[\n\\begin{split}\n\\text{Earlier, } &\\text{we obtained} \\\\\n\\sqrt{n}(\\hat \\theta_{\\text{MLE}} - \\theta_0) &\\xrightarrow{D} N\\Bigg(0, \\frac{2\\theta^2}{1 + 2\\theta}\\Bigg) \\\\\n\\text{Which is } &\\text{equivalent to} \\\\\n\\hat \\theta_{\\text{MLE}} - \\theta_0 &\\xrightarrow{D} \\frac{1}{\\sqrt n} N\\Bigg(0, \\frac{2\\theta^2}{1 + 2\\theta}\\Bigg) \\\\\n\\hat \\theta_{\\text{MLE}} - \\theta_0 &\\xrightarrow{D}N\\Bigg(0, \\frac{1}{n} \\cdot \\frac{2\\theta^2}{1 + 2\\theta}\\Bigg) \\\\\n\\hat \\theta_{\\text{MLE}} &\\xrightarrow{D}N\\Bigg(\\theta_0 , \\frac{1}{n} \\cdot \\frac{2\\theta^2}{1 + 2\\theta}\\Bigg) \\\\\n\\end{split}\n\\]"
  },
  {
    "objectID": "week3.html#example-convergence-of-a-transformation-of-the-uniform",
    "href": "week3.html#example-convergence-of-a-transformation-of-the-uniform",
    "title": "3  Asymptotics of maximum likelihood estimators",
    "section": "3.6 Example: Convergence of a transformation of the Uniform",
    "text": "3.6 Example: Convergence of a transformation of the Uniform\nLet \\(U_1, U_2, ...\\) be iid observations from a uniform distribution over the unit interval \\([0,1]\\) and define \\(\\displaystyle Y_n = \\Bigg(\\prod_{i = 1}^n U_i \\Bigg)^{-1/n}\\). Recall that \\(\\displaystyle \\prod_{i = 1}^n x_i = \\exp\\Bigg(\\sum_{i = 1}^n \\ln x_i\\Bigg)\\) and show that \\(Y_n\\) converges in probability to \\(e\\). We have\n\\[\n\\begin{split}\nf_U(u) &= 1 \\\\\nY_n &= \\Bigg(\\prod_{i = 1}^n U_i \\Bigg)^{-1/n} \\\\\n&= e^{\\Big(\\displaystyle \\sum_{i = 1}^n \\ln U_i\\Big)^{-1/n}} \\\\\n&= e^{\\Big(\\displaystyle -\\frac{1}{n} \\sum_{i = 1}^n \\ln U_i\\Big)} \\\\\n\\ln U_i &\\xrightarrow{P} E\\Big[\\ln U_i\\Big] \\\\\nE\\Big[ \\ln U_i\\Big] &= \\int_0^1 \\ln U_i ~f_U(u) ~du \\\\\n&= \\int_0^1 \\ln U_i  ~du\\\\\n&= u\\ln(u) - u \\Bigg|_0^1 \\\\\n&=\\ln(1) - 1 \\\\\n&=-1 \\\\\n\\ln U_i &\\xrightarrow{P} -1 \\\\\n\\text{By continuous} &\\text{ mapping, } \\\\\nh(\\ln U_i) &\\xrightarrow{P} h(-1) \\\\\n\\exp{\\Big(\\displaystyle -\\frac{1}{n} \\sum_{i = 1}^n \\ln U_i\\Big)} &\\xrightarrow{P} \\exp{\\Big(\\displaystyle -\\frac{1}{n} \\sum_{i = 1}^n -1\\Big)} \\\\\n&= e^{n/n} \\\\\n&= e\n\\end{split}\n\\]"
  },
  {
    "objectID": "week3.html#example-average-relative-efficiency",
    "href": "week3.html#example-average-relative-efficiency",
    "title": "3  Asymptotics of maximum likelihood estimators",
    "section": "3.7 Example: Average relative efficiency",
    "text": "3.7 Example: Average relative efficiency\nSuppose iid \\(X_i \\sim \\text{Poisson}(\\lambda)\\) and we are interested in estimating \\(P(X = 0) = e^{-\\lambda}\\). Derive two estimators; (1) based on the Poisson process and (2) by setting \\(Y_i = I(X_i = 0)\\), in which case we have iid \\(Y_i \\sim \\text{Bernoulli}(e^{-\\lambda})\\). Compare their efficiencies. We have\n\\[\n\\begin{split}\nE[X_i] &= \\text{Var}[X_i]  =\\lambda \\\\\n\\text{By } &\\text{WLLN, }  \\\\\n\\bar X_n &\\xrightarrow{P} \\lambda\\\\\n\\text{By  } &\\text{continuous mapping, } \\\\\nh(\\bar X_n) &\\xrightarrow{P} h(\\lambda) \\\\\ne^{-\\bar X_n} &\\xrightarrow{P} e^{- \\lambda} \\\\\n\\text{By } &\\text{CLT}, \\\\\n\\sqrt n(\\bar X_n - \\lambda) &\\xrightarrow{D} N(0,\\lambda)\\\\\n\\text{By } &\\text{Delta method}, \\\\\n\\sqrt n\\Big(h(\\bar X_n) - h(\\lambda)\\Big) &\\xrightarrow{D} N(0,\\lambda\\Big[h'(\\lambda)\\Big]^2) \\\\\n\\text{Let } h(\\lambda) &= e^{-\\lambda}. \\text{Then, } \\\\\nh'(\\lambda) &= -e^{-\\lambda} \\\\\n[h'(\\lambda)\\Big]^2 &= e^{-2\\lambda}. \\text{Thus, } \\\\\n\\sqrt n\\Big(h(\\bar X_n) - h(\\lambda)\\Big) &\\xrightarrow{D} N(0,\\lambda e^{-2\\lambda}) \\\\\n\\end{split}\n\\]\nIf, instead, we set \\(Y_i = I(X_i = 0)\\), then we have\n\\[\n\\begin{split}\n\\text{iid } Y_i &\\sim \\text{Bernoulli}(e^{-\\lambda}) \\\\\nE[Y_i] &= e^{-\\lambda} \\\\\n\\text{Var}[Y_i] &= e^{-\\lambda}(1-e^{-\\lambda}) \\\\\n\\text{By the} &\\text{ WLLN,} \\\\\n\\bar Y_n &\\xrightarrow{P} e^{-\\lambda} \\\\\n\\text{By the} &\\text{ CLT}, \\\\\n\\sqrt n(\\bar Y_n - e^{-\\lambda}) &\\xrightarrow{D} N\\Big(0,e^{-\\lambda}(1-e^{-\\lambda})\\Big)\\\\\n\\end{split}\n\\]\nThe average relative efficiency of \\(\\bar Y_n\\) vs. \\(e^{-\\bar X}\\) is given by\n\\[\n\\begin{split}\n\\text{ARE}(\\bar Y, e^{-\\bar X}) &= \\frac{\\lambda e^{-2\\lambda}}{e^{-\\lambda}(1-e^{-\\lambda})} \\\\\n&= \\frac{\\lambda e^{-\\lambda}}{1 - e^{-\\lambda}}\n\\end{split}\n\\]\nThis is a decreasing function. Thus, the Poisson estimator \\(e^{-\\bar X_n}\\) is more efficient than the Bernoulli estimator \\(\\bar Y_n\\)."
  },
  {
    "objectID": "week4.html",
    "href": "week4.html",
    "title": "4  Hypothesis testing",
    "section": "",
    "text": "A note on notation: \\(z_{\\alpha} = \\Phi^{-1}(1-\\alpha)\\). For example, when \\(\\alpha = 0.05, z_{\\alpha} \\approx 1.64\\). We have \\(P(Z \\le z_\\alpha) = 1- \\alpha\\) and \\(P(Z > z_\\alpha) = \\alpha\\).\n\n\\(z_\\alpha\\) satisfies \\(P(Z > z_\\alpha) = \\alpha\\), where \\(Z \\sim N(0,1)\\)\n\\(-z_\\alpha = z_{1 - \\alpha}\\) satisfies \\(P(Z \\le z_\\alpha) = \\alpha\\), where \\(Z \\sim N(0,1)\\)\n\\(t_{n-1,\\alpha}\\) satisfies \\(P(T_{n-1} >t_{n-1,\\alpha}) = \\alpha\\), where \\(T_{n-1} \\sim t_{n=1}\\)\n\\(-t_{n-1,\\alpha} = t_{n-1,1 -\\alpha}\\) satisfies \\(P(T_{n-1} \\le t_{n-1,\\alpha}) = \\alpha\\), where \\(T_{n-1} \\sim t_{n-1}\\)\n\\(\\chi^2_{r, \\alpha}\\) satisfies \\(P(\\chi^2_r > \\chi^2_{r,\\alpha}) = \\alpha\\), where \\(\\chi^2_p\\) is a chi-squared random variable with p degrees of freedom.\n\\(-\\chi^2_{r, \\alpha} = \\chi^2_{r, 1-\\alpha}\\) satisfies \\(P(\\chi^2_r \\le \\chi^2_{r,\\alpha}) = \\alpha\\), where \\(\\chi^2_p\\) is a chi-squared random variable with p degrees of freedom."
  },
  {
    "objectID": "week4.html#wald-score-and-lr-test-statistic-simple-null",
    "href": "week4.html#wald-score-and-lr-test-statistic-simple-null",
    "title": "4  Hypothesis testing",
    "section": "4.2 Wald, Score, and LR test statistic: Simple null",
    "text": "4.2 Wald, Score, and LR test statistic: Simple null\nSuppose we want to test \\(H_0: \\theta = \\theta_0\\) vs. \\(H_a: \\theta \\ne \\theta_0\\). We can use the following test statistics (which are asymptotically equivalent):\n\\[\n\\begin{split}\n\\text{Wald: }T_W &= \\frac{(\\hat \\theta_{\\text{MLE}} -\\theta_0)^2}{\\Big\\{nI_1(\\hat \\theta_{\\text MLE})\\Big\\}^{-1}} \\\\\n&= \\frac{(\\hat \\theta_{\\text{MLE}} -\\theta_0)^2}{\\text{Var}(\\hat \\theta_{\\text{MLE}})} \\\\\n&= nI_1(\\hat \\theta_{\\text{MLE}})(\\hat \\theta_{\\text{MLE}} -\\theta_0)^2   \\\\\n\\text{Score: } T_S &= \\frac{\\Big[S_n(\\theta_0)\\Big]^2}{nI_1(\\theta_0)}\\\\\n\\text{Likelihood ratio: } T_{LR} &= -2\\Big\\{\\ell(\\theta_0) - \\ell(\\hat \\theta_{\\text{MLE}}) \\Big\\} \\\\\n&=  2\\Big\\{\\ell(\\hat \\theta_{\\text{MLE}}) - \\ell(\\theta_0)  \\Big\\} \\\\\n&= -2\\ln\\Bigg\\{\\frac{L(\\theta_0)}{L(\\hat \\theta_{\\text{MLE}})} \\Bigg\\} \\\\\n&=  2\\ln\\Bigg\\{\\frac{ L(\\hat \\theta_{\\text{MLE}})}{L(\\theta_0)} \\Bigg\\}\n\\end{split}\n\\]\n\n4.2.1 Example: Test statistics for Normal(\\(\\theta,1\\))\nLet iid \\(Y_i \\sim N(\\theta,1)\\) and \\(H_0: \\theta = \\theta_0\\). Obtain the three test statistics.\n\\[\n\\begin{split}\nL(\\theta) &= \\prod_{i = 1}^n \\frac{1}{\\sqrt{2\\pi}} \\exp\\Bigg(-\\frac{1}{2}(Y_i - \\theta)^2 \\Bigg) \\\\\n&= (2\\pi)^{-n/2} \\exp\\Big(-\\frac{1}{2} \\sum_{i = 1}^n (Y_i - \\theta)^2\\Big) \\\\\n\\ell(\\theta) &= -n/2\\ln(2\\pi)  -\\frac{1}{2} \\sum_{i = 1}^n (Y_i - \\theta)^2 \\\\\nS_n(\\theta) &= \\frac{d}{d\\theta} \\ell(\\theta) \\\\\n&= \\sum_{i = 1}^n Y_i - \\theta = 0 \\\\\n\\Rightarrow n\\theta &= \\sum_{i = 1}^n Y_i \\\\\n\\hat \\theta_{\\text{MLE}} &= \\frac{1}{n} \\sum_{i = 1}^n Y_i = \\bar Y \\\\\nI_1(\\theta) &= - E\\Big[\\frac{\\partial}{\\partial\\theta}  S_i(\\theta) \\Big] \\\\\n&= - E\\Big[\\frac{\\partial}{\\partial\\theta}  Y_i - \\theta \\Big] \\\\\n&= -E[-1] \\\\\n&=1 \\\\\nT_W &= (\\hat \\theta_{\\text{MLE}} - \\theta_0)^2 \\cdot nI_1(\\hat \\theta_{\\text{MLE}}) \\\\\n&= n(\\bar Y - \\theta_0)^2 \\\\\n\\ell(\\theta_0) &=  -n/2\\ln(2\\pi)  -\\frac{1}{2} \\sum_{i = 1}^n (Y_i - \\theta_0)^2 \\\\\n\\ell(\\hat \\theta_{\\text{MLE}}) &=  -n/2\\ln(2\\pi)  -\\frac{1}{2} \\sum_{i = 1}^n (Y_i - \\bar Y)^2 \\\\\nT_{LR} &= 2\\Big\\{\\ell(\\hat \\theta_{\\text{MLE}}) - \\ell(\\theta_0)  \\Big\\} \\\\\n&= 2\\Big\\{-n/2\\ln(2\\pi)  -\\frac{1}{2} \\sum_{i = 1}^n (Y_i - \\bar Y)^2 - \\Big( -n/2\\ln(2\\pi)  -\\frac{1}{2} \\sum_{i = 1}^n (Y_i - \\theta_0)^2\\Big)  \\Big\\} \\\\\n&= 2\\Big\\{-\\frac{1}{2} \\sum_{i = 1}^n (Y_i - \\bar Y)^2 + \\frac{1}{2} \\sum_{i = 1}^n (Y_i - \\theta_0)^2 \\Big\\} \\\\\n&= 2\\Big\\{\\frac{1}{2} \\sum_{i = 1}^n (Y_i - \\theta_0)^2 -\\frac{1}{2} \\sum_{i = 1}^n (Y_i - \\bar Y)^2 \\Big\\} \\\\\n&= 2\\Big\\{\\frac{1}{2} \\sum_{i = 1}^n (Y_i - \\theta_0)^2 -(Y_i - \\bar Y)^2 \\Big\\} \\\\\n&=  \\sum_{i = 1}^n (Y_i - \\theta_0)^2 -(Y_i - \\bar Y)^2 \\\\\n&= \\sum_{i = 1}^n Y_i^2 - 2\\theta_0Y_i + \\theta_0^2 - (Y_i^2 - 2\\bar YY_i + \\bar Y^2) \\\\\n&=\\sum_{i = 1}^n - 2\\theta_0Y_i + \\theta_0^2 + 2\\bar YY_i - \\bar Y^2 \\\\\n&= -2\\theta_0\\sum_{i = 1}^n Y_i +n\\theta_0^2 + 2\\bar Y\\sum_{i=1}^n Y_i -n\\bar Y ^2 \\\\\n&= -2\\theta_0n\\bar Y +n\\theta_0^2 + 2\\bar Yn\\bar Y -n\\bar Y ^2 \\\\\n&= n\\Big(-2\\theta_0\\bar Y +\\theta_0^2 + 2\\bar Y^2 -\\bar Y ^2 \\Big) \\\\\n&= n\\Big( \\bar Y ^2 -2\\theta_0\\bar Y +\\theta_0^2  \\Big) \\\\\n&= n\\Big(\\bar Y - \\theta_0 \\Big)^2 \\\\\nT_S &= \\frac{\\Big[S_n(\\theta_0)\\Big]^2}{nI_1(\\theta_0)}\\\\\n&= \\frac{\\Big[S_n(\\theta_0)\\Big]^2}{n} \\\\\n&= \\frac{1}{n} \\Bigg({\\sum_{i = 1}^n Y_i - \\theta_0}\\Bigg)^2 \\\\\n&= \\frac{1}{n} \\Big(n\\bar Y - n\\theta_0 \\Big)^2 \\\\\n&= \\frac{1}{n} \\Big(n(\\bar Y - \\theta_0) \\Big)^2 \\\\\n&= \\frac{n^2}{n} (\\bar Y - \\theta_0)^2 \\\\\n&= n(\\bar Y - \\theta_0)^2\n\\end{split}\n\\]\n\n\n4.2.2 Example: Test statistics for Normal(\\(\\mu, \\sigma^2\\)) with \\(\\sigma^2\\) known\nNow, let iid \\(Y_i \\sim N(\\mu,\\sigma^2)\\) with \\(\\sigma^2\\) known and \\(H_0: \\mu = \\mu_0\\). Obtain the three test statistics.\n\\[\n\\begin{split}\n\\ell(\\mu) &= -n/2\\ln(2\\pi\\sigma^2)  -\\frac{1}{2\\sigma^2} \\sum_{i = 1}^n (Y_i - \\mu)^2 \\\\\nS_n(\\mu) &= \\frac{1}{\\sigma^2}\\sum_{i = 1}^n Y_i - \\mu = 0 \\\\\n\\hat \\mu_{\\text{MLE}} &= \\frac{1}{n} \\sum_{i = 1}^n Y_i = \\bar Y \\\\\nI_1(\\mu) &= - E\\Big[\\frac{\\partial}{\\partial\\mu}  S_i(\\mu) \\Big] \\\\\n&= - E\\Big[\\frac{\\partial}{\\partial\\mu}  \\frac{1}{\\sigma^2} \\Big(Y_i - \\mu \\Big) \\Big] \\\\\n&= -E\\Big[-\\frac{1}{\\sigma^2}\\Big] \\\\\n&=\\frac{1}{\\sigma^2}\\\\\nT_W &= (\\hat \\mu_{\\text{MLE}} - \\mu_0)^2 \\cdot nI_1(\\hat \\mu_{\\text{MLE}}) \\\\\n&= \\frac{n(\\bar Y - \\mu_0)^2}{\\sigma^2} \\\\\n\\ell(\\mu_0) &= -n/2\\ln(2\\pi\\sigma^2)  -\\frac{1}{2\\sigma^2} \\sum_{i = 1}^n (Y_i - \\mu_0)^2 \\\\\n\\ell(\\hat \\mu_{\\text{MLE}}) &= -n/2\\ln(2\\pi\\sigma^2)  -\\frac{1}{2\\sigma^2} \\sum_{i = 1}^n (Y_i - \\bar Y)^2 \\\\\nT_{LR} &= 2\\Big\\{\\ell(\\hat \\mu_{\\text{MLE}}) - \\ell(\\mu_0)  \\Big\\} \\\\\n&= 2\\Big\\{-n/2\\ln(2\\pi\\sigma^2)  -\\frac{1}{2\\sigma^2} \\sum_{i = 1}^n (Y_i - \\bar Y)^2 - \\\\ &~~~~~~~~~\\Big(-n/2\\ln(2\\pi\\sigma^2)  -\\frac{1}{2\\sigma^2} \\sum_{i = 1}^n (Y_i - \\mu_0)^2 \\Big)  \\Big\\} \\\\\n&= 2\\Big\\{\\frac{1}{2\\sigma^2} \\sum_{i = 1}^n (Y_i - \\mu_0)^2 -(Y_i - \\bar Y)^2 \\Big\\} \\\\\n&=  \\frac{1}{\\sigma^2}\\sum_{i = 1}^n (Y_i - \\mu_0)^2 -(Y_i - \\bar Y)^2 \\\\\n&= \\frac{n(\\bar Y - \\mu_0)^2}{\\sigma^2} \\\\\nT_S &= \\frac{\\Big[S_n(\\mu_0)\\Big]^2}{nI_1(\\mu_0)}\\\\\n&=  \\Big(\\frac{1}{\\sigma^2}\\sum_{i = 1}^n Y_i - \\mu_0 \\Big)^2 \\frac{1}{nI_1(\\mu_0)} \\\\\n&=  \\Big(\\frac{1}{\\sigma^2}\\sum_{i = 1}^n Y_i - \\mu_0 \\Big)^2 \\frac{\\sigma^2}{n} \\\\\n&=  \\frac{\\sigma^2}{n\\sigma^4}\\Big(\\sum_{i = 1}^n Y_i - \\mu_0 \\Big)^2 \\\\\n&=  \\frac{1}{n\\sigma^2}\\Big(\\sum_{i = 1}^n Y_i - \\mu_0 \\Big)^2 \\\\\n&=  \\frac{n^2}{n\\sigma^2}\\Big(Y_i - \\mu_0 \\Big)^2 \\\\\n&= \\frac{n(\\bar Y - \\mu_0)^2}{\\sigma^2} \\\\\n\\end{split}\n\\]\nNow, say we want to test \\(H_0: \\mu \\le \\mu_0\\) vs. \\(H_a: \\mu > \\mu_0\\). We have obtained the T-statistic \\[T_W = T_S = T_{LR} =  \\frac{n(\\bar Y - \\mu_0)^2}{\\sigma^2}\\]\nWe reject \\(H_0\\) if \\(T > \\chi^2_{1,\\alpha}\\). For example, when \\(\\alpha = 0.05, \\chi^2_{1,0.05} \\approx 3.84\\). This is equivalent to rejecting \\(H_0\\) if \\(Z > z_{\\alpha}\\), where \\(z_\\alpha\\) satisfies \\(P(Z \\ge z_\\alpha) = \\alpha\\) and \\[Z = \\sqrt T = \\frac{\\bar Y - \\mu_0}{\\sigma/\\sqrt n}\\]"
  },
  {
    "objectID": "week4.html#wald-test-simple-null-h_0-theta-theta_0",
    "href": "week4.html#wald-test-simple-null-h_0-theta-theta_0",
    "title": "4  Hypothesis testing",
    "section": "4.3 Wald test: Simple null \\((H_0: \\theta = \\theta_0)\\)",
    "text": "4.3 Wald test: Simple null \\((H_0: \\theta = \\theta_0)\\)\nLet \\(Y_i\\) be iid with density \\(f(y | \\theta)\\). Consider a simple null hypothesis \\(H_0: \\theta = \\theta_0\\). Suppose that \\(\\hat \\theta_{\\text{MLE}}\\) is a consistent root of the likelihood equation. Then,\n\\[\n\\begin{split}\n\\sqrt{n}(\\hat \\theta_{\\text{MLE}} - \\theta) &\\xrightarrow{D} N\\Bigg(0, \\frac{1}{I_1(\\theta)}\\Bigg) \\\\\nZ_W =\\sqrt{nI_1(\\theta)}(\\hat \\theta_{\\text{MLE}} - \\theta) &\\xrightarrow{D} N(0, 1)\n\\end{split}\n\\]\nIf \\(nI_1(\\hat \\theta_{\\text{MLE}})/nI_1(\\theta_0)\\) converges in probability to 1 as \\(n \\rightarrow \\infty\\), then \\[T_W = Z^2_W =nI_1(\\theta)(\\hat \\theta_{\\text{MLE}} -\\theta)^2 \\xrightarrow{D} \\chi_1^2 \\text{ under } H_0,\\] and we reject the null hypothesis is \\(T_W > \\chi^2_{1,\\alpha}\\), where \\(\\chi^2_{1,\\alpha} = Q(1 - \\alpha)\\) and \\(Q\\) is the quantile function associated with the \\(\\chi^2\\) distribution. For example, when \\(\\alpha = 0.05, \\chi^2_{1,0.05} \\approx 3.84\\).\n\n4.3.1 Multi-parameter formulation: Simple null (\\(H_0: \\pmb \\theta = \\pmb \\theta_0\\))\nThe more general vector-formulation of the Wald statistic (including a possible alternative formulation) is given by\n\\[\n\\begin{split}\n&\\hat{\\pmb\\theta}_{\\text{MLE}} \\xrightarrow{D} N\\Big({\\pmb\\theta}_0, \\Big[\\pmb I_T(\\pmb \\theta_0) \\Big]^{-1}\\Big) \\\\\n&\\sqrt{\\pmb I_T(\\pmb \\theta_0)} (\\hat{\\pmb\\theta}_{\\text{MLE}} - \\pmb \\theta_0) \\xrightarrow{D} N\\Big({\\pmb\\theta}_0,  \\mathbb{1}_b \\Big) \\\\\n&(1) ~T_W = (\\hat{\\pmb\\theta}_{\\text{MLE}} - {\\pmb\\theta}_0)^T \\{\\pmb I_T(\\hat {\\pmb \\theta}_{\\text{MLE}})\\}(\\hat{\\pmb\\theta}_{\\text{MLE}} - {\\pmb\\theta}_0) \\xrightarrow{D} \\chi^2_b \\\\\n&\\text{Reject if }   T_W > \\chi^2_{b,\\alpha} \\\\\n&(2) ~T_W' = (\\hat{\\pmb\\theta}_{\\text{MLE}} - {\\pmb\\theta}_0)^T \\{\\pmb I_T({\\pmb \\theta}_0)\\}(\\hat{\\pmb\\theta}_{\\text{MLE}} - {\\pmb\\theta}_0) \\xrightarrow{D} \\chi^2_b \\\\\n&\\text{Reject if }   T'_W > \\chi^2_{b,\\alpha}\n\\end{split}\n\\]\n\n\n4.3.2 \\(H_0: \\theta = \\theta_0\\) vs. \\(H_a: \\theta \\ne \\theta_0\\)\nThere are two possible forms for the Wald test, depending on whether we plug \\(\\theta_0\\) or \\(\\hat \\theta_{\\text{MLE}}\\) into the Fisher information matrix. If \\(nI_1(\\hat \\theta_{\\text{MLE}})/nI_1(\\theta_0)\\) converges in probability to 1 as \\(n \\rightarrow \\infty\\), we have the following rejection rules at significance level \\(\\alpha\\) for a two-sided test against \\(Ha: \\theta \\ne \\theta_0\\):\n\\[\n\\begin{split}\n&\\text{First approach (plug in } \\hat\\theta_{\\text{MLE}}): \\text{Under } H_0,\\\\\n&T_W = nI_1(\\hat \\theta_{\\text{MLE}})(\\hat \\theta_{\\text{MLE}} -\\theta_0)^2 \\xrightarrow{D} \\chi_1^2  \\\\\n&Z_W = \\sqrt{nI_1(\\hat \\theta_{\\text{MLE}})}(\\hat \\theta_{\\text{MLE}} - \\theta_0) \\xrightarrow{D} N(0, 1) \\\\\n&\\text{Reject } H_0 \\text{ if, equivalently, } \\\\\n&(1) ~Z_W > z_{\\alpha/2}, \\text{ or } Z_W < -z_{\\alpha/2}~; \\\\\n&(2)~\\hat \\theta_{\\text{MLE}} < \\theta_0 -\\frac{z_{\\alpha/2}}{\\sqrt{nI_1(\\hat \\theta_{\\text{MLE}})}}, \\text{ or }  \\hat \\theta_{\\text{MLE}} > \\theta_0 +\\frac{z_{\\alpha/2}}{\\sqrt{nI_1(\\hat \\theta_{\\text{MLE}})}} \\\\\\\\\n&\\text{Second approach (plug in } \\theta_0): \\text{Under } H_0,\\\\\n&T_W' = nI_1( \\theta_0)(\\hat \\theta_{\\text{MLE}} -\\theta_0)^2 \\xrightarrow{D} \\chi_1^2  \\\\\n&Z_W' = \\sqrt{nI_1( \\theta_0)}(\\hat \\theta_{\\text{MLE}} - \\theta_0) \\xrightarrow{D} N(0, 1) \\\\\n&\\text{Reject } H_0 \\text{ if, equivalently, } \\\\\n&(1) ~Z'_W > z_{\\alpha/2}, \\text{ or } Z'_W < -z_{\\alpha/2}~; \\\\\n&(2)~\\hat \\theta_{\\text{MLE}} < \\theta_0 -\\frac{z_{\\alpha/2}}{\\sqrt{nI_1( \\theta_0)}}, \\text{ or }  \\hat \\theta_{\\text{MLE}} > \\theta_0 +\\frac{z_{\\alpha/2}}{\\sqrt{nI_1( \\theta_0)}} \\\\\n\\end{split}\n\\]\n\n\n4.3.3 \\(H_0: \\theta = \\theta_0\\) vs. \\(H_a: \\theta < \\theta_0\\)\nIf \\(nI_1(\\hat \\theta_{\\text{MLE}})/nI_1(\\theta_0)\\) converges in probability to 1 as \\(n \\rightarrow \\infty\\), we have, for a one-sided test against \\(H_a: \\theta < \\theta_0\\),\n\\[\n\\begin{split}\n&\\text{First approach (plug in } \\hat\\theta_{\\text{MLE}}): \\text{Under } H_0,\\\\\n&T_W = nI_1(\\hat \\theta_{\\text{MLE}})(\\hat \\theta_{\\text{MLE}} -\\theta_0)^2 \\xrightarrow{D} \\chi_1^2  \\\\\n&Z_W = \\sqrt{nI_1(\\hat \\theta_{\\text{MLE}})}(\\hat \\theta_{\\text{MLE}} - \\theta_0) \\xrightarrow{D} N(0, 1) \\\\\n&\\text{Reject } H_0 \\text{ if, equivalently, } \\\\\n&Z_W  = \\sqrt{nI_1(\\hat \\theta_{\\text{MLE}})}(\\hat \\theta_{\\text{MLE}} - \\theta_0) <- z_{\\alpha}, \\text{ or }\\\\\n&\\hat \\theta_{\\text{MLE}} < \\theta_0 -\\frac{z_{\\alpha}}{\\sqrt{nI_1(\\hat \\theta_{\\text{MLE}})}}\\\\\\\\\n&\\text{Second approach (plug in } \\theta_0): \\text{Under } H_0,\\\\\n&T_W' = nI_1( \\theta_0)(\\hat \\theta_{\\text{MLE}} -\\theta_0)^2 \\xrightarrow{D} \\chi_1^2  \\\\\n&Z_W' = \\sqrt{nI_1( \\theta_0)}(\\hat \\theta_{\\text{MLE}} - \\theta_0) \\xrightarrow{D} N(0, 1) \\\\\n&\\text{Reject } H_0 \\text{ if, equivalently, } \\\\\n&Z'_W  =  \\sqrt{nI_1( \\theta_0)}(\\hat \\theta_{\\text{MLE}} - \\theta_0) <- z_{\\alpha}, \\text{ or }\\\\\n&\\hat \\theta_{\\text{MLE}} < \\theta_0 -\\frac{z_{\\alpha}}{\\sqrt{nI_1( \\theta_0)}}\\\\\n\\end{split}\n\\]\n\n\n4.3.4 \\(H_0: \\theta = \\theta_0\\) vs. \\(H_a: \\theta > \\theta_0\\)\nIf \\(nI_1(\\hat \\theta_{\\text{MLE}})/nI_1(\\theta_0)\\) converges in probability to 1 as \\(n \\rightarrow \\infty\\), we have, for a one-sided test against \\(H_a: \\theta > \\theta_0\\),\n\\[\n\\begin{split}\n&\\text{First approach (plug in } \\hat\\theta_{\\text{MLE}}): \\text{Under } H_0,\\\\\n&T_W = nI_1(\\hat \\theta_{\\text{MLE}})(\\hat \\theta_{\\text{MLE}} -\\theta_0)^2 \\xrightarrow{D} \\chi_1^2  \\\\\n&Z_W = \\sqrt{nI_1(\\hat \\theta_{\\text{MLE}})}(\\hat \\theta_{\\text{MLE}} - \\theta_0) \\xrightarrow{D} N(0, 1) \\\\\n&\\text{Reject } H_0 \\text{ if, equivalently, } \\\\\n&Z_W  = \\sqrt{nI_1(\\hat \\theta_{\\text{MLE}})}(\\hat \\theta_{\\text{MLE}} - \\theta_0) > z_{\\alpha}, \\text{ or }\\\\\n&\\hat \\theta_{\\text{MLE}} > \\theta_0 +\\frac{z_{\\alpha}}{\\sqrt{nI_1(\\hat \\theta_{\\text{MLE}})}}\\\\\\\\\n&\\text{Second approach (plug in } \\theta_0): \\text{Under } H_0,\\\\\n&T_W' = nI_1( \\theta_0)(\\hat \\theta_{\\text{MLE}} -\\theta_0)^2 \\xrightarrow{D} \\chi_1^2  \\\\\n&Z_W' = \\sqrt{nI_1( \\theta_0)}(\\hat \\theta_{\\text{MLE}} - \\theta_0) \\xrightarrow{D} N(0, 1) \\\\\n&\\text{Reject } H_0 \\text{ if, equivalently, } \\\\\n&Z'_W  =  \\sqrt{nI_1( \\theta_0)}(\\hat \\theta_{\\text{MLE}} - \\theta_0) > z_{\\alpha}, \\text{ or }\\\\\n&\\hat \\theta_{\\text{MLE}} > \\theta_0 +\\frac{z_{\\alpha}}{\\sqrt{nI_1( \\theta_0)}}\\\\\n\\end{split}\n\\]\n\n\n4.3.5 Example: Bernoulli test of \\(H_0: p = p_0\\) vs. \\(H_a: p > p_0\\)\nWe have iid \\(Y_i \\sim\\) Bernoulli(\\(p\\)). Consider a simple null hypothesis \\(H_0: p = p_0\\) vs. \\(H_a: p > p_0\\). Derive \\(\\ell(p), S_n(p), \\hat p_{\\text{MLE}}\\), and \\(I_T(p)\\). From this, derive two expressions for the Wald test.\n\\[\n\\begin{split}\nE[Y_i] &= p \\\\\nL(p) &= \\prod_{i = 1}^n p^{Y_i} (1-p)^{1 - Y_i} \\\\\n\\ell(p) &= \\sum_{i = 1}^n Y_i\\ln p + (1-Y_i)\\ln(1-p) \\\\\nS_n(p) = \\frac{d\\ell(p)}{dp} &= \\sum_{i = 1}^n \\frac{Y_i}{p} - \\frac{1-Y_i}{1 - p} = 0 \\\\\n\\sum_{i = 1}^n \\frac{Y_i}{p} &= \\sum_{i = 1}^n \\frac{1-Y_i}{1 - p} \\\\\n\\frac{1}{p}\\sum_{i = 1}^n {Y_i} &= \\frac{1}{{1 - p}}\\sum_{i = 1}^n {1-Y_i} \\\\\n\\frac{1 - p}{p} &= \\frac{\\sum_{i = 1}^n {1-Y_i}}{\\sum_{i = 1}^n {Y_i}} \\\\\n\\frac{1}{p} - 1 &= \\frac{n}{\\sum_{i = 1}^n Y_i} - 1 \\\\\n\\frac{1}{p} &= \\frac{n}{\\sum_{i = 1}^n Y_i} \\\\\n\\hat p_{\\text{MLE}} &= \\frac{1}{n} \\sum_{i = 1}^n Y_i = \\bar Y \\\\\nI_1(p) &= - E\\Big[\\frac{\\partial}{\\partial p} S_i(p) \\Big] \\\\\n&= - E\\Big[\\frac{\\partial}{\\partial p} {Y_i} \\cdot{p}^{-1} - (1-Y_i) \\cdot(1 - p)^{-1} \\Big]\\\\\n&= - E\\Big[ {Y_i} \\cdot-{p}^{-2} - (1-Y_i) \\cdot-(1 - p)^{-2} \\cdot -1 \\Big]\\\\\n&= - E\\Big[ -\\frac{Y_i}{{p}^{2}} - \\frac{1-Y_i}{(1 - p)^{2}}  \\Big]\\\\\n&= E\\Big[ \\frac{Y_i}{{p}^{2}} + \\frac{1-Y_i}{(1 - p)^{2}}  \\Big]\\\\\n&= \\frac{1}{p^2}E[ {Y_i}] + \\frac{1}{(1-p)^2} E[{1-Y_i}]  \\\\\n&= \\frac{p}{p^2} + \\frac{1-p}{(1-p)^2} \\\\\n&= \\frac{1}{p} + \\frac{1}{1-p} \\\\\n&= \\frac{1 -p}{p(1-p)} + \\frac{p}{p(1-p)} \\\\\n&= \\frac{1}{p(1 - p)} \\\\\nI_T(p) = nI_1(p) &= \\frac{n}{p(1 - p)} \\\\\\\\\n\\text{Under } H_0&, \\sqrt{nI_1(p)}(\\hat p_{\\text{MLE}} - p) \\xrightarrow{D} N(0, 1) \\\\\n(1) \\text{ Based on } Z_W &=  \\sqrt{nI_1(\\hat p_{\\text{MLE}})}(\\hat p_{\\text{MLE}} - p_0) \\xrightarrow{D} N(0, 1), \\\\\n\\text{Reject } H_0 \\text{ if } Z_W &= \\frac{\\sqrt n (\\hat p_{\\text{MLE}} - p_0)}{\\sqrt{\\hat p_{\\text{MLE}}(1 - \\hat p_{\\text{MLE}})}}  > z_{\\alpha} \\\\\n(2) \\text{ Based on } Z'_W &=  \\sqrt{nI_1( p_0)}(\\hat p_{\\text{MLE}} - p_0) \\xrightarrow{D} N(0, 1), \\\\\n\\text{Reject } H_0 \\text{ if } Z'_W &= \\frac{\\sqrt n (\\hat p_{\\text{MLE}} - p_0)}{\\sqrt{ p_0(1 -  p_0)}}  > z_{\\alpha} \\\\\n\\end{split}\n\\]"
  },
  {
    "objectID": "week4.html#score-lagrange-multiplier-test-simple-null-h_0-theta-theta_0",
    "href": "week4.html#score-lagrange-multiplier-test-simple-null-h_0-theta-theta_0",
    "title": "4  Hypothesis testing",
    "section": "4.4 Score (Lagrange Multiplier) test: Simple null \\((H_0: \\theta = \\theta_0)\\)",
    "text": "4.4 Score (Lagrange Multiplier) test: Simple null \\((H_0: \\theta = \\theta_0)\\)\n\\[\n\\begin{split}\nT_S = \\frac{[S_n(\\theta_0)]^2}{nI_1(\\theta_0)}~ &\\xrightarrow{D} \\chi^2_1 \\\\\n\\text{Reject } H_0 \\text{ if } T_S &> \\chi^2_{1, \\alpha} \\\\\nZ_S = \\frac{S_n(\\theta_0)}{\\sqrt{nI_1(\\theta_0)}}~ &\\xrightarrow{D} N(0,1) \\\\\n\\text{Reject } H_0 \\text{ if } Z_S &> z_{\\alpha} \\\\\n\\end{split}\n\\]\n\n4.4.1 Multi-parameter formulation: Simple null (\\(H_0: \\pmb \\theta = \\pmb \\theta_0\\))\nUnder \\(H_0\\),\n\\[\n\\begin{split}\nT_S = \\pmb S_n(\\pmb \\theta_0)^T [\\pmb I_T(\\pmb \\theta_0)]^{-1} \\pmb S_n(\\pmb \\theta_0) &\\xrightarrow {D} \\chi^2_b \\\\\n[\\pmb I_T(\\pmb \\theta_0)]^{-1/2} \\pmb S_n(\\pmb \\theta_0) &\\xrightarrow {D} N(\\pmb 0, \\mathbb{1}_b)\n\\end{split}\n\\]\n\n\n4.4.2 Example: Bernoulli test of \\(H_0: p = p_0\\) vs. \\(H_a: p > p_0\\)\nReturn to the example above. What is the Score test statistic?\n\\[\n\\begin{split}\nT_S &= \\frac{[S_n(\\theta_0)]^2}{nI_1(\\theta_0)} \\\\\n&= \\Bigg(\\sum_{i = 1}^n \\frac{Y_i}{p_0} - \\frac{1-Y_i}{1 - p_0}\\Bigg)^2 \\Bigg(\\frac{p_0(1 - p_0)}{n} \\Bigg) \\\\\n&= \\Bigg(\\sum_{i = 1}^n \\frac{Y_i(1 - p_0)}{p_0(1 - p_0)} - \\frac{p_0(1-Y_i)}{p_0(1 - p_0)}\\Bigg)^2 \\Bigg(\\frac{p_0(1 - p_0)}{n} \\Bigg) \\\\\n&= \\Bigg(\\sum_{i = 1}^n \\frac{Y_i - Y_ip_0}{p_0(1 - p_0)} - \\frac{p_0 -Y_ip_0}{p_0(1 - p_0)}\\Bigg)^2 \\Bigg(\\frac{p_0(1 - p_0)}{n} \\Bigg) \\\\\n&= \\Bigg(\\sum_{i = 1}^n \\frac{Y_i - p_0}{p_0(1 - p_0)} \\Bigg)^2 \\Bigg(\\frac{p_0(1 - p_0)}{n} \\Bigg) \\\\\n&=  \\Bigg(\\frac{n\\bar Y - np_0}{p_0(1 - p_0)} \\Bigg)^2 \\Bigg(\\frac{p_0(1 - p_0)}{n} \\Bigg) \\\\\n&= \\Bigg(n \\frac{(\\bar Y - p_0)}{p_0(1 - p_0)} \\Bigg)^2 \\Bigg(\\frac{p_0(1 - p_0)}{n} \\Bigg) \\\\\n&= \\frac{n^2}{n} \\frac{(\\bar Y - p_0)^2}{p_0^2(1 - p_0)^2} \\Big(p_0(1 - p_0)\\Big) \\\\\n&= \\frac{n(\\bar Y - p_0)^2}{p_0(1 - p_0)} \\\\\nZ_S &= \\sqrt{T_S} \\\\ &=   \\frac{\\sqrt n(\\bar Y - p_0)}{\\sqrt{p_0(1 - p_0)}} \\\\\n&= Z'_W\n\\end{split}\n\\]"
  },
  {
    "objectID": "week4.html#likelihood-ratio-test-simple-null-h_0-theta-theta_0",
    "href": "week4.html#likelihood-ratio-test-simple-null-h_0-theta-theta_0",
    "title": "4  Hypothesis testing",
    "section": "4.5 Likelihood ratio test: Simple null (\\(H_0: \\theta = \\theta_0\\))",
    "text": "4.5 Likelihood ratio test: Simple null (\\(H_0: \\theta = \\theta_0\\))\n\\[\n\\begin{split}\nT_{LR} = 2\\Big\\{ \\ell(\\hat \\theta_{MLE}) - \\ell(\\theta_0) \\Big\\} &\\xrightarrow{D} \\chi^2_1 \\text{ under } H_0, \\text{ as } n \\rightarrow \\infty \\\\\n\\text{Reject } H_0 \\text{ if } T_{LR} &> \\chi^2_{1, \\alpha}\n\\end{split}\n\\]\nWe have the following equivalent definitions:\n\\[\n\\begin{split}\nT_{LR} &= -2\\Big\\{\\ell(\\theta_0) - \\ell(\\hat \\theta_{\\text{MLE}})\\Big\\} \\\\\n&= 2\\Big\\{\\ell(\\hat \\theta_{\\text{MLE}}) -\\ell(\\theta_0) \\Big\\} \\\\\n&= -2\\ln\\Bigg\\{\\frac{L(\\theta_0)}{L(\\hat \\theta_{\\text{MLE}})}\\Bigg\\} \\\\\n&= 2\\ln\\Bigg\\{\\frac{L(\\hat \\theta_{\\text{MLE}})}{L(\\theta_0)}\\Bigg\\}\n\\end{split}\n\\]\n\n4.5.1 Multi-parameter formulation: Simple null (\\(H_0: \\pmb \\theta = \\pmb \\theta_0\\))\n\\[T_{LR} = 2\\Big\\{ \\ell(\\hat {\\pmb \\theta}_{MLE}) - \\ell(\\pmb\\theta_0) \\Big\\} \\xrightarrow{D} \\chi^2_b \\text{ under } H_0, \\text{ as } n \\rightarrow \\infty\\]\nThe distribution of \\(T_{LR}\\) converges to a \\(\\chi^2_r\\) distribution as \\(n \\rightarrow \\infty\\), where the degrees of freedom \\(r\\) are given by the difference between the number of free parameters specified by \\(\\pmb \\theta \\in \\Theta_0\\) (the \\(H_0\\)-restricted parameter space) and the number of free parameters specified by \\(\\pmb \\theta \\in \\Theta\\) (the entire parameter space).\nWe reject \\(H_0\\) iff \\(T_{LR} > \\chi^2_{r, \\alpha}\\).\n\n\n4.5.2 Example: Bernoulli test of \\(H_0: p = p_0\\) vs. \\(H_a: p > p_0\\)\nReturn to the example above. What is the Likelihood ratio test statistic?\n\\[\n\\begin{split}\n\\ell(p) &= \\sum_{i = 1}^n Y_i \\ln p + (1 - Y_i)\\ln(1-p) \\\\\nT_{LR} &= -2\\Big\\{\\ell(p_0) - \\ell(\\hat p_{\\text{MLE}})\\Big\\} \\\\\n&= -2\\Big\\{ \\sum_{i = 1}^n Y_i \\ln p_0 + (1 - Y_i)\\ln(1-p_0)  - \\\\\n&~~~~~~~~~~ \\Big [ \\sum_{i = 1}^n Y_i \\ln \\hat p_{\\text{MLE}} + (1 - Y_i)\\ln(1-\\hat p_{\\text{MLE}})  \\Big]\\Big\\} \\\\\n&= -2 \\Big\\{\\ln\\Big( \\frac{p_0}{\\hat p_{\\text{MLE}}} \\Big)  \\sum_{i = 1}^n Y_i + \\ln\\Big( \\frac{1 - p_0}{1 - \\hat p_{\\text{MLE}}} \\Big) \\sum_{i = 1}^n 1 - Y_i \\Big\\} \\\\\n\\text{Reject } &H_0 \\text{ if } T_{LR} > \\chi^2_{1, \\alpha}\n\\end{split}\n\\]\n\n\n4.5.3 Example: Poisson distribution\nLet \\(X_1, ..., X_n\\) be independent random samples from a Poisson distribution with parameter \\(\\lambda\\) and define the sum \\(Y = \\sum_{i = 1}^n X_i\\). Construct a confidence interval for \\(\\lambda\\) by inverting a LR test statistic testing \\(H_0: \\lambda = \\lambda_0\\) vs. \\(H_a: \\lambda \\ne \\lambda_0\\).\n\\[\n\\begin{split}\nT_{LR} &= -2\\ln\\Big\\{\\frac{L(\\lambda_0)}{L(\\hat \\lambda_{\\text{MLE}})}\\Big\\} \\\\\nL(\\lambda | \\pmb x) &= \\frac{e^{-n\\lambda}\\lambda^{\\sum_ix_i}}{\\prod_i x_i!}, \\text{with } \\hat \\lambda_{\\text{MLE}} = \\bar x \\\\\nL(\\lambda_0) &=\\frac{e^{-n\\lambda_0}\\lambda_0^{\\sum_ix_i}}{\\prod_i x_i!} \\\\\nL(\\hat \\lambda_{\\text{MLE}}) &= \\frac{e^{-n\\bar x}\\bar x^{\\sum_ix_i}}{\\prod_i x_i!} \\\\\nT_{LR} &= -2\\ln\\Big\\{\\frac{L(\\lambda_0)}{L(\\hat \\lambda_{\\text{MLE}})}\\Big\\} \\\\\n&= -2\\ln\\Big\\{ \\frac{e^{-n\\lambda_0}\\lambda_0^{\\sum_ix_i}}{\\prod_i x_i!} \\cdot \\frac{\\prod_i x_i!}{e^{-n\\bar x}\\bar x^{\\sum_ix_i}}\\Big\\} \\\\\n&=-2\\ln\\Big\\{ \\frac{e^{-n\\lambda_0}\\lambda_0^{\\sum_ix_i}}{e^{-n\\bar x}\\bar x^{\\sum_ix_i}}\\Big\\} \\\\\n&=-2\\ln\\Big\\{e^{-n\\lambda_0 + n\\bar x} \\Big(\\frac{\\lambda_0}{\\bar x}\\Big)^{\\sum_i x_i}\\Big\\} \\\\\n&= -2\\Big\\{-n\\lambda_0 + n\\bar x + \\sum_i x_i\\ln\\Big[\\frac{\\lambda_0}{\\bar x}\\Big]\\Big\\} \\\\\n&=-2n\\Big\\{-\\lambda_0 + \\bar x + \\bar x\\ln\\Big[\\frac{\\lambda_0}{\\bar x}\\Big]\\Big\\} \\\\\n&=2n\\Big\\{\\lambda_0 - \\bar x + \\bar x\\ln\\Big[\\frac{\\bar x}{\\lambda_0}\\Big]\\Big\\} \\\\\n\\text{By }& \\text{Wilk's theorem}, \\\\\n&2n\\Big\\{\\lambda_0 - \\bar X + \\bar X\\ln\\Big[\\frac{\\bar X}{\\lambda_0}\\Big] \\Big\\} \\sim \\chi_1^2\n\\end{split}\n\\]"
  },
  {
    "objectID": "week4.html#composite-null-hypotheses-h_0-pmb-theta_1-pmb-theta_10",
    "href": "week4.html#composite-null-hypotheses-h_0-pmb-theta_1-pmb-theta_10",
    "title": "4  Hypothesis testing",
    "section": "4.6 Composite null hypotheses (\\(H_0: \\pmb \\theta_1 = \\pmb \\theta_{10}\\))",
    "text": "4.6 Composite null hypotheses (\\(H_0: \\pmb \\theta_1 = \\pmb \\theta_{10}\\))\nEarlier, we defined the test statistics when the null hypothesis is of the form \\(H_0: \\pmb \\theta = \\pmb\\theta_0\\). Note that this restricts the entire parameter vector \\(\\pmb \\theta\\) (for example, for a normal distribution, we would restrict both \\(\\mu\\) and \\(\\sigma^2\\) to \\(\\pmb\\theta_0\\) under the null). However, we are often interested only in certain components of \\(\\pmb \\theta\\). For example, we wish to test \\(\\theta_1 =\\mu\\), while leaving \\(\\theta_2 =\\sigma^2\\) unrestricted.\nIn other words, under a simple null hypothesis, all parameters in \\(\\theta\\) are specified, i.e. assumed to be known. Under a composite null hypothesis, we can account for parameters that are unknown and leave them unrestricted.\n\n4.6.1 Partitioning the information matrix\n\\[\\hat{\\pmb I} = \\pmb I_T(\\hat{\\pmb \\theta}_{\\text{MLE}}) = n \\pmb I_1(\\hat{\\pmb \\theta}_{\\text{MLE}})\\]\n\\[\n\\hat{\\pmb I} =\n\\begin{bmatrix}\n\\hat{\\pmb I}_{11} & \\hat{\\pmb I}_{12} \\\\\n\\hat{\\pmb I}_{21} & \\hat{\\pmb I}_{22} \\\\\n\\end{bmatrix}\n\\]\n\n\n4.6.2 Wald test: Composite null (\\(H_0: \\pmb \\theta_1 = \\pmb \\theta_{10}\\)) with unknown parameter(s)\nConsider testing \\(H_0: \\pmb \\theta_1 = \\pmb \\theta_{10}\\) versus \\(H_a: \\pmb \\theta_1 \\ne \\pmb \\theta_{10}\\).\nRecall that, in the simple null hypothesis (\\(H_0: \\pmb \\theta = \\pmb \\theta_0\\)) case,\n\\[\nT_W = (\\hat{\\pmb\\theta}_{\\text{MLE}} - {\\pmb\\theta}_0)^T \\{\\pmb I_T(\\hat {\\pmb \\theta}_{\\text{MLE}})\\}(\\hat{\\pmb\\theta}_{\\text{MLE}} - {\\pmb\\theta}_0) \\xrightarrow{D} \\chi^2_b\n\\]\nInstead, when we have a composite null hypothesis of the form \\(H_0: \\pmb \\theta_1 = \\pmb \\theta_{10}\\) (and thus, we leave at least one parameter \\(\\in \\pmb \\theta\\) unknown under the null), we replace \\(\\{\\pmb I_T(\\hat {\\pmb \\theta}_{\\text{MLE}})\\}\\) with\n\\[\n\\begin{split}\n\\Big(\\hat{\\pmb I}_{11}- \\hat{\\pmb I}_{12}\\Big[\\hat{\\pmb I}_{22}\\Big]^{-1}\\hat{\\pmb I}_{21} \\Big)  &= \\Bigg( \\hat{\\pmb I}_{11}- \\frac{\\hat{\\pmb I}_{12}\\hat{\\pmb I}_{21}}{\\hat{\\pmb I}_{22}} \\Bigg) \\\\\n\\text{If } \\hat{\\pmb I}_{12} &\\equiv \\hat{\\pmb I}_{21},\\\\\n\\Big(\\hat{\\pmb I}_{11}- \\hat{\\pmb I}_{12}(\\hat{\\pmb I}_{22})^{-1}\\hat{\\pmb I}_{21} \\Big)  &= \\Bigg( \\hat{\\pmb I}_{11}- \\frac{(\\hat{\\pmb I}_{12})^2}{\\hat{\\pmb I}_{22}} \\Bigg)\n\\end{split}\n\\]\nNotice the similarity with the Fisher information matrix we formulated earlier, when the other parameter is unknown. In particular, when the other parameter \\(\\eta\\) is unknown, we have the Fisher information\n\\[I^*_{\\theta\\theta} = I_{\\theta\\theta} - \\Big[ I_{\\theta\\eta} \\cdot I^{-1}_{\\eta\\eta} \\cdot I_{\\eta\\theta} \\Big] = I_{\\theta\\theta} - \\dfrac{I_{\\theta\\eta}^2}{I_{\\eta\\eta}}\\]\nPutting this all together, we obtain the Wald test statistic for testing the composite null hypothesis \\(H_0: \\pmb \\theta_1 = \\pmb \\theta_{10}\\) versus \\(H_a: \\pmb \\theta_1 \\ne \\pmb \\theta_{10}\\):\n\\[\n\\begin{split}\n&\\text{Under } H_0, \\text{ when } n \\rightarrow \\infty,  \\\\\nT_W &= (\\hat{\\pmb\\theta}_1 - {\\pmb\\theta}_{10})^T \\Big(\\hat{\\pmb I}_{11}- \\hat{\\pmb I}_{12}\\Big[\\hat{\\pmb I}_{22}\\Big]^{-1}\\hat{\\pmb I}_{21} \\Big)(\\hat{\\pmb\\theta}_1 - {\\pmb\\theta}_{10}) \\\\\n&= (\\hat{\\pmb\\theta}_1 - {\\pmb\\theta}_{10})^T \\Bigg( \\hat{\\pmb I}_{11}- \\frac{(\\hat{\\pmb I}_{12})^2}{\\hat{\\pmb I}_{22}} \\Bigg)(\\hat{\\pmb\\theta}_1 - {\\pmb\\theta}_{10}) \\xrightarrow{D} \\chi^2_r\n\\end{split}\n\\]\n\n\n4.6.3 Score test: Composite null (\\(H_0: \\pmb \\theta_1 = \\pmb \\theta_{10}\\)) with unknown parameter(s)\nRecall that, in the simple null hypothesis (\\(H_0: \\pmb \\theta = \\pmb \\theta_0\\)) case,\n\\[\nT_S = \\pmb S_n(\\pmb \\theta_0)^T [\\pmb I_T(\\pmb \\theta_0)]^{-1} \\pmb S_n(\\pmb \\theta_0) \\xrightarrow {D} \\chi^2_b \\\\\n\\]\nThe Score statistic \\(T_S\\) for the composite null hypothesis requires calculating the MLE of \\(\\pmb \\theta\\) under the restriction imposed by \\(H_0\\). We denote this restricted MLE by \\(\\tilde{\\pmb \\theta}\\), and obtain the total Fisher information matrix evaluated at the restricted MLE: \\(\\tilde{\\pmb I} = \\pmb I_T(\\tilde{\\pmb \\theta})\\).\nPutting this all together, we obtain the Score test statistic for testing the composite null hypothesis \\(H_0: \\pmb \\theta_1 = \\pmb \\theta_{10}\\) versus \\(H_a: \\pmb \\theta_1 \\ne \\pmb \\theta_{10}\\):\n\\[\n\\begin{split}\n&\\text{Under } H_0, \\text{ when } n \\rightarrow \\infty,  \\\\\nT_S &= \\pmb S_{1n}(\\pmb \\theta_{10})^T \\Big(\\tilde{\\pmb I}_{11}- \\tilde{\\pmb I}_{12}\\Big[\\tilde{\\pmb I}_{22}\\Big]^{-1}\\tilde{\\pmb I}_{21} \\Big)^{-1} \\pmb S_{1n}(\\pmb \\theta_{10}) \\\\\n&= \\pmb S_{1n}(\\pmb \\theta_{10})^T \\Bigg( \\tilde{\\pmb I}_{11}- \\frac{(\\tilde{\\pmb I}_{12})^2}{\\tilde{\\pmb I}_{22}} \\Bigg)^{-1} \\pmb S_{1n}(\\pmb \\theta_{10})\n\\xrightarrow {D} \\chi^2_r \\\\\n\\end{split}\n\\]\n\n\n4.6.4 Likelihood ratio test: Composite null (\\(H_0: \\pmb \\theta_1 = \\pmb \\theta_{10}\\)) with unknown parameter(s)\nWe can measure the relative plausibility of \\(H_1\\) to \\(H_0\\) by the log likelihood ratio. To generalize the case of simple hypotheses, assume that \\(H_0\\) specifies \\(\\theta \\in \\Theta_0\\) and \\(H_1\\) specifies \\(\\theta \\in \\Theta_1\\). Let \\(\\Theta = \\Theta_0 \\cup \\Theta_1\\). We have the simple (\\(\\Lambda\\)) and the generalized (\\(\\Lambda^*\\)) log-likelihood ratio:\n\\[\n\\begin{split}\n\\text{In the simple case, } \\Lambda &=\n\\ln\\frac{f(X_1, ..., X_n | H_1)}{f(X_1, ..., X_n | H_0)} \\\\\n\\text{In the general case, } \\Lambda^*&=\n\\ln\\frac{\\sup_{\\theta \\in \\Theta_1} f(X_1, ..., X_n | \\pmb\\theta)}{\\sup_{\\theta \\in \\Theta_0}f(X_1, ..., X_n | \\pmb\\theta)} \\\\  \n&= \\ln\\frac{\\sup_{\\theta \\in \\Theta_1} L(\\pmb \\theta | \\pmb X)}{\\sup_{\\theta \\in \\Theta_0} L(\\pmb \\theta | \\pmb X)} \\\\  \n\\end{split}\n\\]\nWe can again use the likelihood ratio statistic\n\\[\n\\begin{split}\nT_{LR} &=  2\\ln\\frac{\\sup_{\\theta \\in \\Theta} f(X_1, ..., X_n | \\pmb \\theta)}{\\sup_{\\theta \\in \\Theta_0}f(X_1, ..., X_n | \\pmb \\theta)}  \\\\\n&= 2\\ln\\frac{\\sup_{\\theta \\in \\Theta} L(\\pmb\\theta | \\pmb X)}{\\sup_{\\theta \\in \\Theta_0}L(\\pmb\\theta | \\pmb X)} \\\\\n&= -2\\ln \\frac{\\sup_{\\theta \\in \\Theta_0}L(\\pmb\\theta | \\pmb X)}{\\sup_{\\theta \\in \\Theta} L(\\pmb\\theta | \\pmb X)}   \\\\\n&= -2\\Big\\{ \\ell(\\tilde{\\pmb \\theta}) - \\ell(\\hat{\\pmb \\theta}) \\Big\\},\n\\end{split}\n\\]\nwhere \\(\\tilde{\\pmb \\theta}\\) is the restricted MLE under \\(H_0\\) and \\(\\hat{\\pmb \\theta}\\) is the unrestricted MLE.\nLarge values of \\(T_{LR}\\) provide stronger evidence against \\(H_0\\). According to Wilks Theorem, when the joint distribution of \\(X_1, ..., X_n\\) depends on \\(p\\) unknown parameters and \\(p_0\\) unknown parameters under \\(H_0\\), under regularity conditions and assuming \\(H_0\\) is true, the distribution of \\(T_{LR}\\) tends to a \\(\\chi^2\\) distribution with degrees of freedom \\(r = p - p_0\\) as \\(n \\rightarrow \\infty\\). For n large, we compare the value of \\(T_{LR}\\) to the expected values from a \\(\\chi^2_{r}\\).\n\\[\n\\begin{split}\n\\text{Under } H_0, \\text{ as } n \\rightarrow \\infty, \\\\\nT_{LR} = -2\\Big\\{ \\ell(\\tilde{\\pmb \\theta}) - \\ell(\\hat{\\pmb \\theta}) \\Big\\} &\\xrightarrow{D} \\chi^2_{r}\n\\end{split}\n\\]\nThe critical region for a test with approximate significance level \\(\\alpha\\) is given by\n\\[\nR = \\{\\pmb X: T_{LR} \\ge \\chi^2_{r,\\alpha}\\}\n\\]\nIn a simple case when we are testing \\(H_0: \\theta = 0\\) against \\(H_a: \\theta \\ne 0\\), we have \\(p = 1\\), \\(p_0 = 0\\), and \\(\\nu = p - p_0 = 1\\). In this case, \\(\\sup_{\\theta \\in \\Omega} L(\\theta | \\pmb X)\\) simplifies to \\(L(\\hat\\theta_{\\text{MLE}})\\); \\(\\sup_{\\theta \\in \\Theta_0}L(\\theta | \\pmb X)\\) simplifies to \\(L(\\theta_0)\\)."
  },
  {
    "objectID": "week4.html#example-normality-with-unknown-variance-t-test",
    "href": "week4.html#example-normality-with-unknown-variance-t-test",
    "title": "4  Hypothesis testing",
    "section": "4.7 Example: Normality with unknown variance (t-test)",
    "text": "4.7 Example: Normality with unknown variance (t-test)\nWe have iid \\(Y_i \\sim N(\\mu, \\sigma^2)\\) with \\(\\sigma^2\\) unknown. We want to test \\(H_0: \\mu = \\mu_0\\) vs. \\(H_a: \\mu \\ne \\mu_0\\). Notice that this is a composite null: we leave \\(\\sigma^2\\) unspecified under the null.\nFor the Wald statistic, recall that\n\\[\nT_W = (\\hat{\\pmb\\theta}_1 - {\\pmb\\theta}_{10})^T \\Bigg( \\hat{\\pmb I}_{11}- \\frac{(\\hat{\\pmb I}_{12})^2}{\\hat{\\pmb I}_{22}} \\Bigg)(\\hat{\\pmb\\theta}_1 - {\\pmb\\theta}_{10}) \\xrightarrow{D} \\chi^2_r\n\\]\nWe have\n\\[\n\\begin{split}\n{\\pmb\\theta}_{10} &= \\mu_{0} \\\\\n\\hat{\\pmb\\theta}_1 &= \\hat \\mu = \\bar Y \\\\\n\\text{In the simple case, we } &\\text{have the total Fisher information matrix  } \\\\\n\\text{} I_T(\\theta) &=\n\\begin{bmatrix}\n\\displaystyle \\frac{n}{\\sigma^2} & 0 \\\\\n0 & \\displaystyle \\frac{2n}{\\sigma^2}\\\\\n\\end{bmatrix} \\\\\n\\text{In the composite case, } &\\text{we have } \\\\\n\\pmb I_T(\\hat  {\\pmb \\theta}_{\\text{MLE}}) &=\n\\begin{bmatrix}\n\\displaystyle \\frac{n}{s^2} & 0 \\\\\n0 & \\displaystyle \\frac{2n}{s^2}\\\\\n\\end{bmatrix}, \\text{ where }  \\\\\ns^2 = \\hat \\sigma^2 &= \\frac{1}{n} \\sum_{i = 1}^n (Y_i - \\bar Y)^2. \\text{ Thus, }\\\\\n\\Bigg( \\hat{\\pmb I}_{11}- \\frac{(\\hat{\\pmb I}_{12})^2}{\\hat{\\pmb I}_{22}} \\Bigg) &= \\frac{n}{s^2} \\\\\nT_W &= (\\hat{\\pmb\\theta}_1 - {\\pmb\\theta}_{10})^T \\Bigg( \\hat{\\pmb I}_{11}- \\frac{(\\hat{\\pmb I}_{12})^2}{\\hat{\\pmb I}_{22}} \\Bigg)(\\hat{\\pmb\\theta}_1 - {\\pmb\\theta}_{10}) \\\\\n&= (\\hat \\mu - \\mu_0)^T(\\frac{n}{s^2})(\\hat \\mu - \\mu_0) \\\\\n&= (\\bar Y - \\mu_0)^T(\\frac{n}{s^2})(\\bar Y - \\mu_0) \\\\\n&= \\frac{n(\\bar Y - \\mu_0)^2}{s^2}\n\\end{split}\n\\]\nFor the Score statistic, recall that\n\\[\nT_S =\\pmb S_{1n}(\\pmb \\theta_{10})^T \\Bigg( \\tilde{\\pmb I}_{11}- \\frac{(\\tilde{\\pmb I}_{12})^2}{\\tilde{\\pmb I}_{22}} \\Bigg)^{-1} \\pmb S_{1n}(\\pmb \\theta_{10})\n\\]\nWe know\n\\[\n\\begin{split}\n\\pmb I_T(  {\\tilde{\\pmb \\theta}}) &=\n\\begin{bmatrix}\n\\displaystyle \\frac{n}{\\tilde \\sigma^2} & 0 \\\\\n0 & \\displaystyle \\frac{2n}{\\tilde \\sigma^2}\\\\\n\\end{bmatrix} \\\\\n\\text{Recall that } S_{\\mu n}(\\mu,\\sigma^2) &= \\frac{1}{\\sigma^2} \\sum_{i = 1}^n (Y_i - \\mu) \\\\\n\\text{ and }S_{\\sigma^2 n}(\\mu,\\sigma^2) &= \\frac{-n}{2\\sigma^2} + \\frac{1}{2\\sigma^4} \\sum_{i = 1}^n (Y_i - \\mu)^2 \\\\\n\\pmb S_{1n}(\\pmb \\theta_{10}) &= S_{\\mu n}(\\mu_0,\\tilde{\\sigma}^2) \\\\\n&=\\frac{1}{\\tilde\\sigma^2} \\sum_{i = 1}^n (Y_i - \\mu_0) \\\\\n\\end{split}\n\\]\nFirst, we fix \\(\\mu\\) to \\(\\mu_0\\) in order to solve \\(S_{\\sigma^2 n}(\\mu_0,\\sigma^2) = 0\\) and obtain \\(\\tilde\\sigma^2\\):\n\\[\n\\begin{split}\nS_{\\sigma^2 n}(\\mu_0,\\sigma^2) &= \\frac{-n}{2\\sigma^2} + \\frac{1}{2\\sigma^4} \\sum_{i = 1}^n (Y_i - \\mu_0)^2 = 0 \\\\\n&\\Rightarrow  \\frac{1}{2\\sigma^4} \\sum_{i = 1}^n (Y_i - \\mu_0)^2 = \\frac{n}{2\\sigma^2} \\\\\n&\\Rightarrow \\frac{2\\sigma^2}{2\\sigma^4} = \\frac{n}{ \\sum_{i = 1}^n (Y_i - \\mu_0)^2} \\\\\n\\tilde{\\sigma}^2 &= \\frac{1}{n}\\sum_{i = 1}^n (Y_i - \\mu_0)^2 \\\\\n\\text{Using the identity } \\sum_{i = 1}^n (Y_i - \\mu_0)^2 &= \\sum_{i = 1}^n \\Big [(Y_i - \\bar Y)^2 + (\\bar Y - \\mu_0)^2 \\Big] \\\\\n&= \\sum_{i = 1}^n (Y_i - \\bar Y)^2 +  \\sum_{i = 1}^n (\\bar Y - \\mu_0)^2 \\\\\n&= n(\\bar Y - \\mu_0)^2  + \\sum_{i = 1}^n (Y_i - \\bar Y)^2, \\text{ we have} \\\\    \n\\tilde{\\sigma}^2 &= \\frac{1}{n}\\sum_{i = 1}^n (Y_i - \\mu_0)^2 \\\\\n&= \\frac{1}{n} \\Big[ n(\\bar Y - \\mu_0)^2  + \\sum_{i = 1}^n (Y_i - \\bar Y)^2\\Big] \\\\\n&= s^2 + (\\bar Y - \\mu_0)^2\n\\end{split}\n\\]\nNow,\n\\[\n\\begin{split}\n\\pmb I_T(  {\\tilde{\\pmb \\theta}}) &=\n\\begin{bmatrix}\n\\displaystyle \\frac{n}{\\tilde \\sigma^2} & 0 \\\\\n0 & \\displaystyle \\frac{2n}{\\tilde \\sigma^2}\\\\\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n\\displaystyle \\frac{n}{s^2 + (\\bar Y - \\mu_0)^2} & 0 \\\\\n0 & \\displaystyle \\frac{2n}{s^2 + (\\bar Y - \\mu_0)^2}\\\\\n\\end{bmatrix} \\\\\n\\Bigg( \\tilde{\\pmb I}_{11}- \\frac{(\\tilde{\\pmb I}_{12})^2}{\\tilde{\\pmb I}_{22}} \\Bigg)^{-1} &= \\Bigg(\\frac{n}{s^2 + (\\bar Y - \\mu_0)^2}\\Bigg)^{-1} \\\\\n&= \\frac{s^2 + (\\bar Y - \\mu_0)^2}{n}\n\\\\\n\\pmb S_{1n}(\\pmb \\theta_{10}) &= S_{\\mu n}(\\mu_0,\\tilde{\\sigma}^2) \\\\\n&=\\frac{1}{\\tilde\\sigma^2} \\sum_{i = 1}^n (Y_i - \\mu_0) \\\\\n&= \\frac{1}{s^2 + (\\bar Y - \\mu_0)^2}  \\sum_{i = 1}^n (Y_i - \\mu_0) \\\\\n&= \\frac{1}{s^2 + (\\bar Y - \\mu_0)^2} \\sum_{i = 1}^n Y_i - \\sum_{i = 1}^n \\mu_0 \\\\\n&= \\frac{n\\bar Y - n\\mu_0}{s^2 + (\\bar Y - \\mu_0)^2} \\\\\n&= \\frac{n(\\bar Y - \\mu_0)}{s^2 + (\\bar Y - \\mu_0)^2} \\\\\nT_S &=\\pmb S_{1n}(\\pmb \\theta_{10})^T \\Bigg( \\tilde{\\pmb I}_{11}- \\frac{(\\tilde{\\pmb I}_{12})^2}{\\tilde{\\pmb I}_{22}} \\Bigg)^{-1} \\pmb S_{1n}(\\pmb \\theta_{10}) \\\\\n&=  \\frac{n(\\bar Y - \\mu_0)}{s^2 + (\\bar Y - \\mu_0)^2} \\Bigg(\\frac{s^2 + (\\bar Y - \\mu_0)^2}{n} \\Bigg) \\frac{n(\\bar Y - \\mu_0)}{s^2 + (\\bar Y - \\mu_0)^2} \\\\\n&= \\frac{n\\Big(s^2 + (\\bar Y - \\mu_0)^2 \\Big) n(\\bar Y - \\mu_0)^2}{n\\Big(s^2 + (\\bar Y - \\mu_0)^2 \\Big)s^2 + (\\bar Y - \\mu_0)^2}  \\\\\n&= \\frac{n(\\bar Y - \\mu_0)^2}{s^2 + (\\bar Y - \\mu_0)^2}\n\\end{split}\n\\]\nFor the Likelihood Ratio test statistic, recall that\n\\[\nT_{LR} = -2\\Big\\{ \\ell(\\tilde{\\pmb \\theta}) - \\ell(\\hat{\\pmb \\theta}) \\Big\\}\n\\]\nWe have\n\\[\n\\begin{split}\n\\ell(\\mu, \\sigma^2) &= -n/2\\ln(2\\pi) - n/2\\ln[\\sigma^2] - \\frac{1}{2\\sigma^2} \\sum_{i = 1}^n (Y_i - \\mu)^2 \\\\\n\\ell(\\tilde{\\pmb \\theta}) &= \\ell(\\mu_0, \\tilde \\sigma^2) \\\\\n&=  -n/2\\ln(2\\pi) - n/2\\ln[\\tilde\\sigma^2] - \\frac{1}{2\\tilde\\sigma^2} \\sum_{i = 1}^n (Y_i - \\mu_0)^2 \\\\\n\\text{Recall that } \\tilde \\sigma^2 &=  \\frac{1}{n}\\sum_{i = 1}^n (Y_i - \\mu_0)^2 \\\\\n\\text{Thus, } -\\frac{1}{2\\tilde\\sigma^2} &= -\\frac{n}{2\\sum_{i = 1}^n (Y_i - \\mu_0)^2} \\text{ and } \\\\\n\\ell(\\tilde{\\pmb \\theta}) &= -n/2\\ln(2\\pi) - n/2\\ln[\\tilde\\sigma^2] -\\frac{n\\sum_{i = 1}^n (Y_i - \\mu_0)^2}{2\\sum_{i = 1}^n (Y_i - \\mu_0)^2}  \\\\\n&=-n/2\\ln(2\\pi) - n/2\\ln[\\tilde\\sigma^2] -{n}/{2}\\\\\n\\ell(\\hat{\\pmb \\theta}) &= -n/2\\ln(2\\pi) - n/2\\ln[s^2] - \\frac{1}{2s^2} \\sum_{i = 1}^n (Y_i - \\bar Y)^2  \\\\\n&= -n/2\\ln(2\\pi) - n/2\\ln[s^2] - \\frac{1}{2s^2}  ns^2  \\\\\n&= -n/2\\ln(2\\pi) - n/2\\ln[s^2] - n/2  \\\\\nT_{LR} &= -2\\Big\\{ \\ell(\\tilde{\\pmb \\theta}) - \\ell(\\hat{\\pmb \\theta}) \\Big\\} \\\\\n&= -2\\Big[ - n/2\\ln[\\tilde \\sigma^2]  + n/2\\ln[s^2] \\Big] \\\\\n&= 2 \\Big[n/2\\ln[\\tilde \\sigma^2]  - n/2\\ln[s^2] \\Big] \\\\\n&= 2\\Big[n/2 \\Big(\\ln[\\tilde \\sigma^2]  - \\ln[s^2] \\Big)  \\Big] \\\\\n&= n \\ln \\Big(\\frac{\\tilde \\sigma^2}{s^2} \\Big) \\\\\n&= n \\ln \\Bigg(\\frac{s^2 + (\\bar Y - \\mu_0)^2}{s^2} \\Bigg) \\\\\n&= n \\ln \\Bigg(1 + \\frac{(\\bar Y - \\mu_0)^2}{s^2} \\Bigg)\n\\end{split}\n\\]"
  },
  {
    "objectID": "week4.html#power-function-and-consistency-of-tests",
    "href": "week4.html#power-function-and-consistency-of-tests",
    "title": "4  Hypothesis testing",
    "section": "4.8 Power function and consistency of tests",
    "text": "4.8 Power function and consistency of tests\nThe power function of a test with rejection region \\(R\\) based on a sample of size \\(n\\) is given by\n\\[\\beta_n(\\pmb \\theta) = P_{\\pmb \\theta}(\\pmb Y \\in R)\\]\nIf \\(\\pmb \\theta \\notin \\Theta_0\\), \\(\\beta_n(\\pmb \\theta)\\) is the probability of detecting the alternative: \\(\\beta_n(\\pmb \\theta) =1 - \\beta(R)\\).\nIf \\(\\pmb \\theta \\in \\Theta_0\\), \\(\\beta_n(\\pmb \\theta)\\) is the Type I error probability: \\(\\beta_n(\\pmb \\theta) = \\alpha(R)\\).\nA sequence of tests is called consistent against a specific alternative \\(\\pmb \\theta_1\\) if \\(\\beta_n(\\pmb \\theta_1) \\xrightarrow{n \\rightarrow \\infty} 1\\).\nFor example, for a test of \\(H_0: \\theta \\le \\theta_0\\) versus \\(H_a: \\theta > \\theta_0\\), say we use a test statistic \\(T_n\\) with rejection region\n\\[\n\\frac{\\sqrt n(T_n - \\theta_0)}{\\sigma(\\theta_0)} > z_{\\alpha}\n\\]\nThen, the test is consistent against all alternatives \\(\\theta_1 > \\theta_0\\) if, for all \\(\\theta\\),\n\\[\n\\begin{split}\n\\sqrt{n}(T_n - \\theta) &\\xrightarrow D N\\Big(0, \\sigma^2(\\theta)\\Big) \\text{ or, equivalently,} \\\\\n\\frac{\\sqrt{n}(T_n - \\theta)}{\\sigma(\\theta)} &\\xrightarrow D N(0, 1)\n\\end{split}\n\\]\n\n4.8.1 Power example: \\(N(\\mu, \\sigma^2)\\) with \\(\\sigma^2\\) known\nRecall the example in which we obtained the test statistics for \\(Y_i \\sim N(\\mu, \\sigma^2)\\) with \\(\\sigma^2\\) known and a simple null hypothesis \\(H_0: \\mu = \\mu_0\\). We obtained\n\\[\n\\begin{split}\nT_W = T_S = T_{LR} &= \\frac{n(\\bar Y -\\mu_0)^2}{\\sigma^2} \\\\\n\\text{We reject } H_0 \\text{ if } T_W &> \\chi^2_{1,\\alpha}\\\\\nZ_W = Z_S = Z_{LR} &= \\frac{\\sqrt n(\\bar Y -\\mu_0)}{\\sigma} \\\\\n& = \\frac{\\bar Y - \\mu_0}{\\sigma/\\sqrt{n}}\\\\\n\\text{We reject } H_0 \\text{ if } Z_W &> z_{\\alpha}\\\\\n\\end{split}\n\\]\nThe corresponding power function is\n\\[\n\\begin{split}\n\\beta_n(\\mu) &= P_\\mu (\\pmb Y \\in R) \\\\\n&= P_\\mu\\Bigg(\\frac{\\bar Y - \\mu_0}{\\sigma/\\sqrt n} >  z_{\\alpha}\\Bigg) \\\\\n&= P_\\mu\\Bigg(\\frac{\\bar Y - \\mu + \\mu - \\mu_0}{\\sigma/\\sqrt n} >  z_{\\alpha}\\Bigg) \\\\\n&= P_\\mu\\Bigg(\\frac{\\bar Y - \\mu}{\\sigma/\\sqrt n} >  z_{\\alpha} - \\frac{\\mu - \\mu_0}{\\sigma/\\sqrt n}\\Bigg) \\\\\n\\text{Recall that } E[\\bar Y] &= \\mu; ~\\text{Var}[\\bar Y] = \\sigma^2/n; ~\\text{SD}[\\bar Y] =\\sigma/\\sqrt n. \\text{ Thus}, \\\\\n\\beta_n(\\mu) &= P_\\mu \\Bigg(Z > z_{\\alpha} - \\frac{\\mu - \\mu_0}{\\sigma/\\sqrt n} \\Bigg)  \\\\\n&= 1 - P_\\mu \\Bigg(Z \\le z_{\\alpha} - \\frac{\\mu - \\mu_0}{\\sigma/\\sqrt n} \\Bigg) \\\\\n&= 1 - \\Phi\\Bigg( z_{\\alpha} - \\frac{\\mu - \\mu_0}{\\sigma/\\sqrt n}\\Bigg)\\\\\n\\text{Recall that } 1-\\Phi(x) &= \\Phi(-x). \\text{ Thus, } \\\\\n\\beta_n(\\mu) &= \\Phi\\Bigg(\\frac{\\mu - \\mu_0}{\\sigma/\\sqrt n} - z_{\\alpha} \\Bigg) \\\\\n\\end{split}\n\\]\nSuppose we require a 5% level test with a minimum power of 80% to reject \\(H_0: \\mu = \\mu_0\\) in favor of \\(H_a: \\mu > \\mu_0\\) if \\(\\mu \\ge \\mu_0 + \\sigma\\). What is the required sample size?\nSince \\(\\beta_n(\\mu_1)\\) is increasing in \\(\\mu_1\\), we need to determine n such that \\(\\beta_n(\\mu_0 + \\sigma) \\ge 0.8\\).\n\\[\n\\begin{split}\n\\beta_n(\\mu_1) &= \\Phi\\Bigg(\\frac{\\mu_1 - \\mu_0}{\\sigma/\\sqrt n} - z_{\\alpha} \\Bigg) \\\\\n\\beta_n(\\mu_0 +\\sigma) &= \\Phi\\Bigg(\\frac{\\mu_0 +\\sigma - \\mu_0}{\\sigma/\\sqrt n} - z_{\\alpha} \\Bigg) \\\\\n&= \\Phi(\\sqrt n - z_{\\alpha}) \\\\\n&\\ge 0.8 \\\\\n\\sqrt n - z_{0.05} &\\ge \\Phi^{-1}(0.8) \\\\\n\\sqrt n &\\ge \\Phi^{-1}(0.8)  + \\Phi^{-1}(0.95) \\\\\nn &\\ge \\Big(\\Phi^{-1}(0.8)  + \\Phi^{-1}(0.95)\\Big)^2 \\\\\n&\\approx 6.18\n\\end{split}\n\\]\nThus, we need \\(n = 7\\) datapoints.\n\n\n4.8.2 Power example: \\(N(\\mu, \\sigma^2)\\) with \\(\\mu\\) and \\(\\sigma^2\\) unknown (t-test)\nEarlier, we obtained the Wald statistic for a t-test\n\\[\n\\begin{split}\nT_W &= \\frac{n(\\bar Y - \\mu_0)^2}{S^2} \\\\\nZ_W &= \\frac{\\sqrt n(\\bar Y - \\mu_0)}{S} \\\\\n\\end{split}\n\\]\nWe know, by the CLT,\n\\[\n\\begin{split}\n&\\sqrt n(\\bar Y - \\mu) \\xrightarrow D N(0, \\sigma^2) \\\\\n&\\frac{\\sqrt n(\\bar Y - \\mu_0)}{\\sigma}  \\xrightarrow D N(0, 1) \\\\\n&\\text{We also know } S_n \\text{ is a } \\text{consistent estimator of } \\sigma: \\\\\n&S_n \\xrightarrow P \\sigma \\text{ for } n \\rightarrow \\infty \\\\\n&\\text{Using } \\text{Slutsky's lemma, } \\\\\n&\\frac{\\sqrt n(\\bar Y - \\mu_0)}{S} \\xrightarrow{D} N(0,1)\n\\end{split}\n\\]\nThus, \\(\\displaystyle Z_W = \\frac{\\sqrt n(\\bar Y - \\mu_0)}{S} > z_{\\alpha}\\) is consistent against all alternatives.\n\n\n4.8.3 Power example: Consistency for asymptotically normal test statistics\nSay we have rejection region\n\\[\n\\frac{\\sqrt n(T_n - \\theta_0)}{\\sigma(\\theta_0)} > z_{\\alpha}\n\\]\nand we know\n\\[\n\\sqrt n(T_n - \\theta) \\xrightarrow D N\\Big(0, \\sigma^2(\\theta)\\Big) \\text{ for all } \\theta\n\\]\nThen, for some alternative \\(\\theta_1\\), we have the power function\n\\[\n\\begin{split}\n\\beta_n(\\theta_1) &= P_\\theta \\Bigg(\\frac{\\sqrt n(T_n - \\theta_0)}{\\sigma(\\theta_0)} > z_{\\alpha} \\Bigg)\\\\\n&= P_\\theta\\Big({\\sqrt n(T_n - \\theta_1 + \\theta_1- \\theta_0)} > z_{\\alpha}{\\sigma(\\theta_0)} \\Big)\\\\\n&= P_\\theta\\Big({\\sqrt n(T_n - \\theta_1)} > z_{\\alpha}{\\sigma(\\theta_0) - \\sqrt n( \\theta_1- \\theta_0)} \\Big)\\\\\n\\text{If } \\theta_1 &> \\theta_0, \\text{ then} \\\\\n&\\lim_{n \\rightarrow \\infty} z_{\\alpha}{\\sigma(\\theta_0) - \\sqrt n( \\theta_1- \\theta_0)} = -\\infty \\\\\n&\\text{Because }\\sqrt n(T_n - \\theta) \\xrightarrow D N\\Big(0, \\sigma^2(\\theta)\\Big) \\text{ for all } \\theta, \\\\\n&\\lim_{n \\rightarrow \\infty} P_\\theta\\Big({\\sqrt n(T_n - \\theta_1)} > z_{\\alpha}{\\sigma(\\theta_0) - \\sqrt n( \\theta_1- \\theta_0)} \\Big) = 1. \\text{ Hence, } \\\\\n\\beta_n(\\theta_1) &\\xrightarrow{n \\rightarrow \\infty} 1 \\text{ for all } \\theta_1 > \\theta_0.\n\\end{split}\n\\]\n\n\n4.8.4 Power example: Consistency for asymptotically normal test statistics with nuisance parameters\nSay we have rejection region\n\\[\n\\frac{\\sqrt n (T_n - \\theta_0)}{S_n} > z_{\\alpha}\n\\]\nWe know\n\\[\n\\begin{split}\n\\sqrt n(T_n - \\theta) &\\xrightarrow D N\\Big(0, \\sigma^2(\\theta,\\eta) \\Big) \\text{ for all } \\theta \\text{ and } \\eta\\\\\nS^2_n &\\text{ is a consistent estimator of } \\sigma^2(\\theta,\\eta)\n\\end{split}\n\\]\nThen, for some alternative \\(\\theta_1\\), we have the power function\n\\[\n\\begin{split}\n\\beta_n(\\theta_1) &= P\\Bigg(\\frac{\\sqrt n (T_n - \\theta_0)}{S_n} > z_{\\alpha} \\Bigg) \\\\\n&= P\\Big({\\sqrt n (T_n -\\theta_1 + \\theta_1 - \\theta_0)} > z_{\\alpha}{S_n} \\Big) \\\\\n&= P\\Big({\\sqrt n (T_n -\\theta_1)} > z_{\\alpha}{S_n} -\\sqrt n(\\theta_1 - \\theta_0) \\Big) \\\\\n\\text{If } \\theta_1 &> \\theta_0, \\text{ then} \\\\\n&\\lim_{n \\rightarrow \\infty} z_{\\alpha}{S_n} -\\sqrt n(\\theta_1 - \\theta_0) = -\\infty \\\\\n&\\text{Because } \\sqrt n(T_n - \\theta) \\xrightarrow D N\\Big(0, \\sigma^2(\\theta,\\eta) \\Big) \\text{ for all } \\theta \\text{ and } \\eta, \\\\\n&\\lim_{n \\rightarrow \\infty} P\\Big({\\sqrt n (T_n -\\theta_1)} > z_{\\alpha}{S_n} -\\sqrt n(\\theta_1 - \\theta_0) \\Big) = 1.  \\text{ Hence, } \\\\\n\\beta_n(\\theta_1) &\\xrightarrow{n \\rightarrow \\infty} 1 \\text{ for all } \\theta_1 > \\theta_0.\n\\end{split}\n\\]"
  },
  {
    "objectID": "week4.html#asymptotic-power-approximation-and-sample-size",
    "href": "week4.html#asymptotic-power-approximation-and-sample-size",
    "title": "4  Hypothesis testing",
    "section": "4.9 Asymptotic power approximation and sample size",
    "text": "4.9 Asymptotic power approximation and sample size\nFor a given alternative \\(\\theta_1\\) and sample size \\(n\\), we can write\n\\[\n\\begin{split}\n\\theta_1 &= \\theta_0 + \\frac{\\Delta}{\\sqrt n} \\\\\n\\Delta &= \\sqrt n(\\theta_1 - \\theta_0)\n\\end{split}\n\\]\nAn approximation of the power for the alternative \\(\\theta_1\\) is then given by\n\\[\n\\begin{split}\n\\beta_n(\\theta_1) &\\approx \\Phi \\Bigg(\\frac{\\sqrt n(\\theta_1 - \\theta_1)}{\\sigma(\\theta_0)} - z_{\\alpha} \\Bigg) \\\\\n&= \\Phi \\Bigg(\\frac{\\Delta}{\\sigma(\\theta_0)} - z_{\\alpha} \\Bigg) \\\\\n\\end{split}\n\\]\nAnd the minimum required sample size is given by\n\\[\nn \\ge \\frac{(z_{\\alpha} + z_{1 -\\beta})^2}{(\\theta_1 - \\theta_0)^2}\\sigma^2(\\theta_0)\n\\]\n\n4.9.1 Asymptotic power approximation: Derivation and example\nSay we have rejection region\n\\[\n\\begin{split}\n&\\frac{\\sqrt n(T_n - \\theta_0)}{\\sigma(\\theta_0)} > z_{\\alpha} \\\\\n\\beta_n(\\theta) &= P_\\theta\\Bigg(\\frac{\\sqrt n(T_n - \\theta_0)}{\\sigma(\\theta_0) } > z_{\\alpha}  \\Bigg) \\\\\n&\\text{Define a local alternative } \\theta_1 \\\\\n\\theta_1 &= \\theta_0 + \\frac{\\Delta}{\\sqrt n} \\Leftrightarrow \\Delta =\\sqrt{n}(\\theta_1 - \\theta_0) \\\\\n\\beta_n(\\theta) &= P_\\theta\\Bigg(\\frac{\\sqrt n(T_n - \\theta_1 + \\theta_1 - \\theta_0)}{\\sigma(\\theta_0) } > z_{\\alpha}  \\Bigg) \\\\\n&= P_\\theta\\Bigg(\\frac{\\sqrt n(T_n - \\theta_1)}{\\sigma(\\theta_0) } > z_{\\alpha} - \\frac{\\sqrt n(\\theta_1 - \\theta_0)}{\\sigma(\\theta_0)}  \\Bigg) \\\\\n&\\text{We know}\\\\\n&\\sqrt n(T_n - \\theta) \\xrightarrow D N\\Big(0, \\sigma^2(\\theta) \\Big) \\text{ for all } \\theta \\text{ and } \\\\\n&\\frac{\\sqrt n(T_n - \\theta)}{\\sigma(\\theta)} \\xrightarrow D N\\Big(0, 1 \\Big) \\text{ for all } \\theta \\\\\n&\\text{We can also see that that} \\\\\n\\lim_{n \\rightarrow \\infty } \\theta_1 &= \\lim_{n \\rightarrow \\infty }\\theta_0 + \\frac{\\Delta}{\\sqrt n} \\\\\n&= \\theta_0 \\\\\n&\\text{If } \\sigma^2(\\theta) \\text{ is continous, then} \\\\\n&\\sigma^2(\\theta_1) \\rightarrow \\sigma^2(\\theta_0) \\text{ when } \\theta_1 \\rightarrow \\theta_0\\\\\n&\\text{Thus, when } \\theta_1 \\xrightarrow{n \\rightarrow \\infty} \\theta_0, \\\\\n&\\frac{\\sqrt n(T_n - \\theta_1)}{\\sigma(\\theta_0) } \\xrightarrow D N(0,1) \\\\\n\\end{split}\n\\]\nWe have obtained the asymptotic power of the test,\n\\[\n\\begin{split}\n\\beta_n(\\theta_1) &\\xrightarrow{n \\rightarrow \\infty} P_\\theta\\Bigg(\\frac{\\sqrt n(T_n - \\theta_1)}{\\sigma(\\theta_0) } > z_{\\alpha} - \\frac{\\sqrt n(\\theta_1 - \\theta_0)}{\\sigma(\\theta_0)}  \\Bigg) \\\\\n&= P_\\theta\\Bigg(Z > z_{\\alpha} - \\frac{\\sqrt n(\\theta_1 - \\theta_0)}{\\sigma(\\theta_0)}  \\Bigg) \\\\\n&= 1 - P_\\theta\\Bigg(Z \\le z_{\\alpha} - \\frac{\\sqrt n(\\theta_1 - \\theta_0)}{\\sigma(\\theta_0)}   \\Bigg) \\\\\n&= 1 - \\Phi\\Bigg(z_{\\alpha} - \\frac{\\sqrt n(\\theta_1 - \\theta_0)}{\\sigma(\\theta_0)}   \\Bigg) \\\\\n&= \\Phi\\Bigg( \\frac{\\sqrt n(\\theta_1 - \\theta_0)}{\\sigma(\\theta_0)} - z_{\\alpha}   \\Bigg) \\\\\n&= \\Phi\\Bigg( \\frac{\\Delta}{\\sigma(\\theta_0)} - z_{\\alpha}   \\Bigg)\n\\end{split}\n\\]\n\n\n4.9.2 Asymptotic power approximation: Bernoulli example\nSuppose iid \\(Y_i \\sim\\) Bernoulli(\\(p\\)). For \\(T_n = \\bar Y\\), we have, by the CLT,\n\\[\n\\begin{split}\n\\frac{\\sqrt n(T_n - p)}{\\sqrt{p(1 - p)}} &\\xrightarrow D N(0,1)\\\\\n\\sigma^2(p) = p(1 - p) &\\text{ is continuous}\n\\end{split}\n\\]\nThen, the approximate power for the test \\(H_0: p \\le p_0\\) versus \\(H_a: p > p_0\\) against a fixed alternative \\(p_1\\) equals\n\\[\n\\begin{split}\n\\beta_n(p_1) &\\approx \\Phi\\Bigg( \\frac{\\sqrt n(p_1 - p_0)}{\\sigma(p_0)} - z_{\\alpha}   \\Bigg) \\\\\n&=\\Phi\\Bigg( \\frac{\\sqrt n(p_1 - p_0)}{\\sqrt{p_0(1 - p_0)}} - z_{\\alpha}   \\Bigg) \\\\\n\\end{split}\n\\]"
  },
  {
    "objectID": "week4.html#asymptotic-equivalence",
    "href": "week4.html#asymptotic-equivalence",
    "title": "4  Hypothesis testing",
    "section": "4.10 Asymptotic equivalence",
    "text": "4.10 Asymptotic equivalence\nIf the rejection regions of two tests correspond to \\(V_n > u_{\\alpha}\\) and \\(V'_n > u_{\\alpha}\\), then the tests will be asymptotically equivalent if\n\\[\nP(V_n > u_{\\alpha}, V'n \\le u_{\\alpha}) +P(V_n \\le u_{\\alpha}, V'n > u_{\\alpha}) \\rightarrow 0 \\text{ when } n \\rightarrow \\infty\n\\]\nThat is, as \\(n \\rightarrow \\infty\\), the probability of the two tests giving discordant results goes to zero."
  },
  {
    "objectID": "week5.html",
    "href": "week5.html",
    "title": "5  Confidence intervals",
    "section": "",
    "text": "An interval estimate of a parameter \\(\\theta\\) is any pair of functions \\(L(\\pmb x)\\) and \\(U(\\pmb x)\\) of a sample \\(\\pmb x\\) that satisfy \\(L(\\pmb x) \\le U(\\pmb x)\\) for all \\(\\pmb x \\in \\chi\\). \\([L(\\pmb X), U(\\pmb X)]\\) is called a random interval, confidence interval, or confidence set.\nThe coverage probability of an interval estimator \\([L(\\pmb X), U(\\pmb X)]\\) of a parameter \\(\\theta\\) is the probability \\(P_\\theta\\Big(\\theta \\in [L(\\pmb X), U(\\pmb X)] \\Big)\\). The confidence coefficient/level of an interval estimator \\([L(\\pmb X), U(\\pmb X)]\\) of a parameter \\(\\theta\\) is the infimum of the coverage probabilities \\(\\inf_\\theta P_\\theta\\Big(\\theta \\in [L(\\pmb X), U(\\pmb X)] \\Big)\\).\nNote: in the probability statement \\(P_\\theta\\Big(\\theta \\in [L(\\pmb X), U(\\pmb X)] \\Big)\\), \\(\\theta\\) is fixed and \\(\\pmb X\\) is random: the probability statement refers to \\(\\pmb X\\), not \\(\\theta\\)."
  },
  {
    "objectID": "week5.html#duality-between-test-statistics-and-confidence-sets",
    "href": "week5.html#duality-between-test-statistics-and-confidence-sets",
    "title": "5  Confidence intervals",
    "section": "5.1 Duality between test statistics and confidence sets",
    "text": "5.1 Duality between test statistics and confidence sets\nFor a level \\(\\alpha\\) test with rejection region \\(R(\\theta_0)\\), \\(C(\\pmb X) = \\Big[\\theta_0: \\pmb X \\notin R(\\theta_0) \\Big]\\) is a \\(1 - \\alpha\\) confidence set. Thus, we can always obtain the confidence set \\(C(\\pmb X)\\) by inverting a test.\nFor a \\(1 - \\alpha\\) confidence set \\(C(\\pmb X)\\), \\(R(\\theta_0) = \\Big[\\pmb X : \\theta_0 \\notin C(\\pmb X) \\Big]\\) is the rejection region of a level \\(\\alpha\\) test."
  },
  {
    "objectID": "week5.html#example-obtaining-a-confidence-interval-by-inverting-the-t-test",
    "href": "week5.html#example-obtaining-a-confidence-interval-by-inverting-the-t-test",
    "title": "5  Confidence intervals",
    "section": "5.2 Example: Obtaining a confidence interval by inverting the t-test",
    "text": "5.2 Example: Obtaining a confidence interval by inverting the t-test\nThe t-test for \\(H_0: \\mu = \\mu_0\\) vs. \\(H_a: \\mu \\ne \\mu_0\\) rejects \\(H_0\\) if\n\\[\n\\Bigg| \\frac{\\bar X - \\mu_0}{S/\\sqrt n} \\Bigg| > t_{n-1, \\frac{\\alpha}{2}}\n\\]\nThis is an \\(\\alpha\\) level test. Derive the appropriate confidence \\(1 - \\alpha\\) interval. We have\n\\[\n\\begin{split}\nR(\\mu_0) &= \\Bigg[\\pmb x:  \\Bigg| \\frac{\\bar x - \\mu_0}{s/\\sqrt n} \\Bigg| > t_{n-1, \\frac{\\alpha}{2}}\\Bigg] \\\\\n&=\\Bigg[\\pmb x:   \\frac{\\bar x - \\mu_0}{s/\\sqrt n} > t_{n-1, \\frac{\\alpha}{2}} ~\\text{ or }~ -\\Bigg(\\frac{\\bar x - \\mu_0}{s/\\sqrt n} \\Bigg) > t_{n-1, \\frac{\\alpha}{2}}  \\Bigg] \\\\\n&=\\Bigg[\\pmb x:   {\\bar x - \\mu_0} > t_{n-1, \\frac{\\alpha}{2}}\\frac{s}{\\sqrt n} ~\\text{ or }~ \\frac{\\bar x - \\mu_0}{s/\\sqrt n}  < -t_{n-1, \\frac{\\alpha}{2}}  \\Bigg] \\\\\n&=\\Bigg[\\pmb x:   {\\bar x} > \\mu_0 + t_{n-1, \\frac{\\alpha}{2}}\\frac{s}{\\sqrt n} ~\\text{ or }~ {\\bar x}  < \\mu_0 -t_{n-1, \\frac{\\alpha}{2}}\\frac{s}{\\sqrt n}  \\Bigg] \\\\\nC(\\pmb x) &= \\Big[\\mu_0: \\pmb x \\notin R(\\mu_0) \\Big] \\\\\n&= \\Bigg[\\mu_0:   {\\bar x} \\le \\mu_0 + t_{n-1, \\frac{\\alpha}{2}}\\frac{s}{\\sqrt n} ~\\text{ and }~ {\\bar x}  \\ge \\mu_0 -t_{n-1, \\frac{\\alpha}{2}}\\frac{s}{\\sqrt n}  \\Bigg] \\\\\n&= \\Bigg[\\mu_0:   {\\bar x} - t_{n-1, \\frac{\\alpha}{2}}\\frac{s}{\\sqrt n} \\le \\mu_0  ~\\text{ and }~ {\\bar x} +t_{n-1, \\frac{\\alpha}{2}}\\frac{s}{\\sqrt n}  \\ge \\mu_0   \\Bigg] \\\\\n&= \\Bigg[\\mu_0:   {\\bar x} - t_{n-1, \\frac{\\alpha}{2}}\\frac{s}{\\sqrt n} \\le \\mu_0  ~\\text{ and }~ \\mu_0  \\le{\\bar x} +t_{n-1, \\frac{\\alpha}{2}}\\frac{s}{\\sqrt n}    \\Bigg] \\\\\n&= \\Bigg[\\mu_0:   {\\bar x} - t_{n-1, \\frac{\\alpha}{2}}\\frac{s}{\\sqrt n} \\le \\mu_0  \\le{\\bar x} +t_{n-1, \\frac{\\alpha}{2}}\\frac{s}{\\sqrt n}    \\Bigg] \\\\\n&\\text{We have obtained the } 1- \\alpha \\text{ confidence interval}\\\\\nC(\\pmb X) &= \\Bigg[{\\bar X} - t_{n-1, \\frac{\\alpha}{2}}\\frac{S}{\\sqrt n}, {\\bar X} +t_{n-1, \\frac{\\alpha}{2}}\\frac{S}{\\sqrt n} \\Bigg]\n\\end{split}\n\\]"
  },
  {
    "objectID": "week5.html#interval-estimators-based-on-pivots",
    "href": "week5.html#interval-estimators-based-on-pivots",
    "title": "5  Confidence intervals",
    "section": "5.3 Interval estimators based on pivots",
    "text": "5.3 Interval estimators based on pivots\nA random variable \\(Q(\\pmb X,\\theta)\\) is a pivotal quantity (or pivot) if the distribution of \\(Q(\\pmb X, \\theta)\\) is independent of all parameters. If \\(\\pmb X \\sim F(\\pmb x | \\theta)\\) then \\(Q(\\pmb X, \\theta)\\) given \\(\\theta\\) has the same distribution for all values of \\(\\theta\\). A pivot that is also a statistic (a function of the observed data \\(\\pmb X\\)) is called an ancillary statistic.\nFor example, the distribution of the \\(T\\) statistic \\(Q(\\pmb X, \\mu, \\sigma^2) =\\displaystyle T = \\frac{\\bar X - \\mu}{S/\\sqrt n} \\sim t_{n-1}\\) does not depend on the parameters \\(\\mu\\) and \\(\\sigma^2\\). Thus, \\(T\\) is a pivotal quantity, and using the pivot \\(T \\sim t_{n-1}\\) , we have\n\\[P\\Big(-t_{n-1,\\frac{\\alpha}{2}}  \\le T \\le t_{n-1,\\frac{\\alpha}{2}} \\Big) = 1-\\alpha\\]\nIf \\(Q(\\pmb X, \\theta)\\) is a pivot, we can find numbers \\(a\\) and \\(b\\) such that\n\\[P_\\theta\\Big(a \\le Q(\\pmb X,\\theta) \\le b \\Big) \\ge 1 - \\alpha\\]\nThen, \\[C(\\pmb x) = \\Big[\\theta: a \\le Q(\\pmb x, \\theta) \\le b \\Big]\\] and \\(C(\\pmb X)\\) is a \\(1 - \\alpha\\) confidence set for \\(\\theta\\).\nIf \\(Q(\\pmb x, \\theta)\\) is an increasing function of \\(\\theta\\), then \\(C(\\pmb x)\\) has the form \\[Q^{-1}(\\pmb x, a) \\le \\theta \\le Q^{-1}(\\pmb x, b)\\]\nIf \\(Q(\\pmb x, \\theta)\\) is a decreasing function of \\(\\theta\\), then \\(C(\\pmb x)\\) has the form \\[Q^{-1}(\\pmb x, b) \\le \\theta \\le Q^{-1}(\\pmb x, a)\\]"
  },
  {
    "objectID": "week5.html#example-confidence-interval-based-on-the-t-pivot",
    "href": "week5.html#example-confidence-interval-based-on-the-t-pivot",
    "title": "5  Confidence intervals",
    "section": "5.4 Example: Confidence interval based on the T pivot",
    "text": "5.4 Example: Confidence interval based on the T pivot\nUsing the pivot \\[T = \\frac{\\bar X - \\mu}{S/\\sqrt n} \\sim t_{n-1},\\]\nwe have that\n\\[\nP\\Big(-t_{n-1,\\frac{\\alpha}{2}} \\le T \\le t_{n-1,\\frac{\\alpha}{2}} \\Big) = 1 - \\alpha\n\\]\nThus,\n\\[\n\\begin{split}\nP\\Big(-t_{n-1,\\frac{\\alpha}{2}} \\le T \\le t_{n-1,\\frac{\\alpha}{2}} \\Big)\n&= P\\Big(-t_{n-1,\\frac{\\alpha}{2}} \\le \\frac{\\bar X - \\mu}{S/\\sqrt n}\\le t_{n-1,\\frac{\\alpha}{2}} \\Big) \\\\\n&=P\\Big(-t_{n-1,\\frac{\\alpha}{2}}\\frac{S}{\\sqrt n} \\le {\\bar X - \\mu}\\le t_{n-1,\\frac{\\alpha}{2}}\\frac{S}{\\sqrt n} \\Big) \\\\\n&=P\\Big(-\\bar X -t_{n-1,\\frac{\\alpha}{2}}\\frac{S}{\\sqrt n} \\le { - \\mu}\\le -\\bar X + t_{n-1,\\frac{\\alpha}{2}}\\frac{S}{\\sqrt n} \\Big) \\\\\n&=P\\Big(\\bar X +t_{n-1,\\frac{\\alpha}{2}}\\frac{S}{\\sqrt n} \\ge { \\mu}\\ge \\bar X - t_{n-1,\\frac{\\alpha}{2}}\\frac{S}{\\sqrt n} \\Big) \\\\\n&=P\\Big(\\bar X - t_{n-1,\\frac{\\alpha}{2}}\\frac{S}{\\sqrt n} \\le { \\mu}\\le  \\bar X +t_{n-1,\\frac{\\alpha}{2}}\\frac{S}{\\sqrt n}  \\Big) \\\\\n&= 1 - \\alpha\n\\end{split}\n\\]\nWe have obtained a \\(1 - \\alpha\\) confidence interval\n\\[\n\\Big[\\bar X - t_{n-1,\\frac{\\alpha}{2}}\\frac{S}{\\sqrt n},~ \\bar X +t_{n-1,\\frac{\\alpha}{2}}\\frac{S}{\\sqrt n}  \\Big]\n\\]"
  },
  {
    "objectID": "week5.html#example-confidence-interval-for-sigma2-based-on-a-pivot",
    "href": "week5.html#example-confidence-interval-for-sigma2-based-on-a-pivot",
    "title": "5  Confidence intervals",
    "section": "5.5 Example: Confidence interval for \\(\\sigma^2\\) based on a pivot",
    "text": "5.5 Example: Confidence interval for \\(\\sigma^2\\) based on a pivot\nWe have iid \\(X_i \\sim N(\\mu,\\sigma^2)\\). Since \\((n-1)S^2/\\sigma^2 \\sim \\chi^2_{n-1}\\), we can find a \\(1-\\alpha\\) confidence interval for \\(\\sigma^2\\) by choosing \\(a\\) and \\(b\\) such that\n\\[\nP\\Bigg(a \\le \\frac{(n-1)S^2}{\\sigma^2} \\le b \\Bigg) = P(a \\le \\chi^2_{n-1} \\le b) = 1 - \\alpha\n\\]\nThis leads to a \\(1 - \\alpha\\) confidence interval for \\(\\sigma^2\\):\n\\[\n\\Bigg[\\frac{(n-1)S^2}{b}, \\frac{(n-1)S^2}{a}   \\Bigg]\n\\]\nTypically, we spread the probability equally over the two endpoints:\n\\[\nP(a \\le \\chi^2_{n-1} \\le b) = P(-\\chi^2_{n-1,\\alpha/2} \\le \\chi^2_{n-1} \\le \\chi^2_{n-1,\\alpha/2})\n\\]"
  },
  {
    "objectID": "week5.html#example-asymptotic-confidence-coefficient",
    "href": "week5.html#example-asymptotic-confidence-coefficient",
    "title": "5  Confidence intervals",
    "section": "5.6 Example: Asymptotic confidence coefficient",
    "text": "5.6 Example: Asymptotic confidence coefficient\nWe have iid \\(X_i \\sim N(\\mu, \\sigma^2)\\) and rejection region\n\\[\n\\Bigg| \\frac{\\sqrt n (\\bar X - \\mu_0)}{S}  \\Bigg| > z_{\\alpha/2}\n\\]\nThis test has asymptotic level \\(\\alpha\\). An asymptotic \\(1 - \\alpha\\) confidence interval is given by\n\\[\n\\Big[\\bar X - z_{\\alpha/2}\\frac{S}{\\sqrt n},~  \\bar X + z_{\\alpha/2}\\frac{S}{\\sqrt n}\\Big]\n\\]"
  },
  {
    "objectID": "exercises.html",
    "href": "exercises.html",
    "title": "6  Exercises",
    "section": "",
    "text": "Consider a sample \\(X_1, ..., X_n\\) from the Pareto family with unknown parameter \\(a > 2\\) and known \\(b > 0\\). The density function is given by\n\\[\n\\begin{split}\nf(x;a) &= \\frac{ab^a}{x^{a + 1}}, ~x > b \\\\\nE[X] &= \\frac{ab}{a - 1} \\\\\n\\text{Var}[X] &= \\frac{b^2a}{(a-1)^2(a-2)}\n\\end{split}\n\\]\nFind the likelihood function, the log likelihood function, the Score function of \\(a\\), and \\(\\hat a\\).\n\\[\n\\begin{split}\nL(a) &= \\prod_{i = 1}^n \\frac{ab^a}{X_i^{a + 1}} \\\\\n\\ell(a) &= \\sum_{i = 1}^n \\ln a + a\\ln b - (a + 1)\\ln X_i \\\\\n&= n\\ln a + na\\ln b   - (a + 1)\\sum_{i = 1}^n\\ln X_i \\\\\nS_n(a) = \\frac{\\partial}{\\partial a} \\ell(a) &=  \\frac{n}{a} + n\\ln b -\\sum_{i = 1}^n\\ln X_i = 0 \\\\\n&\\Rightarrow \\frac{n}{a} = \\sum_{i = 1}^n\\ln X_i -n\\ln b \\\\\n\\hat a &=\\frac{n}{\\sum_{i = 1}^n\\ln X_i -n\\ln b} \\\\\n&= \\frac{n}{\\sum_{i = 1}^n \\ln(X_i/b)}\n\\end{split}\n\\]\nCalculate the total Fisher information matrix of \\(a\\), compute the asymptotic variance of \\(\\hat a\\), and find the asymptotic distribution of \\(\\hat a\\).\n\\[\n\\begin{split}\nS_i(a) &=   \\frac{\\partial}{\\partial a} \\ln a + a\\ln b - (a + 1)\\ln X_i\\\\\n&= \\frac{1}{a} + \\ln b -\\ln X_i \\\\\nI_1(a) &= -E\\Bigg[\\frac{\\partial}{\\partial a} S_i(a)\\Bigg] \\\\\n&= -E\\Bigg[\\frac{\\partial}{\\partial a} \\frac{1}{a} + \\ln b -\\ln X_i\\Bigg] \\\\\n&= -E\\Big[-a^{-2}  \\Big] \\\\\n&=E\\Big[\\frac{1}{a^2} \\Big] \\\\\n&=\\frac{1}{a^2} \\\\\nI_T(a) = nI_1(a) &= \\frac{n}{a^2} \\\\\n\\text{Var}(\\hat a) = [I_T(a)]^{-1} &=\\frac{a^2}{n}, \\text{ the large-sample variance} \\\\\n\\text{For large n, }\\hat a_n &\\sim N(a, \\frac{a^2}{n})\\\\\nV(a) = [I_1(a)]^{-1} &={a^2}, \\text{ the asymptotic variance} \\\\\n\\text{By the asymptotic} &\\text{ normality of the MLE, } \\\\\n\\sqrt n(\\hat a - a) &\\xrightarrow D  N(0, [I_1(a)]^{-1}) \\text{ as } n \\rightarrow \\infty \\\\\n\\sqrt n(\\hat a - a) &\\xrightarrow D  N(0, a^2) \\text{ as } n \\rightarrow \\infty \\\\\n\\end{split}\n\\]\nFind the MLE of the expected value \\(\\displaystyle \\frac{ab}{a - 1}\\) and its asymptotic variance.\n\\[\n\\begin{split}\n\\text{Let } g(a) &=  \\frac{ab}{a - 1}\\\\\n&\\text{By invariance of the MLE, } \\\\\n\\widehat{g(a)} &= g(\\hat a) \\\\\n\\widehat{E[X]}_{\\text{MLE}} &= g(\\hat a) \\\\\n&= \\frac{\\hat ab}{\\hat a - 1} \\\\\n&\\text{By the asymptotic normality of the MLE, } \\\\\n\\sqrt{n}(\\hat a - a) &\\xrightarrow{D} N(0, a^2) \\\\\n&\\text{By the Delta method, } \\\\\n\\sqrt{n}\\Big(g(\\hat a) - g(a)\\Big) &\\xrightarrow{D} N\\Bigg(0, a^2\\Big[g'(a) \\Big]^2 \\Bigg) \\\\\ng(a) &=  \\frac{ab}{a - 1}\\\\\ng'(a) &=  \\frac{b(a-1) - ab}{(a - 1)^2}\\\\\n&= \\frac{-b}{(a - 1)^2} \\\\\n\\Big[g'(a) \\Big]^2 &= \\frac{b^2}{(a - 1)^4} \\\\\n\\sqrt{n}\\Big(g(\\hat a) - g(a)\\Big) &\\xrightarrow{D} N\\Bigg(0, \\frac{(ab)^2}{(a - 1)^4} \\Bigg) \\\\\nV(\\widehat{E[X]}_{\\text{MLE}}) &=\\frac{(ab)^2}{(a - 1)^4}, \\text{ the asymptotic variance} \\\\\n\\text{Var}(\\widehat{E[X]}_{\\text{MLE}}) &=\\frac{(ab)^2}{n(a - 1)^4}, \\text{ the large-sample variance} \\\\\n\\end{split}\n\\]\nFind the asymptotic variance of the sample mean as an estimator of the expected value and compute the Asymptotic Relative Efficiency (ARE) of the two estimators. By the CLT,\n\\[\n\\begin{split}\n\\sqrt{n}\\Big(\\bar X_n - E[X]\\Big) &\\xrightarrow{D} N\\Bigg(0,  \\frac{b^2a}{(a-1)^2(a-2)}\\Bigg) \\\\\nV(\\widehat{E[X]}_{\\text{MLE}}) &=\\frac{(ab)^2}{(a - 1)^4} \\\\\nV(\\bar X_n) &=\\frac{b^2a}{(a-1)^2(a-2)}\\\\\nARE\\Big(V(\\widehat{E[X]}_{\\text{MLE}}), V(\\bar X_n)\\Big) &= \\frac{V(\\bar X_n)}{V(\\widehat{E[X]}} \\\\\n\\text{If } ARE > 1, \\text{ then }&\\widehat{E[X]}_{\\text{MLE}} \\text{ is more efficient} \\\\\n\\text{If } ARE < 1, \\text{ then }&\\bar X_n \\text{ is more efficient}.\\\\\nARE\\Big(V(\\widehat{E[X]}_{\\text{MLE}}), V(\\bar X_n)\\Big) &=  \\frac{b^2a}{(a-1)^2(a-2)} \\cdot \\frac{(a-1)^4}{a^2b^2} \\\\\n&= \\frac{(a-1)^2}{a(a-2)} \\\\\n&= \\frac{a^2 -2a + 1}{a^2 - 2a} \\\\\n&= 1 + \\frac{1}{a(a-2)}\n\\end{split}\n\\]\nWe know that \\(a > 2\\). We can see that for the lowest values of \\(a\\), the \\(ARE\\) is greater than 1 and thus \\(\\widehat{E[X]}_{\\text{MLE}}\\) is more efficient than \\(\\bar X_n\\). \\(\\bar X_n\\) will be a relatively inefficient estimator for low values of \\(a\\), e.g. \\(2 < a \\le 4\\). However, for larger values of \\(a\\), the \\(ARE\\) converges to 1, and the two estimators will be equally efficient.\nConsider testing \\(H_0: a = a_0\\) vs. \\(H_a: a \\ne a_0\\). Derive two different versions of the Wald test and their rejection region.\nApproach 1: Using \\(\\hat a\\) as a plug-estimator for \\(I_T(a)\\)\n\\[\n\\begin{split}\nI_T(a) &= \\frac{n}{a^2} \\\\\nZ_W &= \\sqrt{I_T(\\hat a)}(\\hat a - a_0) \\xrightarrow D N(0,1)  \\\\\n&= \\sqrt{\\frac{n}{\\hat a^2}}(\\hat a - a_0) \\\\\n&= \\frac{\\sqrt n}{\\hat a}(\\hat a - a_0) \\\\\n&\\text{We have obtained the following equivalent rejection regions:} \\\\\n&\\text{Reject } H_0 \\text{ if } \\\\\n|Z_W| &> \\Phi^{-1}(1 -\\alpha/2) \\\\\n|Z_W| &> z_{\\alpha/2} \\\\\nZ_W &> z_{\\alpha/2} \\text{ or } Z_W < -z_{\\alpha/2} \\\\\nR(a_0) &= \\Bigg[\\pmb x: \\Bigg|\\frac{\\sqrt n}{\\hat a}(\\hat a - a_0)\\Bigg| > z_{\\alpha/2} \\Bigg]\n\\end{split}\n\\]\nApproach 2: Evaluating \\(I_T(a)\\) at \\(a_0\\)\n\\[\n\\begin{split}\nI_T(a) &= \\frac{n}{a^2} \\\\\nZ'_W &= \\sqrt{I_T(a_0)}(\\hat a - a_0) \\xrightarrow D N(0,1)  \\\\\n&= \\sqrt{\\frac{n}{a_0^2}}(\\hat a - a_0) \\\\\n&= \\frac{\\sqrt n}{a_0}(\\hat a - a_0) \\\\\n&\\text{We have obtained the following equivalent rejection regions:} \\\\\n&\\text{Reject } H_0 \\text{ if } \\\\\n|Z'_W| &> \\Phi^{-1}(1 -\\alpha/2) \\\\\n|Z'_W| &> z_{\\alpha/2} \\\\\nZ'_W &> z_{\\alpha/2} \\text{ or } Z'_W < -z_{\\alpha/2} \\\\\nR(a_0) &= \\Bigg[\\pmb x: \\Bigg|\\frac{\\sqrt n}{a_0}(\\hat a - a_0)\\Bigg| > z_{\\alpha/2} \\Bigg]\n\\end{split}\n\\]\nConstruct a \\((1-\\alpha)100\\)% confidence interval for both estimators. Compute the confidence intervals for \\(a\\) using \\(\\alpha = 0.05\\), \\(n = 100\\), and \\(\\hat a = 3.5\\).\nApproach 1: Using \\(\\hat a\\) as a plug-estimator for \\(I_T(a)\\)\n\\[\n\\begin{split}\nR(a_0) &= \\Bigg[\\pmb x: \\Bigg|\\frac{\\sqrt n}{\\hat a}(\\hat a - a_0)\\Bigg| > z_{\\alpha/2} \\Bigg] \\\\\nC(\\pmb x) &= \\Big[a_0:\\pmb x \\notin R(a_0) \\Big] \\\\\n&= \\Big[a_0: -z_{\\alpha/2}\\le Z_W \\le z_{\\alpha/2} \\Big] \\\\\n&= \\Big[a_0: -z_{\\alpha/2}\\le \\frac{\\sqrt n}{\\hat a}(\\hat a - a_0) \\le z_{\\alpha/2} \\Big] \\\\\n&=  \\Big[a_0: -\\frac{\\hat a}{\\sqrt n}z_{\\alpha/2}\\le \\hat a - a_0 \\le \\frac{\\hat a}{\\sqrt n}z_{\\alpha/2} \\Big] \\\\\n&=  \\Big[a_0:  -\\hat a -\\frac{\\hat a}{\\sqrt n}z_{\\alpha/2}\\le - a_0 \\le  -\\hat a +\\frac{\\hat a}{\\sqrt n}z_{\\alpha/2} \\Big] \\\\\n&=  \\Big[a_0:  \\hat a +\\frac{\\hat a}{\\sqrt n}z_{\\alpha/2}\\ge a_0 \\ge  \\hat a -\\frac{\\hat a}{\\sqrt n}z_{\\alpha/2} \\Big] \\\\\n&=  \\Big[a_0:  \\hat a -\\frac{\\hat a}{\\sqrt n}z_{\\alpha/2}\\le a_0 \\le  \\hat a +\\frac{\\hat a}{\\sqrt n}z_{\\alpha/2} \\Big] \\\\\nC(\\pmb X) &=  \\Big[\\hat a -\\frac{\\hat a}{\\sqrt n}z_{\\alpha/2},~  \\hat a +\\frac{\\hat a}{\\sqrt n}z_{\\alpha/2} \\Big] \\\\\n&= \\Big[3.5 -\\frac{3.5}{10}z_{\\alpha/2},~ 3.5 +\\frac{3.5}{10}z_{\\alpha/2} \\Big] \\\\\n&= \\Big[2.81,~4.19 \\Big]\n\\end{split}\n\\]\nApproach 2: Evaluating \\(I_T(a)\\) at \\(a_0\\)\n\\[\n\\begin{split}\nR(a_0) &= \\Bigg[\\pmb x: \\Bigg|\\frac{\\sqrt n}{a_0}(\\hat a - a_0)\\Bigg| > z_{\\alpha/2} \\Bigg] \\\\\nC(\\pmb x) &= \\Big[a_0:\\pmb x \\notin R(a_0) \\Big] \\\\\n&= \\Big[a_0: -z_{\\alpha/2}\\le Z'_W \\le z_{\\alpha/2} \\Big] \\\\\n&= \\Big[a_0: -z_{\\alpha/2}\\le \\frac{\\sqrt n}{a_0}(\\hat a - a_0) \\le z_{\\alpha/2} \\Big] \\\\\n&= \\Big[a_0: -\\frac{1}{\\sqrt n}z_{\\alpha/2}\\le \\frac{\\hat a - a_0}{a_0} \\le \\frac{1}{\\sqrt n}z_{\\alpha/2} \\Big] \\\\\n&= \\Big[a_0: -\\frac{1}{\\sqrt n}z_{\\alpha/2}\\le \\frac{\\hat a}{a_0} -1 \\le \\frac{1}{\\sqrt n}z_{\\alpha/2} \\Big] \\\\\n&= \\Big[a_0: 1 -\\frac{1}{\\sqrt n}z_{\\alpha/2}\\le \\frac{\\hat a}{a_0} \\le 1 + \\frac{1}{\\sqrt n}z_{\\alpha/2} \\Big] \\\\\n&= \\Big[a_0: \\frac{1 -\\frac{1}{\\sqrt n}z_{\\alpha/2}}{\\hat a}\\le \\frac{1}{a_0} \\le \\frac{1 + \\frac{1}{\\sqrt n}z_{\\alpha/2}}{\\hat a} \\Big] \\\\\n&= \\Big[a_0: \\frac{\\hat a}{1 -\\frac{1}{\\sqrt n}z_{\\alpha/2}}\\ge {a_0} \\ge \\frac{\\hat a}{1 + \\frac{1}{\\sqrt n}z_{\\alpha/2}} \\Big] \\\\\n&= \\Big[a_0: \\frac{\\hat a}{1 +\\frac{1}{\\sqrt n}z_{\\alpha/2}}\\le {a_0} \\le \\frac{\\hat a}{1 - \\frac{1}{\\sqrt n}z_{\\alpha/2}} \\Big] \\\\\nC(\\pmb X) &=\\Big[\\frac{\\hat a}{1 +\\frac{1}{\\sqrt n}z_{\\alpha/2}}, ~\\frac{\\hat a}{1 - \\frac{1}{\\sqrt n}z_{\\alpha/2}} \\Big] \\\\\n&= \\Big[\\frac{3.5}{1 +\\frac{1}{10}z_{\\alpha/2}}, ~\\frac{3.5}{1 - \\frac{1}{10}z_{\\alpha/2}} \\Big] \\\\\n&= \\Big[2.93, ~4.35 \\Big] \\\\\n\\end{split}\n\\]"
  },
  {
    "objectID": "exercises.html#m-estimators",
    "href": "exercises.html#m-estimators",
    "title": "6  Exercises",
    "section": "6.2 M-estimators",
    "text": "6.2 M-estimators\nSuppose that for each of \\(n\\) independent subjects, the weekly number of alcoholic beverages \\(X\\) is measured, along with two repeated blood pressure measurements \\(Y_1\\) and \\(Y_2\\). Interest lies in a regression analysis of blood pressure \\(Y\\) on weekly number of alcoholic beverages \\(X\\):\n\\[\nE(Y|X) = \\theta_1 + \\theta_2X\n\\]\nSuppose that the data analyst ignores that the data from the same subject are dependent. The investigator thus solves the following estimation equations:\n\\[\n0 = \\sum_{i = 1}^n \\sum_{j = 1}^2 \\begin{pmatrix} 1 \\\\ X_i \\end{pmatrix} (Y_{ij} - \\theta_1 - \\theta_2X_i)\n\\]\nShow that the obtained estimator is consistent, despite ignoring the dependence in the data.\nWe have\n\\[\n\\begin{split}\n0 &= \\sum_{i = 1}^n U_i (\\hat \\theta) \\\\\n\\text{Let }  U_i(\\theta) &= \\sum_{j = 1}^2 \\begin{pmatrix} 1 \\\\ X_i \\end{pmatrix} (Y_{ij} - \\theta_1 - \\theta_2X_i) \\\\\n\\text{To show } &\\text{consistency, we need to show} \\\\\nE\\Big[U_i(\\theta) \\Big] &= 0 \\\\\nE\\Big[U_i(\\theta) \\Big] &= E\\Bigg[ \\sum_{j = 1}^2 \\begin{pmatrix} 1 \\\\ X_i \\end{pmatrix} (Y_{ij} - \\theta_1 - \\theta_2X_i)\\Bigg] \\\\&= \\sum_{j = 1}^2E\\Bigg[  \\begin{pmatrix} 1 \\\\ X_i \\end{pmatrix} (Y_{ij} - \\theta_1 - \\theta_2X_i)\\Bigg] \\\\\n\\text{Recall that } E\\Big[Y_{ij} \\Big] &= E\\Big[E\\Big(Y_{ij} | X_i \\Big) \\Big]. \\text{ Thus, } \\\\\nE\\Big[U_i(\\theta) \\Big] &=\\sum_{j = 1}^2E\\Bigg[  E\\Bigg(\\begin{pmatrix} 1 \\\\ X_i \\end{pmatrix} (Y_{ij} - \\theta_1 - \\theta_2X_i) \\Bigg| X_i \\Bigg)\\Bigg] \\\\\n&= \\sum_{j = 1}^2E\\Bigg[  \\begin{pmatrix} 1 \\\\ X_i \\end{pmatrix} \\Big(E\\Big[Y_{ij} \\Big| X_i \\Big] - \\theta_1 - \\theta_2X_i\\Big) \\Bigg] \\\\\n&= \\sum_{j = 1}^2E\\Bigg[  \\begin{pmatrix} 1 \\\\ X_i \\end{pmatrix} \\Big(\\theta_1 + \\theta_2X_i - \\theta_1 - \\theta_2X_i\\Big) \\Bigg] \\\\\n&= \\sum_{j = 1}^2E\\Bigg[  \\begin{pmatrix} 1 \\\\ X_i \\end{pmatrix} \\Big(0\\Big) \\Bigg] \\\\\n&= 0\n\\end{split}\n\\]\nWhat is the asymptotic variance of this estimator? When the data are iid, then the solution \\(\\hat \\theta\\) to an unbiased estimating equation\n\\[\n0 = \\sum_{i = 1}^n U_i (\\hat \\theta)\n\\]\nis asymptotically normal with\n\\[\n\\begin{split}\n\\sqrt n (\\hat \\theta - \\theta) &\\xrightarrow D N(0, \\Sigma), \\text{where} \\\\\n\\Sigma &= E\\Bigg(\\frac{\\partial}{\\partial \\theta} U_i(\\theta) \\Bigg)^{-1} \\text{Var}\\Big[ U_i(\\theta)  \\Big] E\\Bigg(\\frac{\\partial}{\\partial \\theta} U_i(\\theta) \\Bigg)^{-1,T} \\\\\nU_i(\\theta) &= \\sum_{j = 1}^2 \\begin{pmatrix} 1 \\\\ X_i \\end{pmatrix} (Y_{ij} - \\theta_1 - \\theta_2X_i) \\\\\nU_i(\\theta_1) &= \\sum_{j = 1}^2 Y_{ij} - \\theta_1 - \\theta_2X_i \\\\\n\\frac{\\partial}{\\partial \\theta_1} U_i(\\theta_1) &=\\sum_{j = 1}^2-1 \\\\\n&= -2 \\\\\n\\frac{\\partial}{\\partial \\theta_2} U_i(\\theta_1) = \\frac{\\partial}{\\partial \\theta_1} U_i(\\theta_2) &= \\sum_{j = 1}^2 -X_i \\\\\n&=-2X_i \\\\\nU_i(\\theta_2) &= \\sum_{j = 1}^2 Y_{ij}X_i - \\theta_1X_i - \\theta_2X_i^2 \\\\\n\\frac{\\partial}{\\partial \\theta_2} U_i(\\theta_2) &=  \\sum_{j = 1}^2 -X_i^2 \\\\\n&= -2X_i^2 \\\\\n\\frac{\\partial}{\\partial\\theta} U_i(\\theta) &= \\begin{bmatrix} -2 & -2X_i \\\\ -2X_i & - 2X_i^2 \\end{bmatrix} \\\\\n&= -2 \\begin{bmatrix} 1 & X_i \\\\ X_i & X_i^2 \\end{bmatrix} \\\\\n\\Bigg(\\frac{\\partial}{\\partial\\theta} U_i(\\theta)\\Bigg)^{-1} &= -\\frac{1}{2}\\begin{bmatrix} 1 & X_i \\\\ X_i & X_i^2 \\end{bmatrix}^{-1} \\\\\n\\end{split}\n\\]\n\\[\n\\begin{split}\n\\text{Var}\\Big[U_i(\\theta) \\Big] &= E\\Big [U_i(\\theta)^2 \\Big] - E\\Big[U_i(\\theta) \\Big]^2 \\\\\n&= E\\Big [U_i(\\theta)^2 \\Big] \\\\\n&= E\\Big [U_i(\\theta)U_i(\\theta)^T \\Big] \\\\\n&= E\\Bigg[\\sum_{j = 1}^2 \\begin{pmatrix} 1 \\\\ X_i \\end{pmatrix} (Y_{ij} - \\theta_1 - \\theta_2X_i) \\sum_{j = 1}^2 \\begin{pmatrix} 1 & X_i \\end{pmatrix} (Y_{ij} - \\theta_1 - \\theta_2X_i)  \\Bigg] \\\\\n&= E\\Bigg[\\Bigg(\\sum_{j = 1}^2 (Y_{ij} - \\theta_1 - \\theta_2X_i) \\Bigg)^2 \\begin{pmatrix} 1 \\\\ X_i \\end{pmatrix} \\begin{pmatrix} 1 & X_i \\end{pmatrix} \\Bigg] \\\\\n&\\text{Recall that } \\begin{pmatrix} a_{11} \\\\ a_{21} \\end{pmatrix} \\begin{pmatrix} b_{11} & b_{12} \\end{pmatrix} =  \\begin{pmatrix} a_{11}b_{11} & a_{11}b_{12} \\\\ a_{21}b_{11} & a_{21}b_{12}  \\end{pmatrix}. \\text{ Thus, } \\\\\n\\text{Var}\\Big[U_i(\\theta) \\Big] &=E\\Bigg[\\Bigg(\\sum_{j = 1}^2 (Y_{ij} - \\theta_1 - \\theta_2X_i) \\Bigg)^2 \\begin{pmatrix} a_{11}b_{11} & a_{11}b_{12} \\\\ a_{21}b_{11} & a_{21}b_{12}  \\end{pmatrix} \\Bigg] \\\\\n&=E\\Bigg[\\Bigg(\\sum_{j = 1}^2 (Y_{ij} - \\theta_1 - \\theta_2X_i) \\Bigg)^2 \\begin{pmatrix} 1 & X_i \\\\ X_i & X_i^2 \\end{pmatrix} \\Bigg] \\\\\n\\Sigma &= E\\Bigg(\\frac{\\partial}{\\partial \\theta} U_i(\\theta) \\Bigg)^{-1} \\text{Var}\\Big[ U_i(\\theta)  \\Big] E\\Bigg(\\frac{\\partial}{\\partial \\theta} U_i(\\theta) \\Bigg)^{-1,T} \\\\\n&= -\\frac{1}{2}E\\begin{bmatrix} 1 & X_i \\\\ X_i & X_i^2 \\end{bmatrix} ^{-1}E\\Bigg[\\Bigg(\\sum_{j = 1}^2 (Y_{ij} - \\theta_1 - \\theta_2X_i) \\Bigg)^2 \\begin{pmatrix} 1 \\\\ X_i \\end{pmatrix} \\begin{pmatrix} 1 & X_i \\end{pmatrix} \\Bigg] \\Bigg( -\\frac{1}{2}E\\begin{bmatrix} 1 & X_i \\\\ X_i & X_i^2 \\end{bmatrix} ^{-1} \\Bigg) \\\\\n&= \\frac{1}{4}E\\begin{bmatrix} 1 & X_i \\\\ X_i & X_i^2 \\end{bmatrix} ^{-1}E\\Bigg[\\Bigg(\\sum_{j = 1}^2 (Y_{ij} - \\theta_1 - \\theta_2X_i) \\Bigg)^2 \\begin{pmatrix} 1 \\\\ X_i \\end{pmatrix} \\begin{pmatrix} 1 & X_i \\end{pmatrix} \\Bigg]  E\\begin{bmatrix} 1 & X_i \\\\ X_i & X_i^2 \\end{bmatrix} ^{-1} \\\\\n\\end{split}\n\\]"
  }
]