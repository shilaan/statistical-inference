{"title":"Exercises","markdown":{"headingText":"Exercises","containsRefs":false,"markdown":"\n## Likelihood-based tests and confidence regions\n\nConsider a sample $X_1, ..., X_n$ from the Pareto family with unknown parameter $a > 2$ and known $b > 0$. The density function is given by \n$$\n\\begin{split}\nf(x;a) &= \\frac{ab^a}{x^{a + 1}}, ~x > b \\\\\nE[X] &= \\frac{ab}{a - 1} \\\\\n\\text{Var}[X] &= \\frac{b^2a}{(a-1)^2(a-2)}\n\\end{split}\n$$\n\nFind the likelihood function, the log likelihood function, the Score function of $a$, and $\\hat a$.\n\n$$\n\\begin{split}\nL(a) &= \\prod_{i = 1}^n \\frac{ab^a}{X_i^{a + 1}} \\\\\n\\ell(a) &= \\sum_{i = 1}^n \\ln a + a\\ln b - (a + 1)\\ln X_i \\\\\n&= n\\ln a + na\\ln b   - (a + 1)\\sum_{i = 1}^n\\ln X_i \\\\\nS_n(a) = \\frac{\\partial}{\\partial a} \\ell(a) &=  \\frac{n}{a} + n\\ln b -\\sum_{i = 1}^n\\ln X_i = 0 \\\\\n&\\Rightarrow \\frac{n}{a} = \\sum_{i = 1}^n\\ln X_i -n\\ln b \\\\\n\\hat a &=\\frac{n}{\\sum_{i = 1}^n\\ln X_i -n\\ln b} \\\\\n&= \\frac{n}{\\sum_{i = 1}^n \\ln(X_i/b)}\n\\end{split}\n$$\n\n\\newpage\n\nCalculate the total Fisher information matrix of $a$, compute the asymptotic variance of $\\hat a$, and find the asymptotic distribution of $\\hat a$.\n\n$$\n\\begin{split}\nS_i(a) &=   \\frac{\\partial}{\\partial a} \\ln a + a\\ln b - (a + 1)\\ln X_i\\\\\n&= \\frac{1}{a} + \\ln b -\\ln X_i \\\\\nI_1(a) &= -E\\Bigg[\\frac{\\partial}{\\partial a} S_i(a)\\Bigg] \\\\\n&= -E\\Bigg[\\frac{\\partial}{\\partial a} \\frac{1}{a} + \\ln b -\\ln X_i\\Bigg] \\\\\n&= -E\\Big[-a^{-2}  \\Big] \\\\\n&=E\\Big[\\frac{1}{a^2} \\Big] \\\\\n&=\\frac{1}{a^2} \\\\\nI_T(a) = nI_1(a) &= \\frac{n}{a^2} \\\\\n\\text{Var}(\\hat a) = [I_T(a)]^{-1} &=\\frac{a^2}{n}, \\text{ the large-sample variance} \\\\\n\\text{For large n, }\\hat a_n &\\sim N(a, \\frac{a^2}{n})\\\\\nV(a) = [I_1(a)]^{-1} &={a^2}, \\text{ the asymptotic variance} \\\\\n\\text{By the asymptotic} &\\text{ normality of the MLE, } \\\\\n\\sqrt n(\\hat a - a) &\\xrightarrow D  N(0, [I_1(a)]^{-1}) \\text{ as } n \\rightarrow \\infty \\\\\n\\sqrt n(\\hat a - a) &\\xrightarrow D  N(0, a^2) \\text{ as } n \\rightarrow \\infty \\\\\n\\end{split}\n$$\n\n\\newpage \n\nFind the MLE of the expected value $\\displaystyle \\frac{ab}{a - 1}$ and its asymptotic variance. \n\n$$\n\\begin{split}\n\\text{Let } g(a) &=  \\frac{ab}{a - 1}\\\\\n&\\text{By invariance of the MLE, } \\\\\n\\widehat{g(a)} &= g(\\hat a) \\\\\n\\widehat{E[X]}_{\\text{MLE}} &= g(\\hat a) \\\\\n&= \\frac{\\hat ab}{\\hat a - 1} \\\\\n&\\text{By the asymptotic normality of the MLE, } \\\\\n\\sqrt{n}(\\hat a - a) &\\xrightarrow{D} N(0, a^2) \\\\\n&\\text{By the Delta method, } \\\\\n\\sqrt{n}\\Big(g(\\hat a) - g(a)\\Big) &\\xrightarrow{D} N\\Bigg(0, a^2\\Big[g'(a) \\Big]^2 \\Bigg) \\\\\ng(a) &=  \\frac{ab}{a - 1}\\\\\ng'(a) &=  \\frac{b(a-1) - ab}{(a - 1)^2}\\\\\n&= \\frac{-b}{(a - 1)^2} \\\\\n\\Big[g'(a) \\Big]^2 &= \\frac{b^2}{(a - 1)^4} \\\\\n\\sqrt{n}\\Big(g(\\hat a) - g(a)\\Big) &\\xrightarrow{D} N\\Bigg(0, \\frac{(ab)^2}{(a - 1)^4} \\Bigg) \\\\\nV(\\widehat{E[X]}_{\\text{MLE}}) &=\\frac{(ab)^2}{(a - 1)^4}, \\text{ the asymptotic variance} \\\\\n\\text{Var}(\\widehat{E[X]}_{\\text{MLE}}) &=\\frac{(ab)^2}{n(a - 1)^4}, \\text{ the large-sample variance} \\\\\n\\end{split}\n$$\n\n\\newpage\n\nFind the asymptotic variance of the sample mean as an estimator of the expected value and compute the Asymptotic Relative Efficiency (ARE) of the two estimators. By the CLT,\n\n$$\n\\begin{split}\n\\sqrt{n}\\Big(\\bar X_n - E[X]\\Big) &\\xrightarrow{D} N\\Bigg(0,  \\frac{b^2a}{(a-1)^2(a-2)}\\Bigg) \\\\\nV(\\widehat{E[X]}_{\\text{MLE}}) &=\\frac{(ab)^2}{(a - 1)^4} \\\\\nV(\\bar X_n) &=\\frac{b^2a}{(a-1)^2(a-2)}\\\\\nARE\\Big(V(\\widehat{E[X]}_{\\text{MLE}}), V(\\bar X_n)\\Big) &= \\frac{V(\\bar X_n)}{V(\\widehat{E[X]}} \\\\\n\\text{If } ARE > 1, \\text{ then }&\\widehat{E[X]}_{\\text{MLE}} \\text{ is more efficient} \\\\\n\\text{If } ARE < 1, \\text{ then }&\\bar X_n \\text{ is more efficient}.\\\\\nARE\\Big(V(\\widehat{E[X]}_{\\text{MLE}}), V(\\bar X_n)\\Big) &=  \\frac{b^2a}{(a-1)^2(a-2)} \\cdot \\frac{(a-1)^4}{a^2b^2} \\\\\n&= \\frac{(a-1)^2}{a(a-2)} \\\\\n&= \\frac{a^2 -2a + 1}{a^2 - 2a} \\\\\n&= 1 + \\frac{1}{a(a-2)}\n\\end{split}\n$$\n\nWe know that $a > 2$. We can see that for the lowest values of $a$, the $ARE$ is greater than 1 and thus $\\widehat{E[X]}_{\\text{MLE}}$ is more efficient than $\\bar X_n$. $\\bar X_n$ will be a relatively inefficient estimator for low values of $a$, e.g. $2 < a \\le 4$. However, for larger values of $a$, the $ARE$ converges to 1, and the two estimators will be equally efficient. \n\n\\newpage\n\nConsider testing $H_0: a = a_0$ vs. $H_a: a \\ne a_0$. Derive two different versions of the Wald test and their rejection region.\n\n**Approach 1: Using $\\hat a$ as a plug-estimator for $I_T(a)$**\n\n$$\n\\begin{split}\nI_T(a) &= \\frac{n}{a^2} \\\\\nZ_W &= \\sqrt{I_T(\\hat a)}(\\hat a - a_0) \\xrightarrow D N(0,1)  \\\\\n&= \\sqrt{\\frac{n}{\\hat a^2}}(\\hat a - a_0) \\\\\n&= \\frac{\\sqrt n}{\\hat a}(\\hat a - a_0) \\\\\n&\\text{We have obtained the following equivalent rejection regions:} \\\\\n&\\text{Reject } H_0 \\text{ if } \\\\\n|Z_W| &> \\Phi^{-1}(1 -\\alpha/2) \\\\\n|Z_W| &> z_{\\alpha/2} \\\\\nZ_W &> z_{\\alpha/2} \\text{ or } Z_W < -z_{\\alpha/2} \\\\\nR(a_0) &= \\Bigg[\\pmb x: \\Bigg|\\frac{\\sqrt n}{\\hat a}(\\hat a - a_0)\\Bigg| > z_{\\alpha/2} \\Bigg]\n\\end{split}\n$$\n\n\n\n**Approach 2: Evaluating $I_T(a)$ at $a_0$**\n\n$$\n\\begin{split}\nI_T(a) &= \\frac{n}{a^2} \\\\\nZ'_W &= \\sqrt{I_T(a_0)}(\\hat a - a_0) \\xrightarrow D N(0,1)  \\\\\n&= \\sqrt{\\frac{n}{a_0^2}}(\\hat a - a_0) \\\\\n&= \\frac{\\sqrt n}{a_0}(\\hat a - a_0) \\\\\n&\\text{We have obtained the following equivalent rejection regions:} \\\\\n&\\text{Reject } H_0 \\text{ if } \\\\\n|Z'_W| &> \\Phi^{-1}(1 -\\alpha/2) \\\\\n|Z'_W| &> z_{\\alpha/2} \\\\\n Z'_W &> z_{\\alpha/2} \\text{ or } Z'_W < -z_{\\alpha/2} \\\\\nR(a_0) &= \\Bigg[\\pmb x: \\Bigg|\\frac{\\sqrt n}{a_0}(\\hat a - a_0)\\Bigg| > z_{\\alpha/2} \\Bigg]\n\\end{split}\n$$\n\nConstruct a $(1-\\alpha)100$% confidence interval for both estimators. Compute the confidence intervals for $a$ using $\\alpha = 0.05$, $n = 100$, and $\\hat a = 3.5$. \n\n**Approach 1: Using $\\hat a$ as a plug-estimator for $I_T(a)$**\n\n$$\n\\begin{split}\nR(a_0) &= \\Bigg[\\pmb x: \\Bigg|\\frac{\\sqrt n}{\\hat a}(\\hat a - a_0)\\Bigg| > z_{\\alpha/2} \\Bigg] \\\\\nC(\\pmb x) &= \\Big[a_0:\\pmb x \\notin R(a_0) \\Big] \\\\\n&= \\Big[a_0: -z_{\\alpha/2}\\le Z_W \\le z_{\\alpha/2} \\Big] \\\\\n&= \\Big[a_0: -z_{\\alpha/2}\\le \\frac{\\sqrt n}{\\hat a}(\\hat a - a_0) \\le z_{\\alpha/2} \\Big] \\\\\n&=  \\Big[a_0: -\\frac{\\hat a}{\\sqrt n}z_{\\alpha/2}\\le \\hat a - a_0 \\le \\frac{\\hat a}{\\sqrt n}z_{\\alpha/2} \\Big] \\\\ \n&=  \\Big[a_0:  -\\hat a -\\frac{\\hat a}{\\sqrt n}z_{\\alpha/2}\\le - a_0 \\le  -\\hat a +\\frac{\\hat a}{\\sqrt n}z_{\\alpha/2} \\Big] \\\\ \n&=  \\Big[a_0:  \\hat a +\\frac{\\hat a}{\\sqrt n}z_{\\alpha/2}\\ge a_0 \\ge  \\hat a -\\frac{\\hat a}{\\sqrt n}z_{\\alpha/2} \\Big] \\\\ \n&=  \\Big[a_0:  \\hat a -\\frac{\\hat a}{\\sqrt n}z_{\\alpha/2}\\le a_0 \\le  \\hat a +\\frac{\\hat a}{\\sqrt n}z_{\\alpha/2} \\Big] \\\\ \nC(\\pmb X) &=  \\Big[\\hat a -\\frac{\\hat a}{\\sqrt n}z_{\\alpha/2},~  \\hat a +\\frac{\\hat a}{\\sqrt n}z_{\\alpha/2} \\Big] \\\\ \n&= \\Big[3.5 -\\frac{3.5}{10}z_{\\alpha/2},~ 3.5 +\\frac{3.5}{10}z_{\\alpha/2} \\Big] \\\\\n&= \\Big[2.81,~4.19 \\Big]\n\\end{split}\n$$\n\n\\newpage\n\n**Approach 2: Evaluating $I_T(a)$ at $a_0$**\n\n$$\n\\begin{split}\nR(a_0) &= \\Bigg[\\pmb x: \\Bigg|\\frac{\\sqrt n}{a_0}(\\hat a - a_0)\\Bigg| > z_{\\alpha/2} \\Bigg] \\\\\nC(\\pmb x) &= \\Big[a_0:\\pmb x \\notin R(a_0) \\Big] \\\\\n&= \\Big[a_0: -z_{\\alpha/2}\\le Z'_W \\le z_{\\alpha/2} \\Big] \\\\\n&= \\Big[a_0: -z_{\\alpha/2}\\le \\frac{\\sqrt n}{a_0}(\\hat a - a_0) \\le z_{\\alpha/2} \\Big] \\\\\n&= \\Big[a_0: -\\frac{1}{\\sqrt n}z_{\\alpha/2}\\le \\frac{\\hat a - a_0}{a_0} \\le \\frac{1}{\\sqrt n}z_{\\alpha/2} \\Big] \\\\\n&= \\Big[a_0: -\\frac{1}{\\sqrt n}z_{\\alpha/2}\\le \\frac{\\hat a}{a_0} -1 \\le \\frac{1}{\\sqrt n}z_{\\alpha/2} \\Big] \\\\\n&= \\Big[a_0: 1 -\\frac{1}{\\sqrt n}z_{\\alpha/2}\\le \\frac{\\hat a}{a_0} \\le 1 + \\frac{1}{\\sqrt n}z_{\\alpha/2} \\Big] \\\\\n&= \\Big[a_0: \\frac{1 -\\frac{1}{\\sqrt n}z_{\\alpha/2}}{\\hat a}\\le \\frac{1}{a_0} \\le \\frac{1 + \\frac{1}{\\sqrt n}z_{\\alpha/2}}{\\hat a} \\Big] \\\\\n&= \\Big[a_0: \\frac{\\hat a}{1 -\\frac{1}{\\sqrt n}z_{\\alpha/2}}\\ge {a_0} \\ge \\frac{\\hat a}{1 + \\frac{1}{\\sqrt n}z_{\\alpha/2}} \\Big] \\\\\n&= \\Big[a_0: \\frac{\\hat a}{1 +\\frac{1}{\\sqrt n}z_{\\alpha/2}}\\le {a_0} \\le \\frac{\\hat a}{1 - \\frac{1}{\\sqrt n}z_{\\alpha/2}} \\Big] \\\\\nC(\\pmb X) &=\\Big[\\frac{\\hat a}{1 +\\frac{1}{\\sqrt n}z_{\\alpha/2}}, ~\\frac{\\hat a}{1 - \\frac{1}{\\sqrt n}z_{\\alpha/2}} \\Big] \\\\\n&= \\Big[\\frac{3.5}{1 +\\frac{1}{10}z_{\\alpha/2}}, ~\\frac{3.5}{1 - \\frac{1}{10}z_{\\alpha/2}} \\Big] \\\\\n&= \\Big[2.93, ~4.35 \\Big] \\\\\n\\end{split}\n$$\n\\newpage\n\n## M-estimators\n\nSuppose that for each of $n$ independent subjects, the weekly number of alcoholic beverages $X$ is measured, along with two repeated blood pressure measurements $Y_1$ and $Y_2$. Interest lies in a regression analysis of blood pressure $Y$ on weekly number of alcoholic beverages $X$: \n\n$$\nE(Y|X) = \\theta_1 + \\theta_2X\n$$\n\nSuppose that the data analyst ignores that the data from the same subject are dependent. The investigator thus solves the following estimation equations:\n$$\n0 = \\sum_{i = 1}^n \\sum_{j = 1}^2 \\begin{pmatrix} 1 \\\\ X_i \\end{pmatrix} (Y_{ij} - \\theta_1 - \\theta_2X_i) \n$$\nShow that the obtained estimator is consistent, despite ignoring the dependence in the data.\n\nWe have\n$$\n\\begin{split}\n0 &= \\sum_{i = 1}^n U_i (\\hat \\theta) \\\\\n\\text{Let }  U_i(\\theta) &= \\sum_{j = 1}^2 \\begin{pmatrix} 1 \\\\ X_i \\end{pmatrix} (Y_{ij} - \\theta_1 - \\theta_2X_i) \\\\\n\\text{To show } &\\text{consistency, we need to show} \\\\\nE\\Big[U_i(\\theta) \\Big] &= 0 \\\\\nE\\Big[U_i(\\theta) \\Big] &= E\\Bigg[ \\sum_{j = 1}^2 \\begin{pmatrix} 1 \\\\ X_i \\end{pmatrix} (Y_{ij} - \\theta_1 - \\theta_2X_i)\\Bigg] \\\\&= \\sum_{j = 1}^2E\\Bigg[  \\begin{pmatrix} 1 \\\\ X_i \\end{pmatrix} (Y_{ij} - \\theta_1 - \\theta_2X_i)\\Bigg] \\\\\n\\text{Recall that } E\\Big[Y_{ij} \\Big] &= E\\Big[E\\Big(Y_{ij} | X_i \\Big) \\Big]. \\text{ Thus, } \\\\\nE\\Big[U_i(\\theta) \\Big] &=\\sum_{j = 1}^2E\\Bigg[  E\\Bigg(\\begin{pmatrix} 1 \\\\ X_i \\end{pmatrix} (Y_{ij} - \\theta_1 - \\theta_2X_i) \\Bigg| X_i \\Bigg)\\Bigg] \\\\\n&= \\sum_{j = 1}^2E\\Bigg[  \\begin{pmatrix} 1 \\\\ X_i \\end{pmatrix} \\Big(E\\Big[Y_{ij} \\Big| X_i \\Big] - \\theta_1 - \\theta_2X_i\\Big) \\Bigg] \\\\\n&= \\sum_{j = 1}^2E\\Bigg[  \\begin{pmatrix} 1 \\\\ X_i \\end{pmatrix} \\Big(\\theta_1 + \\theta_2X_i - \\theta_1 - \\theta_2X_i\\Big) \\Bigg] \\\\\n&= \\sum_{j = 1}^2E\\Bigg[  \\begin{pmatrix} 1 \\\\ X_i \\end{pmatrix} \\Big(0\\Big) \\Bigg] \\\\\n&= 0\n\\end{split}\n$$\n\nWhat is the asymptotic variance of this estimator? When the data are iid, then the solution $\\hat \\theta$ to an unbiased estimating equation \n$$\n0 = \\sum_{i = 1}^n U_i (\\hat \\theta)\n$$\n\nis asymptotically normal with \n$$\n\\begin{split}\n\\sqrt n (\\hat \\theta - \\theta) &\\xrightarrow D N(0, \\Sigma), \\text{where} \\\\\n\\Sigma &= E\\Bigg(\\frac{\\partial}{\\partial \\theta} U_i(\\theta) \\Bigg)^{-1} \\text{Var}\\Big[ U_i(\\theta)  \\Big] E\\Bigg(\\frac{\\partial}{\\partial \\theta} U_i(\\theta) \\Bigg)^{-1,T} \\\\\nU_i(\\theta) &= \\sum_{j = 1}^2 \\begin{pmatrix} 1 \\\\ X_i \\end{pmatrix} (Y_{ij} - \\theta_1 - \\theta_2X_i) \\\\\nU_i(\\theta_1) &= \\sum_{j = 1}^2 Y_{ij} - \\theta_1 - \\theta_2X_i \\\\\n\\frac{\\partial}{\\partial \\theta_1} U_i(\\theta_1) &=\\sum_{j = 1}^2-1 \\\\\n&= -2 \\\\\n\\frac{\\partial}{\\partial \\theta_2} U_i(\\theta_1) = \\frac{\\partial}{\\partial \\theta_1} U_i(\\theta_2) &= \\sum_{j = 1}^2 -X_i \\\\\n&=-2X_i \\\\\nU_i(\\theta_2) &= \\sum_{j = 1}^2 Y_{ij}X_i - \\theta_1X_i - \\theta_2X_i^2 \\\\\n\\frac{\\partial}{\\partial \\theta_2} U_i(\\theta_2) &=  \\sum_{j = 1}^2 -X_i^2 \\\\\n&= -2X_i^2 \\\\\n\\frac{\\partial}{\\partial\\theta} U_i(\\theta) &= \\begin{bmatrix} -2 & -2X_i \\\\ -2X_i & - 2X_i^2 \\end{bmatrix} \\\\\n&= -2 \\begin{bmatrix} 1 & X_i \\\\ X_i & X_i^2 \\end{bmatrix} \\\\\n\\Bigg(\\frac{\\partial}{\\partial\\theta} U_i(\\theta)\\Bigg)^{-1} &= -\\frac{1}{2}\\begin{bmatrix} 1 & X_i \\\\ X_i & X_i^2 \\end{bmatrix}^{-1} \\\\\n\\end{split}\n$$\n\n$$\n\\begin{split}\n\\text{Var}\\Big[U_i(\\theta) \\Big] &= E\\Big [U_i(\\theta)^2 \\Big] - E\\Big[U_i(\\theta) \\Big]^2 \\\\\n&= E\\Big [U_i(\\theta)^2 \\Big] \\\\\n&= E\\Big [U_i(\\theta)U_i(\\theta)^T \\Big] \\\\\n&= E\\Bigg[\\sum_{j = 1}^2 \\begin{pmatrix} 1 \\\\ X_i \\end{pmatrix} (Y_{ij} - \\theta_1 - \\theta_2X_i) \\sum_{j = 1}^2 \\begin{pmatrix} 1 & X_i \\end{pmatrix} (Y_{ij} - \\theta_1 - \\theta_2X_i)  \\Bigg] \\\\\n&= E\\Bigg[\\Bigg(\\sum_{j = 1}^2 (Y_{ij} - \\theta_1 - \\theta_2X_i) \\Bigg)^2 \\begin{pmatrix} 1 \\\\ X_i \\end{pmatrix} \\begin{pmatrix} 1 & X_i \\end{pmatrix} \\Bigg] \\\\\n&\\text{Recall that } \\begin{pmatrix} a_{11} \\\\ a_{21} \\end{pmatrix} \\begin{pmatrix} b_{11} & b_{12} \\end{pmatrix} =  \\begin{pmatrix} a_{11}b_{11} & a_{11}b_{12} \\\\ a_{21}b_{11} & a_{21}b_{12}  \\end{pmatrix}. \\text{ Thus, } \\\\\n\\text{Var}\\Big[U_i(\\theta) \\Big] &=E\\Bigg[\\Bigg(\\sum_{j = 1}^2 (Y_{ij} - \\theta_1 - \\theta_2X_i) \\Bigg)^2 \\begin{pmatrix} a_{11}b_{11} & a_{11}b_{12} \\\\ a_{21}b_{11} & a_{21}b_{12}  \\end{pmatrix} \\Bigg] \\\\\n&=E\\Bigg[\\Bigg(\\sum_{j = 1}^2 (Y_{ij} - \\theta_1 - \\theta_2X_i) \\Bigg)^2 \\begin{pmatrix} 1 & X_i \\\\ X_i & X_i^2 \\end{pmatrix} \\Bigg] \\\\\n\\Sigma &= E\\Bigg(\\frac{\\partial}{\\partial \\theta} U_i(\\theta) \\Bigg)^{-1} \\text{Var}\\Big[ U_i(\\theta)  \\Big] E\\Bigg(\\frac{\\partial}{\\partial \\theta} U_i(\\theta) \\Bigg)^{-1,T} \\\\\n&= -\\frac{1}{2}E\\begin{bmatrix} 1 & X_i \\\\ X_i & X_i^2 \\end{bmatrix} ^{-1}E\\Bigg[\\Bigg(\\sum_{j = 1}^2 (Y_{ij} - \\theta_1 - \\theta_2X_i) \\Bigg)^2 \\begin{pmatrix} 1 \\\\ X_i \\end{pmatrix} \\begin{pmatrix} 1 & X_i \\end{pmatrix} \\Bigg] \\Bigg( -\\frac{1}{2}E\\begin{bmatrix} 1 & X_i \\\\ X_i & X_i^2 \\end{bmatrix} ^{-1} \\Bigg) \\\\\n&= \\frac{1}{4}E\\begin{bmatrix} 1 & X_i \\\\ X_i & X_i^2 \\end{bmatrix} ^{-1}E\\Bigg[\\Bigg(\\sum_{j = 1}^2 (Y_{ij} - \\theta_1 - \\theta_2X_i) \\Bigg)^2 \\begin{pmatrix} 1 \\\\ X_i \\end{pmatrix} \\begin{pmatrix} 1 & X_i \\end{pmatrix} \\Bigg]  E\\begin{bmatrix} 1 & X_i \\\\ X_i & X_i^2 \\end{bmatrix} ^{-1} \\\\\n\\end{split}\n$$\n\n\n\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"exercises.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.0.37","theme":"cosmo"},"extensions":{"book":{"multiFile":true}}}}}