{"title":"Asymptotics of maximum likelihood estimators","markdown":{"headingText":"Asymptotics of maximum likelihood estimators","containsRefs":false,"markdown":"\n## Asymptotic normality of the MLE\n\nAs $n \\rightarrow \\infty$, $$\\sqrt{n}(\\hat \\theta - \\theta_0) \\xrightarrow{D} N\\Big(0, I_1(\\theta_0)^{-1}\\Big)$$ \n\n## Asymptotic variance\n\nIf a sequence of estimators $\\pmb W_n$ of $\\pmb \\tau(\\pmb \\theta)$ satisfies\n$$\\sqrt{n}(\\pmb W_n - \\pmb\\tau(\\pmb \\theta)) \\xrightarrow{D} N(\\pmb 0, \\pmb V(\\pmb \\theta)),$$\n\nthen $\\pmb V(\\pmb\\theta)$ is called the asymptotic variance of the sequence $\\pmb W_n$ in $\\pmb\\tau(\\pmb \\theta)$\n\n$$\n\\text{Var}(\\pmb W_n) \\approx \\pmb V(\\pmb \\theta)/n\n$$\nFor scalar $W_n$ and $\\tau$ this becomes\n\n$$\n\\begin{split}\n\\text{if } \\sqrt{n}( W_n - \\tau(\\pmb \\theta)) \\xrightarrow{D} N( 0,  V(\\pmb \\theta)), &\\text{ then, for any } \\epsilon > 0 \\\\\n\\lim_{n \\rightarrow \\infty} P\\Bigg( \\sqrt{n}\\Big( W_n - \\tau(\\pmb \\theta)\\Big) \\le \\epsilon \\Bigg) &= \\Phi\\Bigg(\\frac{\\epsilon}{\\sqrt{V(\\pmb \\theta)}}\\Bigg)\n\\end{split}\n$$\n\n## Asymptotic efficiency\n\nA sequence of estimators $\\pmb W_n$ is called asymptotically efficient for a parameter $\\pmb \\tau(\\pmb \\theta)$ if the asymptotic variance of $\\pmb W_n$ achieves the Cramer-Rao bound. That is,\n\n$$\n\\begin{split}\n\\sqrt{n}(\\pmb W_n - \\pmb\\tau(\\pmb \\theta)) &\\xrightarrow{D} N(\\pmb 0, \\pmb V(\\pmb \\theta)) \\\\\n\\text{ with } \\pmb V(\\pmb \\theta) = \\Big(\\pmb \\tau'&(\\pmb \\theta)\\Big)^T \\Big( \\pmb I_1(\\pmb\\theta) \\Big)^{-1} \\pmb \\tau'(\\pmb \\theta)\n\\end{split}\n$$\n\nThrough the Delta method,\n$$\n\\sqrt{n} \\Big(\\tau(\\hat \\theta_n) - \\tau(\\theta) \\Big) \\xrightarrow{D} N\\Big(0, I_1(\\theta)^{-1} [\\tau'(\\theta)]^2 \\Big)\n$$\n\n## Asymptotic relative efficiency\n\nLet $\\tau(\\pmb \\theta)$ be a scalar function of $\\pmb \\theta$. If two sequences of estimators $W_n$ and $U_n$ satisfy\n$$\n\\begin{split}\n\\sqrt{n}( W_n - \\tau(\\pmb \\theta)) \\xrightarrow{D} N( 0,  V_W(\\pmb \\theta)) \\\\\n\\sqrt{n}( U_n - \\tau(\\pmb \\theta)) \\xrightarrow{D} N( 0,  V_U(\\pmb \\theta)),\n\\end{split}\n$$\n\nthen the asymptotic relative efficiency (ARE) of $U_n$ with respect to $W_n$ is\n\n$$\n\\text{ARE}(U_n, W_n) = \\frac{V_W(\\pmb \\theta)}{V_U(\\pmb \\theta)}\n$$\n\n\\newpage\n\n## Example: N($\\theta, \\theta$)\n\nThe MLE of $\\theta$ based on a set of $n$ iid $X_i \\sim N(\\theta,\\theta)$ measurements is given by \n$$\\displaystyle \\hat \\theta_{\\text{MLE}} = \\frac{1}{2}\\Big(-1 + \\sqrt{1 + 4\\bar Y_n}\\Big), \\text{ with } Y_i = X_i^2$$\n\nShow that $\\hat \\theta_{\\text{MLE}}$ is consistent for $\\theta$. By the weak law of large numbers,\n$$\n\\begin{split}\n\\bar Y_n &\\xrightarrow{P} E[Y_i] \\\\\nE[Y_i] &= E[X_i^2] \\\\\n&= \\text{Var}[X_i] + (E[X_i])^2 \\\\\n&= \\theta + \\theta^2. \\text{ Thus, } \\\\\n\\bar Y_n &\\xrightarrow{P} \\theta + \\theta^2 \\\\ \nh(\\bar Y_n) &\\xrightarrow{P} h(\\theta + \\theta^2) \\\\\n\\frac{1}{2}\\Big(-1 + \\sqrt{1 + 4\\bar Y_n}\\Big)  &\\xrightarrow{P} \\frac{1}{2}\\Big(-1 + \\sqrt{1 + 4(\\theta + \\theta^2)}\\Big) \\\\\n&= \\frac{1}{2}\\Big(-1 + \\sqrt{4\\theta^2 + 4\\theta + 1}\\Big) \\\\\n&= \\frac{1}{2}\\Bigg(-1 + \\sqrt{4\\Big(\\theta^2 + \\theta + \\frac{1}{4}\\Big)}\\Bigg) \\\\\n&= \\frac{1}{2}\\Bigg(-1 + \\sqrt{4\\Big(\\theta + \\frac{1}{2}\\Big)^2}\\Bigg) \\\\\n&= \\frac{1}{2}\\Big(-1 + 2 \\Big[\\theta + \\frac{1}{2}\\Big]\\Big) \\\\\n&=\\frac{1}{2} (-1 + 2\\theta + 1)\\\\\n&= \\theta \\\\\n\\text{Thus, } \\hat \\theta_{\\text{MLE}}  &\\xrightarrow{P}  \\theta\n\\end{split}\n$$\n\n\nShow that $\\hat \\theta_{\\text{MLE}}$ is asymptotically normal. Use the following fact: \n$$\n\\begin{split}\n\\text{When } X \\sim N(\\mu, \\sigma^2), E[X^4] &= \\mu^4 + 6\\mu^2\\sigma^2 + 3\\sigma^4. \\text{ Thus, } \\\\\n\\text{When } X \\sim N(\\theta, \\theta), E[X^4] &= \\theta^4 + 6\\theta^2\\theta + 3\\theta^2 \\\\\n&= \\theta^4 + 6\\theta^3 + 3\\theta^2 \n\\end{split}\n$$\n\n$$\n\\begin{split}\n\\text{Var}[Y_i] \n&=E[Y_i^2] - \\Big(E[Y_i]\\Big)^2 \\\\\n&= E[X_i^4] - (\\theta + \\theta^2)^2 \\\\\n&=\\theta^4 + 6\\theta^2\\theta + 3\\theta^2 - (\\theta + \\theta^2)^2\\\\\n&= \\theta^4 + 6\\theta^3 + 3\\theta^2 - [\\theta^2 +2\\theta^3 + \\theta^4 ] \\\\\n&= 4\\theta^3+2\\theta^2 \\\\\n&= 2\\theta^2(2\\theta + 1) \\\\\n&\\text{Using the CLT, } \\\\\n\\sqrt{n}(\\bar Y_n - \\mu) &\\xrightarrow{D} N\\Big(0, 2\\theta^2(2\\theta + 1)\\Big) \\\\\n&\\text{Using the Delta method, } \\\\\n\\sqrt{n}\\Big(g(\\bar Y_n) - g(\\mu)\\Big) &\\xrightarrow{D} N\\Big(0, 2\\theta^2(2\\theta + 1)\\Big[g'(\\mu) \\Big]^2 \\Big) \\\\\n\\text{Let } g(\\mu) &=\\frac{1}{2}\\Big(-1 + \\sqrt{1 + 4\\mu}\\Big). \\text{ Then, } \\\\\ng'(\\mu) &=\\frac{1}{2} \\cdot \\frac{1}{2}(1 + 4\\mu)^{-1/2} \\cdot 4 \\\\\n&=(1 + 4\\mu)^{-1/2} \\\\\n&= \\frac{1}{\\sqrt{1 + 4\\mu}} \\\\\n&=\\frac{1}{\\sqrt{1 + 4 (\\theta + \\theta^2)}} \\\\\n&=\\frac{1}{\\sqrt{4\\theta^2 + 4\\theta + 1}}  \\\\\n&=\\frac{1}{\\sqrt{4 (\\theta^2+ \\theta + \\frac{1}{4}) }} \\\\\n&=\\frac{1}{\\sqrt{4 (\\theta + \\frac{1}{2})^2 }} \\\\\n&=\\frac{1}{{2 (\\theta + \\frac{1}{2}) }} \\\\\n&=\\frac{1}{{2\\theta + 1 }} \\\\\n[g'(\\mu)]^2 &=\\frac{1}{{(2\\theta + 1)^2 }}. \\text{ Thus, } \\\\\n\\sqrt{n}\\Big(g(\\bar Y_n) - g(\\mu)\\Big) &\\xrightarrow{D} N\\Bigg(0, \\frac{2\\theta^2(2\\theta + 1)}{(2\\theta + 1)^2} \\Bigg)  \\\\\n&= N\\Bigg(0, \\frac{2\\theta^2}{2\\theta + 1} \\Bigg)\n\\end{split}\n$$\n\nConfirm the asymptotic variance by obtaining the Fisher information matrix. We have\n$$\n\\begin{split}\nX_i &\\sim N(\\theta,\\theta)\\\\\nf_{X_i}(x_i) &= \\frac{1}{\\sqrt{2\\pi}} \\theta^{-1/2} \\exp\\Bigg( - \\frac{(x_i - \\theta)^2}{2\\theta} \\Bigg) \\\\\n\\ln f_{X_i}(X_i;\\theta) &= \\ln\\frac{1}{\\sqrt{2\\pi}} -\\frac{1}{2}\\ln\\theta - \\frac{1}{2}\\Bigg[\\frac{(X_i - \\theta)^2}{\\theta}\\Bigg]  \\\\\n&=\\ln\\frac{1}{\\sqrt{2\\pi}} -\\frac{1}{2}\\ln\\theta - \\frac{1}{2}\\Bigg[\\frac{X_i^2 - 2X_i\\theta + \\theta^2}{\\theta}\\Bigg]\\\\\n&=\\ln\\frac{1}{\\sqrt{2\\pi}} -\\frac{1}{2}\\ln\\theta - \\frac{1}{2}\\Bigg[\\frac{X_i^2 }{\\theta} - 2X_i+ \\theta \\Bigg]  \\\\\nS_i(\\theta) = \\frac{\\partial}{\\partial \\theta} \\ln f_{X_i}(X_i;\\theta) &= -\\frac{1}{2\\theta} - \\frac{1}{2}\\Big[ -\\frac{X_i^2}{\\theta^2} +1\\Big] \\\\\n&=-\\frac{1}{2\\theta} +  \\frac{X_i^2}{2\\theta^2} -\\frac{1}{2}\\\\\nI_1(\\theta) &= -E\\Big[\\frac{\\partial}{\\partial \\theta}S_i(\\theta)\\Big] \\\\\n&= -E\\Big[\\frac{\\partial}{\\partial \\theta} -\\frac{1}{2}\\theta^{-1} + \\frac{1}{2}X_i^2 \\cdot\\theta^{-2} - \\frac{1}{2} \\Big] \\\\\n&= -E\\Big[-\\frac{1}{2} \\cdot-\\theta^{-2} +\\frac{1}{2}X_i^2 \\cdot -2\\theta^{-3}\\Big] \\\\\n&=-E\\Big[\\frac{1}{2\\theta^2} - \\frac{X_i^2}{\\theta^3} \\Big] \\\\\n&=E\\Big[ \\frac{X_i^2}{\\theta^3} -\\frac{1}{2\\theta^2} \\Big] \\\\\n&=\\frac{1}{\\theta^3}E[{X_i^2}] -\\frac{1}{2\\theta^2}  \\\\\n&= \\frac{\\theta + \\theta^2}{\\theta^3} -\\frac{1}{2\\theta^2}  \\\\\n&= \\frac{1}{\\theta^2} + \\frac{1}{\\theta} -\\frac{1}{2\\theta^2}  \\\\\n&= \\frac{1}{2\\theta^2}  + \\frac{1}{\\theta} \\\\\n&= \\frac{1}{2\\theta^2}  + \\frac{2\\theta}{\\theta \\cdot 2\\theta} \\\\\n&= \\frac{1 + 2\\theta}{2\\theta^2}  \\\\\n\\text{Var}[\\hat \\theta_{\\text{MLE}}] &\\approx \\frac{1}{n} \\cdot \\frac{2\\theta^2}{1 + 2\\theta} \\\\\n\\end{split}\n$$\n\n\n$$\n\\begin{split}\n\\text{Earlier, } &\\text{we obtained} \\\\\n\\sqrt{n}(\\hat \\theta_{\\text{MLE}} - \\theta_0) &\\xrightarrow{D} N\\Bigg(0, \\frac{2\\theta^2}{1 + 2\\theta}\\Bigg) \\\\\n\\text{Which is } &\\text{equivalent to} \\\\\n\\hat \\theta_{\\text{MLE}} - \\theta_0 &\\xrightarrow{D} \\frac{1}{\\sqrt n} N\\Bigg(0, \\frac{2\\theta^2}{1 + 2\\theta}\\Bigg) \\\\\n\\hat \\theta_{\\text{MLE}} - \\theta_0 &\\xrightarrow{D}N\\Bigg(0, \\frac{1}{n} \\cdot \\frac{2\\theta^2}{1 + 2\\theta}\\Bigg) \\\\\n\\hat \\theta_{\\text{MLE}} &\\xrightarrow{D}N\\Bigg(\\theta_0 , \\frac{1}{n} \\cdot \\frac{2\\theta^2}{1 + 2\\theta}\\Bigg) \\\\\n\\end{split}\n$$\n\n\n\\newpage\n\n## Example: Convergence of a transformation of the Uniform\n\nLet $U_1, U_2, ...$ be iid observations from a uniform distribution over the unit interval $[0,1]$ and define $\\displaystyle Y_n = \\Bigg(\\prod_{i = 1}^n U_i \\Bigg)^{-1/n}$. Recall that $\\displaystyle \\prod_{i = 1}^n x_i = \\exp\\Bigg(\\sum_{i = 1}^n \\ln x_i\\Bigg)$ and show that $Y_n$ converges in probability to $e$. We have \n\n$$\n\\begin{split}\nf_U(u) &= 1 \\\\\nY_n &= \\Bigg(\\prod_{i = 1}^n U_i \\Bigg)^{-1/n} \\\\\n&= e^{\\Big(\\displaystyle \\sum_{i = 1}^n \\ln U_i\\Big)^{-1/n}} \\\\\n&= e^{\\Big(\\displaystyle -\\frac{1}{n} \\sum_{i = 1}^n \\ln U_i\\Big)} \\\\\n\\ln U_i &\\xrightarrow{P} E\\Big[\\ln U_i\\Big] \\\\\nE\\Big[ \\ln U_i\\Big] &= \\int_0^1 \\ln U_i ~f_U(u) ~du \\\\\n &= \\int_0^1 \\ln U_i  ~du\\\\\n &= u\\ln(u) - u \\Bigg|_0^1 \\\\\n &=\\ln(1) - 1 \\\\\n &=-1 \\\\\n\\ln U_i &\\xrightarrow{P} -1 \\\\\n\\text{By continuous} &\\text{ mapping, } \\\\\nh(\\ln U_i) &\\xrightarrow{P} h(-1) \\\\\n\\exp{\\Big(\\displaystyle -\\frac{1}{n} \\sum_{i = 1}^n \\ln U_i\\Big)} &\\xrightarrow{P} \\exp{\\Big(\\displaystyle -\\frac{1}{n} \\sum_{i = 1}^n -1\\Big)} \\\\\n&= e^{n/n} \\\\\n&= e\n\\end{split}\n$$\n\n## Example: Average relative efficiency\n\nSuppose iid $X_i \\sim \\text{Poisson}(\\lambda)$ and we are interested in estimating $P(X = 0) = e^{-\\lambda}$. Derive two estimators; (1) based on the Poisson process and (2) by setting $Y_i = I(X_i = 0)$, in which case we have iid $Y_i \\sim \\text{Bernoulli}(e^{-\\lambda})$. Compare their efficiencies. We have\n\n$$\n\\begin{split}\nE[X_i] &= \\text{Var}[X_i]  =\\lambda \\\\\n\\text{By } &\\text{WLLN, }  \\\\ \n\\bar X_n &\\xrightarrow{P} \\lambda\\\\\n\\text{By  } &\\text{continuous mapping, } \\\\\nh(\\bar X_n) &\\xrightarrow{P} h(\\lambda) \\\\\ne^{-\\bar X_n} &\\xrightarrow{P} e^{- \\lambda} \\\\\n\\text{By } &\\text{CLT}, \\\\\n\\sqrt n(\\bar X_n - \\lambda) &\\xrightarrow{D} N(0,\\lambda)\\\\\n\\text{By } &\\text{Delta method}, \\\\\n\\sqrt n\\Big(h(\\bar X_n) - h(\\lambda)\\Big) &\\xrightarrow{D} N(0,\\lambda\\Big[h'(\\lambda)\\Big]^2) \\\\\n\\text{Let } h(\\lambda) &= e^{-\\lambda}. \\text{Then, } \\\\\nh'(\\lambda) &= -e^{-\\lambda} \\\\\n[h'(\\lambda)\\Big]^2 &= e^{-2\\lambda}. \\text{Thus, } \\\\\n\\sqrt n\\Big(h(\\bar X_n) - h(\\lambda)\\Big) &\\xrightarrow{D} N(0,\\lambda e^{-2\\lambda}) \\\\\n\\end{split}\n$$\n\nIf, instead, we set $Y_i = I(X_i = 0)$, then we have\n\n$$\n\\begin{split} \n\\text{iid } Y_i &\\sim \\text{Bernoulli}(e^{-\\lambda}) \\\\\nE[Y_i] &= e^{-\\lambda} \\\\\n\\text{Var}[Y_i] &= e^{-\\lambda}(1-e^{-\\lambda}) \\\\\n\\text{By the} &\\text{ WLLN,} \\\\\n\\bar Y_n &\\xrightarrow{P} e^{-\\lambda} \\\\\n\\text{By the} &\\text{ CLT}, \\\\\n\\sqrt n(\\bar Y_n - e^{-\\lambda}) &\\xrightarrow{D} N\\Big(0,e^{-\\lambda}(1-e^{-\\lambda})\\Big)\\\\\n\\end{split}\n$$\nThe average relative efficiency of $\\bar Y_n$ vs. $e^{-\\bar X}$ is given by\n$$\n\\begin{split}\n\\text{ARE}(\\bar Y, e^{-\\bar X}) &= \\frac{\\lambda e^{-2\\lambda}}{e^{-\\lambda}(1-e^{-\\lambda})} \\\\\n&= \\frac{\\lambda e^{-\\lambda}}{1 - e^{-\\lambda}}\n\\end{split}\n$$\n\nThis is a decreasing function. Thus, the Poisson estimator $e^{-\\bar X_n}$ is more efficient than the Bernoulli estimator $\\bar Y_n$. \n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"week3.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.0.37","theme":"cosmo"},"extensions":{"book":{"multiFile":true}}}}}