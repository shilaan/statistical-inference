{
  "hash": "086566c2ac57d44ecad7ad8ce8c2eb92",
  "result": {
    "markdown": "\\newcommand{\\indep}{\\perp \\!\\!\\! \\perp}\n\\def\\R{\\mathbb{R}}\n\n# Point estimation\n\n## Maximum likelihood estimation\n\nThe likelihood function corresponds to the density of all observed data as a function of $\\theta$, evaluated at the effective observations $x_1, ..., x_n$. \n\n$$\n\\begin{split}\nL(\\theta|x_1, ..., x_n) &= f_{X_1, X_2, ..., X_n}(x_1, x_2, ..., x_n | \\theta) \\\\\n\\text{Assuming that the data are} &\\text{ independent and have the same distribition function (iid),} \\\\\nf_{X_1, X_2, ..., X_n}(x_1, x_2, ..., x_n | \\theta) &= \\prod_{i = 1}^n f_{X_i}(x_i;\\theta)\n\\end{split}\n$$\n\nA sensible choice for $\\theta$ is the one for which the observed data are most likely,\n\n$$\n\\hat \\theta = \\text{argmax}_\\theta L(\\theta|x_1, ..., x_n)\n$$\n\nBecause maximizing $L(\\theta)$ is not obvious, we often instead maximize the loglikelihood, which (assuming iid) simplifies to\n\n$$\n\\begin{split}\n\\ell(\\theta | x_1 , ..., x_n) &= \\ln L(\\theta | x_1 , ..., x_n) \\\\\n&= \\ln \\prod_{i = 1}^n f_{X_i}(x_i;\\theta) \\\\\n&= \\sum_{i = 1}^n \\ln f_{X_i}(x_i;\\theta) \\\\\n\\hat \\theta &= \\text{argmax}_\\theta \\sum_{i = 1}^n \\ln f_{X_i}(x_i;\\theta)\n\\end{split}\n$$\n\n\n\n\\newpage \n\n### Example: Exponential distribution\n\nHow likely is it to survive 120 days or more? We have \n\n\n$$\n\\begin{split}\nX &\\sim \\text{Exp}(\\theta) \\\\\nf_X(x) &= \\theta\\exp(-\\theta x) \\\\\nF_X(x) &= 1 - \\exp(-\\theta x) \\\\\nP(X \\ge 120) &= 1 - P(X \\le 120) \\\\\n&= 1 - F_X(120)\\\\\n&=1- 1 + \\exp(-120\\theta) \\\\\n&= \\exp(-120\\theta) \\\\\nF_X(x) &= 1 - \\exp(-\\theta x) \\\\\nf_X(x) &= \\frac{d}{dx} F_X(x) \\\\\n&= -\\exp(-\\theta x)\\cdot-\\theta \\\\\n&= \\theta\\exp(-\\theta x) \\\\\nL(\\theta) &= \\prod_{i = 1}^n \\theta\\exp(-\\theta x_i) \\\\\n&= \\theta^n \\prod_{i = 1}^n \\exp(-\\theta x_i) \\\\\n&= \\theta^n \\exp\\Big(\\sum_{i = 1}^n -\\theta x_i\\Big) \\\\\n\\ell(\\theta) &= n\\ln\\theta + \\sum_{i = 1}^n -\\theta x_i \\\\\n&= n\\ln\\theta -\\theta\\sum_{i = 1}^n x_i \\\\\n\\frac{d}{d\\theta}\\ell(\\theta) &= \\frac{n}{\\theta} -\\sum_{i = 1}^n x_i =0 \\\\\n\\frac{n}{\\hat\\theta} &= \\sum_{i = 1}^n x_i \\\\\n\\hat \\theta &= \\frac{n}{\\displaystyle \\sum_{i = 1}^n x_i} = \\frac{1}{\\bar X_n}\n\\end{split}\n$$\n\n\n\\newpage\n\nNow, due to the invariance property of the MLE, we can estimate the probability to survive 120 days as $\\displaystyle P(Y \\ge 120) = \\exp(-120\\hat \\theta)$.\n\n\n$$\n\\begin{split}\n\\hat \\theta &= \\frac{1}{\\bar X_n} \\\\\ng(\\theta) &= \\exp(-\\theta y) \\\\\ng(\\theta)_\\text{MLE} &= g(\\hat \\theta) \\\\\n&= \\exp(-\\hat \\theta y)\n\\end{split}\n$$\n\n\n\n\n### Example: Poisson distribution\n\n|No. of movements|0  |1  |2  |3  |4  |5  |6  |7  |\n|----------------|---|---|---|---|---|---|---|---|\n|Counts          |182|41 |12 |2  |2  |0  |0  |1  |\n\nAssuming a Poisson distribution, show that the MLE equals 0.358. \n\n**Plan of action**\n\n1. Determine the Distribution function $$f_{X_i}(x_i;\\theta)$$\n2. Construct the Likelihood function $$L(\\theta|x_1, ..., x_n) =  \\prod_{i = 1}^n f_{X_i}(x_i;\\theta)$$\n3. Construct the Loglikelihood function $$LL(\\theta|x_1, ..., x_n) = \\ln \\prod_{i = 1}^n f_{X_i}(x_i;\\theta) =   \\sum_{i = 1}^n \\ln f_{X_i}(x_i;\\theta)$$\n4. Take the derivative of the Loglikelihood function, and set to zero, to maximize it\n\nWe have\n\n$$\n\\begin{split}\nf_{X_i}(x_i;\\lambda) &= e^{-\\lambda}\\frac{\\lambda^{x_i}}{x_i!}  \\\\\nL(\\lambda|x_1, ..., x_n) &=  \\prod_{i = 1}^n f_{X_i}(x_i;\\lambda) \\\\\n&=  \\prod_{i = 1}^n e^{-\\lambda}\\frac{\\lambda^{x_i}}{x_i!} \\\\\n\\ell(\\lambda) &= \\sum_{i = 1}^n -\\lambda + x_i\\ln\\lambda -\\ln(x_i!)\\\\\n\\frac{d}{d\\lambda} \\ell(\\lambda) &= \\sum_{i = 1}^n -1 + \\frac{x_i}{\\lambda} \\\\\n&= -n + \\frac{1}{\\lambda}\\sum_{i = 1}^n x_i =0 \\\\\n&\\Rightarrow  \\frac{1}{\\hat \\lambda}\\sum_{i = 1}^n x_i = n \\\\\n\\hat \\lambda &= \\frac{\\sum_{i = 1}^n x_i}{n} = \\bar X_n \\\\\n&= \\frac{41 + 24 + 6 + 8 + 7}{240} \\\\\n&= \\frac{86}{240} \\approx .358\n\\end{split} \n$$\n\n\n### Example: Zero-inflated Poisson model\n\n\n$$\n\\begin{split}\nf_X(x) &= pI(x = 0) + (1 - p)e^{-\\lambda} \\frac{\\lambda^x}{x!} \\\\\n&=\\begin{cases}\np + (1-p)e^{-\\lambda} & \\text{ for } x = 0 \\\\\n(1-p)e^{-\\lambda}\\frac{\\lambda^x}{x!} & \\text{ for } x > 0\n\\end{cases} \\\\\n&\\text{Combining this expression, we get} \\\\\nf_X(x) &= \\Big(p + (1-p)e^{-\\lambda}\\Big)^{I(x = 0)} \\Big((1-p)e^{-\\lambda}\\frac{\\lambda^x}{x!}\\Big)^{I(x > 0)} \\\\\nL(p, \\lambda) &= \\prod_{i = 1}^n f_{X_i}(x_i) \\\\\n&= \\prod_{i = 1}^n \\Big(p + (1-p)e^{-\\lambda}\\Big)^{I(x_i = 0)} \\Big((1-p)e^{-\\lambda}\\frac{\\lambda^{x_i}}{x_i!}\\Big)^{I(x_i > 0)} \\\\\n\\ell(p, \\lambda) &= \\sum_{i = 1}^n I(x_i=0)\\ln\\Big(p + (1-p)e^{-\\lambda}\\Big) \\\\ &~+ I(x_i > 0)\\Big[\\ln(1 - p) -\\lambda +x_i\\ln\\lambda -\\ln(x_i!) \\Big] \\\\\n\\frac{d}{dp} \\ell(p,\\lambda) &=\\sum_{i = 1}^n \\frac{I(x_i=0)(1 -e^{-\\lambda})}{\\Big(p + (1-p)e^{-\\lambda}\\Big)} \n- \\frac{I(x_i >0)}{(1 - p)} \\\\\n&= \\sum_{i: x_i = 0} \\frac{1 -e^{-\\lambda}}{p + (1-p)e^{-\\lambda}} + \\sum_{i: x_i > 0} \\frac{-1}{(1-p)} \\\\\n\\frac{d}{d\\lambda} \\ell(p,\\lambda) &=\\sum_{i = 1}^n \\frac{I(x_i =0)(1 - p)(-e^{-\\lambda})}{\\Big(p + (1-p)e^{-\\lambda}\\Big)} \n+ I(x_i > 0)\\Big[-1 + \\frac{x_i}{\\lambda}\\Big] \\\\\n&= \\sum_{i: x_i = 0}\\frac{(1 - p)(-e^{-\\lambda})}{p + (1-p)e^{-\\lambda}} + \\sum_{i: x_i > 0} -1 + \\frac{x_i}{\\lambda}\n\\end{split}\n$$\n\n\n### Example: Normal distribution\n\n\n$$\n\\begin{split}\nf_{X_i}(x_i; \\mu,\\sigma^2) &= \\frac{1}{(2\\pi\\sigma^2)^{1/2}} \\exp\\Big[-\\frac{(x_i - \\mu)^ 2}{2\\sigma^ 2}\\Big] \\\\\nL(\\mu, \\sigma^2 | x_1, ..., x_n) &= \\prod_{i = 1}^n f_{X_i}(x_i; \\mu,\\sigma^2) \\\\\n&= \\prod_{i = 1}^n \\frac{1}{(2\\pi\\sigma^2)^{1/2}} \\exp\\Big[-\\frac{(x_i - \\mu)^ 2}{2\\sigma^ 2}\\Big] \\\\\n&= \\frac{1}{(2\\pi\\sigma^2)^{n/2}} \\exp\\Big[\\sum_{i = 1}^n-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\Big] \\\\\n\\ell(\\mu, \\sigma^2 | x_1, ..., x_n) &= \\ln \\Big( (2\\pi\\sigma^2)^{-n/2}  \\exp\\Big[\\sum_{i = 1}^n-\\frac{(x_i - \\mu)^ 2}{2\\sigma^ 2}\\Big] \\Big) \\\\\n&= \\ln\\Big( (2\\pi\\sigma^2)^{-n/2} \\Big) + \\sum_{i = 1}^n-\\frac{(x_i - \\mu)^ 2}{2\\sigma^2} \\\\\n&= -n/2\\ln(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i = 1}^n{(x_i - \\mu)^ 2} \\\\\n\\frac{dl}{d\\mu} &= - \\frac{1}{2\\sigma^2} \\cdot 2 \\cdot -1 \\sum_{i = 1}^n (x_i - \\mu) \\\\\n&= \\frac{1}{\\sigma^2} \\sum_{i = 1}^n (x_i - \\mu) \\\\\n&=  \\frac{1}{\\sigma^2} \\Big[(\\sum_{i = 1}^n x_i) -n\\mu  \\Big] = 0 \\\\\n&\\Rightarrow \\Big(\\sum_{i = 1}^n x_i\\Big) -n\\hat\\mu = 0 \\\\\n&\\Rightarrow n\\hat\\mu = \\sum_{i = 1}^n x_i \\\\\n\\hat \\mu &= \\frac{1}{n} \\sum_{i = 1}^n x_i \\\\\n\\end{split}\n$$\n\n$$\n\\begin{split}\n\\frac{dl}{d\\sigma^2} &= -n/2\\frac{2\\pi}{2\\pi\\sigma^2} + \\frac{1}{2\\sigma^4} \\sum_{i = 1}^n{(x_i - \\mu)^ 2} \\\\\n&= \\frac{-n}{2\\sigma^2} + \\frac{1}{2\\sigma^4} \\sum_{i = 1}^n{(x_i - \\mu)^ 2} = 0 \\\\\n&\\Rightarrow \\frac{n}{2\\sigma^2} = \\frac{1}{2\\sigma^4} \\sum_{i = 1}^n{(x_i - \\mu)^ 2} \\\\\n&\\Rightarrow \\frac{n}{2} = \\frac{1}{2\\sigma^2}  \\sum_{i = 1}^n{(x_i - \\mu)^ 2} \\\\\n\\hat\\sigma^2 &= \\frac{1}{n}  \\sum_{i = 1}^n{(x_i - \\hat \\mu)^ 2}\n\\end{split}\n$$\n\n\n\n### Example: Censored exponential distribution\n\nSuppose we have the following days to failure:\n\n$$Y = [2, > 72, 51, >60, 33, 27, 14, 24, 4, >21]$$\n\nSuppose also that the data follow an exponential distribution with parameter $\\lambda$. We have two groups: uncensored observations ($\\delta_i = 1$) and censored observations ($\\delta_i = 0$). For the first group (uncensored), we have $Y_i \\sim \\text{Exponential}(\\lambda)$. For the second group (censored), we have $1 - F_{R_i}(R_i) = 1 - (1 - \\exp(-\\lambda R_i))$. Thus, Y is a random variable with distribution function\n\n\n$$\n\\begin{split}\nf_{Y_i}(y_i; \\lambda)\n&= \\begin{cases}\n\\lambda e^{-\\lambda y_i} & ~~~~~~~~~~~~\\text{ for } \\delta_i = 1 \\\\\ne^{-\\lambda R_i} &  ~~~~~~~~~~~~\\text{ for } \\delta_i = 0 \\text{ and censoring time } R_i\n\\end{cases}\n\\end{split}\n$$\n\n\nCombining this expression, we get\n\n$$\n\\begin{split}\nf_{Y_i }(y_i ; \\lambda)&= \\Big(\\lambda e^{-\\lambda y_i} \\Big)^{\\delta_i}\\Big(1 - (1 - e^{-\\lambda R_i})  \\Big)^{1 -\\delta_i} \\\\\n&= \\Big(\\lambda e^{-\\lambda y_i} \\Big)^{\\delta_i}\\Big(e^{-\\lambda R_i}  \\Big)^{1 -\\delta_i} \\\\\nL(\\lambda|y_i,..., y_n) &= \\prod_{i = 1}^n f_{Y_i}(y_i ; \\lambda) \\\\\n&= \\prod_{i = 1}^n \\Big(\\lambda e^{-\\lambda y_i} \\Big)^{\\delta_i}\\Big(e^{-\\lambda R_i}  \\Big)^{1 -\\delta_i} \\\\\n&= \\prod_{i = 1}^n \\lambda^{\\delta_i} \\exp\\Big({-\\lambda y_i\\delta_i}\\Big)\\exp\\Big({-\\lambda R_i(1-\\delta_i)}\\Big) \\\\\n&= \\prod_{i = 1}^n \\lambda^{\\delta_i}\\exp\\Big({-\\lambda(y_i\\delta_i + R_i(1 - \\delta_i)}\\Big) \\\\\n\\ell(\\lambda) &= \\sum_{i = 1}^n \\delta_i\\ln\\lambda -\\lambda\\Big(y_i\\delta_i + R_i(1 - \\delta_i)\\Big) \\\\\n\\frac{d}{d\\lambda}\\ell(\\lambda) &= \\sum_{i = 1}^n \\frac{\\delta_i}{\\lambda} -\\Big(y_i\\delta_i + R_i(1 - \\delta_i)\\Big) \\\\\n&= \\sum_{i = 1}^n \\frac{\\delta_i}{\\lambda} -y_i\\delta_i - R_i(1 - \\delta_i) = 0 \\\\\n\\sum_{i = 1}^n \\frac{\\delta_i}{\\lambda} &= \\sum_{i = 1}^n y_i\\delta_i + R_i(1 - \\delta_i) \\\\\n\\frac{1}{\\lambda} \\sum_{i = 1}^n \\delta_i &= \\sum_{i = 1}^n y_i\\delta_i + R_i(1 - \\delta_i) \\\\\n\\hat \\lambda &= \\frac{\\sum_{i = 1}^n\\delta_i}{\\sum_{i = 1}^ny_i\\delta_i + R_i(1 - \\delta_i)} \\\\\n&= \\frac{7}{308} \\approx 0.0227\n\\end{split}\n$$\n\n\nThus, the maximum likelihood estimate for the censored data equals the number of uncensored observations divided by the sum of all $Y_i$ (for $\\delta_i = 1$) and all censoring times $R_i$ (for $\\delta_i = 0$). \n\n\\newpage\n\n\n::: {.cell}\n\n```{.r .cell-code}\nyi <- c(2, 51, 33, 27, 14, 24, 4)\nri <- c(72, 60, 21)\n\n#analytical solution\nlambda_hat <- length(yi)/sum(c(yi,ri))\nlambda_hat\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.02272727\n```\n:::\n\n```{.r .cell-code}\n#numerical solution\nnegloglik <- function(lambda){\n  -(sum(log(lambda) - lambda*yi) + sum(-lambda*ri))\n}\n\nnlm(negloglik, p = .03)$estimate\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.0227273\n```\n:::\n:::\n\n\n\n\n\n\\newpage\n\n### Example: Right-censored distribution\n\nLet $X$ be a continuous random variable with distribution function $F_X(x)$ and density function $f_X(x)$. Consider the following random variable $Y$, and obtain its CDF and PDF \n\n\n$$\n\\begin{split}\nY &= \\begin{cases}\nX & \\text{if } X < a \\\\\na & \\text{if } X \\ge a\n\\end{cases} \\\\\n\\end{split}\n$$\n\n\nThis is a right-censored version of $X$ at $x = a$. We have\n\n$$\n\\begin{split}\n\\text{For } X &< a, \\text{ or, equivalently, } Y < a, \\\\\nF_Y(y) &= P(Y \\le y) \\\\&= P(X \\le y) \\\\&= F_X(y) \\\\\nf_Y(y) &= f_X(x) \\\\&= f_X(y) \\\\\n\\text{For } X &\\ge a , \\text{ or, equivalently, } Y = a, \\\\\nF_Y(y) &= F_Y(a) \\\\ &= P(Y \\le a) \\\\&= 1 \\\\\nf_Y(y) &= P(Y = a) \\\\\n&= 1 - P(Y < a) \\\\\n&= 1 - P(X < a ) \\\\\n&= 1 - P(X \\le a ) \\\\\n&= 1 - F_X(a). \\text{ Thus, } \\\\\nf_Y(y) &= \\begin{cases}\nf_X(y) & \\text{if } y < a \\\\\n1 - F_X(a) & \\text{if } y = a\n\\end{cases} \\\\\n&= f_X(y)^{I(y < a)}\\Big[1 - F_X(a)\\Big]^{I(y = a)}\n\\\\\nF_Y(y)&= \\begin{cases}\nF_X(y) & ~~~~~~~\\text{if } y < a \\\\\n1 & ~~~~~~~\\text{if } y = a \\\\\n\\end{cases} \\\\\n&= F_X(y)^{I(y < a)} 1^{I(y = a)}\n\\end{split}\n$$\n\n\n\\newpage\n\nSuppose we have iid data $X_1, ..., X_n$, with $X_i \\sim N(\\mu, 1)$. Derive equations for the MLE $\\hat\\mu$ of $\\mu$ based on the censored data $Y_1, ..., Y_n$. We have\n\n\n$$\n\\begin{split}\nf_{Y_i}(y_i) &= f_{X_i}(y_i)^{I(y_i < a)}\\Big[1 - F_{X_i}(a)\\Big]^{I(y_i = a)} \\\\\nf_{X_i}(x_i) &=\\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\Bigg(-\\dfrac{(x_i - \\mu)^2}{2\\sigma^2}\\Bigg) \\\\\n&= \\frac{1}{\\sqrt{2\\pi}}\\exp\\Big[-\\frac{1}{2}(x - \\mu)^2\\Big] \\\\\nF_{X_i}(x_i) &=\\Phi\\Big(\\frac{x_i - \\mu}{\\sigma}\\Big)\\\\\n&= \\Phi(x_i - \\mu) \\\\\nL(\\mu) &= \\prod_{i = 1}^n f_{X_i}(y_i)^{I(y_i < a)}\\Big[1 - F_{X_i}(a)\\Big]^{I(y_i = a)} \\\\\n&=\\prod_{i = 1}^n \\Bigg(\\frac{1}{\\sqrt{2\\pi}}\\exp\\Big[-\\frac{1}{2}(y_i - \\mu)^2\\Big]\\Bigg)^{I(y_i < a)}\\Big[1 - \\Phi(a - \\mu)\\Big]^{I(y_i = a)} \\\\\n&\\text{Because } 1 - \\Phi(x) \\equiv \\Phi(-x), \\\\\n&=\\prod_{i = 1}^n \\Bigg(\\frac{1}{\\sqrt{2\\pi}}\\exp\\Big[-\\frac{1}{2}(y_i - \\mu)^2\\Big]\\Bigg)^{I(y_i < a)}\\Big[\\Phi(\\mu - a)\\Big]^{I(y_i = a)} \\\\\n\\ell(\\mu) &= \\sum_{i = 1}^n I(y_i < a) \\ln \\Bigg(\\frac{1}{\\sqrt{2\\pi}}\\exp\\Big[-\\frac{1}{2}(y_i - \\mu)^2\\Big] \\Bigg) + I(y_i = a)\\ln\\Big[\\Phi(\\mu - a)\\Big] \\\\\n&=\\sum_{i = 1}^n I(y_i < a)\\Bigg(\\ln\\Big[ \\frac{1}{\\sqrt{2\\pi}} \\Big] - \\frac{1}{2}(y_i - \\mu)^2 \\Bigg)+ I(y_i = a)\\ln\\Big[\\Phi(\\mu - a)\\Big] \\\\\n\\frac{d}{d\\mu} \\ell(\\mu) &=\\sum_{i = 1}^n I(y_i < a) (y_i - \\mu) + I(y_i = a) \\frac{f_X(\\mu - a)}{\\Phi(\\mu - a)} \n\\end{split}\n$$\n\n\nNumerically calculate the MLe for a sample of size $n = 100$ with $\\mu = 0$ and $a = 1$. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nxi <- rnorm(100)\nmean(xi) #the MLE based on the full data\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.009724927\n```\n:::\n\n```{.r .cell-code}\na <- 1\nyi <- ifelse(xi < a, xi, a)\n\n#numerical solution\nnegloglik <- function(mu){\n  #Negative log likelihood expression\n  -sum(I(yi < a)*(-0.5*(yi - mu)^2) + I(xi >= a)*log(pnorm(mu - a)))\n}\n\nnlm(negloglik, p = -.03)$estimate\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.0205651\n```\n:::\n:::\n\n\n\n\\newpage\n\n### Neyman Scott Problem\n\nSuppose we collect 2 measurements $Y_{i1}$ and $Y_{i2}$ for each of $n$ subjects, with iid $Y_{ij}, i = 1, ..., n, j = 1,2, Y_{ij} \\sim N(\\mu_i, \\sigma^2)$ (i.e., same variance $\\sigma^2$ but individual-specific means $\\mu_i$). We are interested in estimating $\\sigma^2$.\n\n$$\n\\begin{split}\nf_{Y_{ij}}(y_{ij}) &=\\begin{cases}\nN(\\mu_i, \\sigma^2) & \\text{ for } j = 1 \\\\\nN(\\mu_i, \\sigma^2) & \\text{ for } j = 2 \n\\end{cases} \\\\\n&= \\Big(\\frac{1}{(2\\pi\\sigma^2)^{1/2}} \\exp\\Big[-\\frac{(y_{ij} - \\mu_i)^ 2}{2\\sigma^ 2}\\Big]\\Big)^{I(j = 1)} \\\\\n&~~~~~\\Big(\\frac{1}{(2\\pi\\sigma^2)^{1/2}} \\exp\\Big[-\\frac{(y_{ij} - \\mu_i)^ 2}{2\\sigma^ 2}\\Big]\\Big)^{I(j = 2)} \\\\\n& = \\prod_{j = 1}^2 \\frac{1}{(2\\pi\\sigma^2)^{1/2}} \\exp\\Big[-\\frac{(y_{ij} - \\mu_i)^ 2}{2\\sigma^ 2}\\Big] \\\\\n&= \\frac{1}{2\\pi\\sigma^2} \\prod_{j = 1}^2 \\exp\\Big[-\\frac{(y_{ij} - \\mu_i)^ 2}{2\\sigma^ 2}\\Big] \\\\\n&= \\frac{1}{2\\pi\\sigma^2}\\exp\\Big[ \\sum_{j = 1}^2-\\frac{(y_{ij} - \\mu_i)^ 2}{2\\sigma^ 2}\\Big] \\\\\n&= \\frac{1}{2\\pi\\sigma^2}\\exp\\Big[- \\frac{1}{2\\sigma^ 2} \\sum_{j = 1}^2  {(y_{ij} - \\mu_i)^ 2}\\Big] \\\\\n&= \\frac{1}{2\\pi\\sigma^2}\\exp\\Big[- \\frac{1}{2\\sigma^ 2} \\Big({(y_{i1} - \\mu_i)^ 2}  + {(y_{i2} - \\mu_i)^ 2}\\Big)  \\Big] \\\\\nL(\\mu_i, \\sigma^2) &= \\prod_{i = 1}^n \\frac{1}{2\\pi\\sigma^2} \\exp\\Big[- \\frac{1}{2\\sigma^ 2} \\Big({(y_{i1} - \\mu_i)^ 2}  + {(y_{i2} - \\mu_i)^ 2}\\Big) \\Big] \\\\\n&= \\frac{1}{(2\\pi\\sigma^2)^n} \\exp\\Big[\\sum_{i = 1}^n- \\frac{1}{2\\sigma^ 2} \\Big({(y_{i1} - \\mu_i)^ 2}  + {(y_{i2} - \\mu_i)^ 2}\\Big)  \\Big] \\\\\n&= \\frac{1}{(2\\pi\\sigma^2)^n} \\exp\\Big[- \\frac{1}{2\\sigma^ 2}\\sum_{i = 1}^n {(y_{i1} - \\mu_i)^ 2}  + {(y_{i2} - \\mu_i)^ 2} \\Big] \\\\\nl (\\mu_i, \\sigma^2)&= \\ln\\Big((2\\pi\\sigma^2)^{-n}\\Big) - \\frac{1}{2\\sigma^ 2}\\sum_{i = 1}^n {(y_{i1} - \\mu_i)^ 2}  + {(y_{i2} - \\mu_i)^ 2} \\\\\n&= -n\\ln(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^ 2}\\sum_{i = 1}^n {(y_{i1} - \\mu_i)^ 2}  + {(y_{i2} - \\mu_i)^ 2} \\\\\n\\end{split}\n$$\n\n$$\n\\begin{split}\n&\\text{For a single individual, we have} \\\\\nl (\\mu_i, \\sigma^2)&= -n\\ln(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^ 2} \\Big( {(y_{i1} - \\mu_i)^ 2}  + {(y_{i2} - \\mu_i)^ 2} \\Big) \\\\\n\\frac{d}{d\\mu_i} l (\\mu_i, \\sigma)&= - \\frac{1}{2\\sigma^ 2}\\Big( -2{(y_{i1} - \\mu_i)}  - {2(y_{i2} - \\mu_i)} \\Big) \\\\\n&= \\frac{-\\Big( -2{(y_{i1} - \\mu_i)}  - {2(y_{i2} - \\mu_i)} \\Big)}{2\\sigma^ 2} \\\\\n&= \\frac{2(y_{i1} - \\mu_i) + 2(y_{i2} - \\mu_i)}{2\\sigma^ 2} \\\\\n&= \\frac{(y_{i1} - \\mu_i) + (y_{i2} - \\mu_i)}{\\sigma^ 2} \\\\\n&= \\frac{y_{i1} + y_{i2} - 2\\mu_i}{\\sigma^2} = 0 \\\\\n&\\Rightarrow 2\\mu_i = y_{i1} + y_{i2} \\\\\n\\hat{\\mu_i} &= \\frac{y_{i1} + y_{i2}}{2}\n\\end{split}\n$$\n\n\nTo estimate $\\sigma^2$, we plug in $\\hat \\mu_i$:\n\n$$\n\\begin{split}\n\\ell(\\mu_i, \\sigma^2)&= -n\\ln(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^ 2}\\sum_{i = 1}^n {(y_{i1} - \\mu_i)^ 2}  + {(y_{i2} - \\mu_i)^ 2} \\\\\n\\frac{d}{d\\sigma^2} \\ell(\\mu_i, \\sigma^2)&= \\frac{-n2\\pi}{2\\pi\\sigma^2} + \\frac{1}{2\\sigma^4}\\sum_{i = 1}^n {(y_{i1} - \\mu_i)^ 2}  + {(y_{i2} - \\mu_i)^ 2} \\\\\n&\\Rightarrow \\frac{1}{2\\sigma^4} \\sum_{i = 1}^n {(y_{i1} - \\mu_i)^ 2}  + {(y_{i2} - \\mu_i)^ 2} =\\frac{n}{\\sigma^2} \\\\\n&\\Rightarrow\\sum_{i = 1}^n {(y_{i1} - \\mu_i)^ 2}  + {(y_{i2} - \\mu_i)^ 2} =\\frac{2n\\sigma^4}{\\sigma^2} \\\\\n\\hat \\sigma^2 &= \\frac{1}{2n} \\sum_{i = 1}^n {(y_{i1} - \\mu_i)^ 2}  + {(y_{i2} - \\mu_i)^ 2} \\\\\n&=  \\frac{1}{2n} \\sum_{i = 1}^n {\\Big(\\frac{y_{i1} - y_{i2}}{2}\\Big)^ 2} + {\\Big(\\frac{y_{i1} - y_{i2}}{2}\\Big)^ 2} \\\\\n&= \\frac{1}{2n}\\sum_{i = 1}^n 2{\\Big(\\frac{y_{i1} - y_{i2}}{2}\\Big)^ 2} \\\\\n&= \\frac{1}{n}\\sum_{i = 1}^n{\\Big(\\frac{1}{2}({y_{i1} - y_{i2}})\\Big)^ 2} \\\\\n&= \\frac{1}{n}\\sum_{i = 1}^n\\frac{1}{4} \\Big({y_{i1} - y_{i2}}\\Big)^ 2 \\\\\n&= \\frac{1}{4n}\\sum_{i = 1}^n{\\Big({y_{i1} - y_{i2}}\\Big)^ 2}\n\\end{split}\n$$\n\n\n\\newpage\n\n$\\hat \\sigma^2$ is a biased estimator for $\\sigma^2$. We have\n\n\n$$\n\\begin{split}\nE\\Big[\\hat \\sigma^2\\Big] &=E \\Bigg[\\frac{1}{4n}\\sum_{i = 1}^n\\Big({Y_{i1} - Y_{i2}}\\Big)^ 2 \\Bigg] \\\\\n&= \\frac{1}{4n} E \\Bigg[\\sum_{i = 1}^n\\Big({Y_{i1} - Y_{i2}}\\Big)^ 2 \\Bigg] \\\\\n&=\\frac{1}{4n} \\sum_{i = 1}^nE \\Bigg[\\Big({Y_{i1} - Y_{i2}}\\Big)^ 2 \\Bigg] \\\\\n\\text{Let } Z &= Y_{i1} - Y_{12}. \\text{ Then, }\\\\\nE[Z] &= E\\Big[Y_{i1} - Y_{12}\\Big] \\\\\n&= E[Y_{i1}] - E[Y_{i2}] \\\\\n&= \\mu_i - \\mu_i = 0. \\\\\n\\text{Var}[Z] &=  E[Z^2] - \\Big(E[Z]\\Big)^2 \\\\\n&= E[Z^2] \\\\\n&= E\\Big[(Y_{i1} - Y_{i2})^2\\Big] \\\\\n&= \\text{Var}\\Big[Y_{i1} - Y_{i2}\\Big] \\\\\n\\text{Because } Y_{i1} &\\indep Y_{i2}, \\\\\n\\text{Var}\\Big[Y_{i1} - Y_{i2}\\Big] &= \\text{Var}[Y_{i1}] +  \\text{Var}[- Y_{i2}] \\\\\n&= \\text{Var}[Y_{i1}] +  \\text{Var}[Y_{i2}] \\\\\n&= 2\\sigma^2. \\text{ Thus, } \\\\\nE\\Big[\\hat \\sigma^2\\Big] &= \\frac{1}{4n} \\sum_{i = 1}^nE \\Bigg[\\Big({Y_{i1} - Y_{i2}}\\Big)^ 2 \\Bigg] \\\\\n&= \\frac{1}{4n} \\sum_{i = 1}^n 2\\sigma^2 \\\\\n&= \\frac{2n\\sigma^2}{4n} \\\\\n&= \\frac{\\sigma^2}{2}\n\\end{split}\n$$\n\nThe bias remains even if $n$ tends to infinity: the MLE of $\\sigma^2$ is inconsistent. Solution: transform the data, such that their distribution is independent of the nuisance parameters. The likelihood of these transformed data is called a **marginal likelihood**, because it averages away some of the information in the data. It results in a **marginal** or **restricted MLE**. \n\n\n$$\n\\begin{split}\n\\text{Let } V_i &= \\frac{Y_{i1}  - Y_{i2}}{\\sqrt 2}. \\text{ Then, } \\\\\nE[V_i] &= E\\Bigg[\\frac{Y_{i1}  - Y_{i2}}{\\sqrt 2}\\Bigg] \\\\\n&= \\frac{1}{\\sqrt 2} E\\Big[Y_{i1}  - Y_{i2}\\Big] = 0. \\\\\n\\text{Var}[V_i] &= E[V_i^2] - \\Big(E[V_i]\\Big)^2 \\\\\n&=E[V_i^2] \\\\\n&= E\\Bigg[\\Bigg(\\frac{Y_{i1}  - Y_{i2}}{\\sqrt 2}\\Bigg)^2\\Bigg] \\\\\n&= E\\Bigg[\\frac{(Y_{i1}  - Y_{i2})^2}{2}\\Bigg] \\\\\n&= \\frac{1}{2} E \\Big[({Y_{i1} - Y_{i2}})^ 2 \\Big] \\\\\n&= \\frac{2\\sigma^2}{2} \\\\\n&= \\sigma^2. \\\\\n\\text{Thus, } V_i &\\sim N(0, \\sigma^2) \\\\\n\\hat \\sigma^2 &= \\frac{1}{n} \\sum_{i = 1}^n \\Big(V_i - E[V_i]\\Big)^2 \\\\\n&= \\frac{1}{n} \\sum_{i = 1}^n V_i^2 \\\\\n&= \\frac{1}{n} \\sum_{i = 1}^n \\frac{(Y_{i1} - Y_{i2})^2}{2} \\\\\n&= \\frac{1}{2n} \\sum_{i = 1}^n (Y_{i1} - Y_{i2})^2 \\\\\nE[\\hat \\sigma^2] &= E\\Bigg[\\frac{1}{n} \\sum_{i = 1}^n V_i^2\\Bigg] \\\\\n&=  \\frac{1}{n} \\sum_{i = 1}^n E\\Big[V_i^2\\Big] \\\\\n&=  \\frac{1}{n} \\sum_{i = 1}^n \\sigma^2 \\\\\n&= \\sigma^2.\n\\end{split}\n$$\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 200; mu <- rnorm(n, mean = 0, sd = 1)\ny1 <- rnorm(n, mean = mu, sd = 1); y2 <- rnorm(n, mean = mu, sd = 1)\nsum((y1 - y2)^2)/(4*n) #biased mle of sigma^2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.4419836\n```\n:::\n\n```{.r .cell-code}\nsum((y1 - y2)^2)/(2*n)  #restricted/marginal mle of sigma^2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8839672\n```\n:::\n:::\n\n\n## Distributions of transformed data\n\nLet X be a random variable with a known distribution and Y a function of X. Then, $Y = h(X)$, $X = h^{-1}(Y)$, and we can use the chain rule to obtain the PDF of $Y$. \n\nFor $h(X)$ **strictly increasing**, we have\n\n$$\\displaystyle f_Y(y) = f_X\\Big(h^{-1}(y)\\Big) \\frac{d}{dy} \\Big[h^{-1}(y)\\Big]$$\n\n\n**Derivation**\n\n$$\n\\begin{split}\nF_Y(y) &= P\\Big(h(X) \\le y\\Big) \\\\ \n&= P\\Big(X \\le h^{-1}(y)\\Big) \\\\\n&= F_X\\Big(h^{-1}(y) \\Big) \\\\\nf_Y(y) &= \\frac{d}{dy} F_X\\Big(h^{-1}(y) \\Big) \\\\\n&= f_X\\Big(h^{-1}(y) \\Big) \\frac{d}{dy} \\Big[h^{-1}(y)  \\Big] \\\\\n\\end{split}\n$$\n\n\nFor example, let $X \\sim N(\\mu, \\sigma^2)$ and $Y = e^X$. Then,\n\n$$\n\\begin{split} \nf_X(x) &= \\dfrac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\Bigg(-\\dfrac{(x - \\mu)^2}{2\\sigma^2}\\Bigg)\\\\\nY &= h(X) = e^X \\\\\nX &= h^{-1}(Y) = \\ln Y \\\\\nf_Y(y) &= f_X\\Big(h^{-1}(y)\\Big)  \\frac{d}{dy} \\Big[ h^{-1}(y) \\Big] \\\\\n&= f_X(\\ln y)  \\frac{d}{dy} \\Big[ \\ln y \\Big]\\\\\n&= f_X(\\ln y) \\cdot \\frac{1}{y} \\\\\n&= \\dfrac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\Bigg(-\\dfrac{(\\ln y - \\mu)^2}{2\\sigma^2}\\Bigg) \\cdot \\frac{1}{y} \\\\\n&= \\dfrac{1}{y\\sigma\\sqrt{2\\pi}}\\exp\\Bigg(-\\dfrac{(\\ln y  - \\mu)^2}{2\\sigma^2}\\Bigg) \\\\\n&\\text{This is } \\text{the lognormal distribution:} \\\\\n&Y \\sim \\text{LN}(\\mu, \\sigma^2)\n\\end{split}\n$$\n\nTo obtain $h^{-1}(y)$:\n\n$$\n\\begin{split}\nF_Y(y) &= P(Y \\le y) \\\\\n&= P(e^X \\le y) \\\\\n&= P(X \\le \\ln y) \\\\\n&= F_X(\\ln y) \\\\\nh^{-1}(y) &= \\ln y\n\\end{split}\n$$\n\n\n\\newpage\n\nFor $h(X)$ **strictly decreasing**, we note that if a continuous function is increasing on its interval, then its inverse is also increasing and continuous. Similarly, if a continuous function $h(X)$ is decreasing on its interval, then its inverse $h^{-1}(Y)$ is also decreasing and continuous (and, consequently, the derivative of both $h(X)$ and $h^{-1}(Y)$ will be negative). We have:\n\n$$\n\\begin{split}\nf_Y(y) &= -f_X\\Big(h^{-1}(y)\\Big) \\frac{d}{dy} \\Big[ h^{-1}(y) \\Big] \\\\\n&= f_X\\Big(h^{-1}(y) \\Big)   \\Bigg| \\frac{d}{dy} \\Big[h^{-1}(y)  \\Big]\\Bigg|\n\\end{split}\n$$\n\n\n**Derivation**\n\n$$\n\\begin{split}\nF_Y(y) &= P(Y \\le y) \\\\\n&=P\\Big(h(X) \\le y\\Big) \\\\ \n&= P\\Big(X \\ge h^{-1}(y)\\Big) \\\\\n&= 1 - P\\Big(X \\le h^{-1}(y)\\Big) \\\\\n&= 1 - F_X\\Big(h^{-1}(y) \\Big) \\\\\nf_Y(y) &= \\frac{d}{dy} \\Bigg[1 - F_X\\Big(h^{-1}(y) \\Big) \\Bigg] \\\\\n&= -\\frac{d}{dy} F_X\\Big(h^{-1}(y) \\Big) \\\\\n&= -f_X\\Big(h^{-1}(y) \\Big) \\cdot \\frac{d}{dy} \\Big[h^{-1}(y)  \\Big] \\\\\n\\text{Because } h(X) &\\text{ is strictly decreasing, } h^{-1}(y) \\text{ is also decreasing and thus } \\frac{d}{dy} \\Big[h^{-1}(y) \\Big]  < 0. \\\\\n\\Rightarrow f_Y(y) &=  f_X\\Big(h^{-1}(y) \\Big) \\cdot -\\frac{d}{dy} \\Big[h^{-1}(y)  \\Big] \\\\\n&= f_X\\Big(h^{-1}(y) \\Big) \\Bigg| \\frac{d}{dy} \\Big[h^{-1}(y)  \\Big] \\Bigg| \\\\\n\\end{split}\n$$\n\n\nFor example, let $Y = h(X) = -e^X$. Then, \n\n$$\n\\begin{split}\nY &= -e^X = h(X) \\\\\n-Y &=e^X \\\\\nX &= \\ln(-Y) = h^{-1}(Y) \\\\\nf_Y(y) &= f_X\\Big(h^{-1}(y) \\Big)   \\Bigg| \\frac{d}{dy} \\Big[h^{-1}(y)  \\Big]\\Bigg| \\\\\n&= f_X\\Big(\\ln(-y) \\Big)   \\Bigg| \\frac{d}{dy} \\Big[\\ln(-y)  \\Big]\\Bigg| \\\\\n&= f_X\\Big(\\ln(-y) \\Big) \\Bigg| \\frac{-1}{-y} \\Bigg| \\\\\n&= \\Bigg|\\frac{1}{y} \\Bigg|f_X\\Big(\\ln(-y)\\Big) \\\\\n\\end{split}\n$$\n\n\n\n\n\n\\newpage\n\n## Score vector\n\nWe have the loglikelihood based on data **for a single subject** $X_i$:\n\n$$\\ell_i(\\theta) = \\ln f_X(X_i;\\theta) = \\ln L_i(\\theta)$$\n\nThe score vector is the derivative of the loglikelihood based on data for a single subject $X_i$:\n\n$$S_i(\\theta) \\equiv \\frac{\\partial \\ell_i(\\theta)}{\\partial \\theta} = \\frac{\\partial \\ln L_i(\\theta)}{\\partial \\theta}$$\n\nThe MLE is obtained by solving $\\displaystyle \\frac{1}{n} \\sum_{i = 1}^n S_i(\\theta) = 0$.\n\nTo obtain the MLE from the score vector, follow these steps:\n\n$$\n\\begin{split}\n\\text{ For a } &\\text{single subject } X_i, \\\\\nL_i(\\theta | X_i) &= f_X(X_i; \\theta) \\\\\n\\ell_i(\\theta) &= \\ln f_X(X_i; \\theta) \\\\\nS_i(\\theta) &= \\frac{\\partial \\ell_i(\\theta)}{\\partial \\theta} \\\\\n\\text{To obtain} &\\text{ the MLE,  solve} \\\\\n\\frac{1}{n} \\sum_{i = 1}^n S_i(\\theta) &= 0 \\\\\n\\end{split}\n$$\n\nA key property of the score statistic is, for any $\\theta$, and assuming regularity conditions that allow interchanging the derivative and integral:\n\n$$\nE_\\theta \\Big[S_i(\\theta)\\Big] = 0\n$$\n\n\nThus, the solution $\\hat \\theta$ to $\\displaystyle\\frac{1}{n} \\sum_{i = 1}^n S_i(\\theta) = 0$ will be close to the population value, which solves $\\displaystyle E_\\theta \\Big[S_i(\\theta)\\Big] = 0$. We use this fact later to show that:\n\n- MLEs are unbiased in large samples\n- MLEs converge to the truth as more data are collected\n\n\n### Example: Score vector for Exponential distribution\n\n\n$$\n\\begin{split}\nX &\\sim \\text{Exp}(\\theta) \\\\\nf_X(x) &= \\theta\\exp(-\\theta x) \\\\\n&\\text{ For a single subject } X_i, \\\\\nL_i(\\theta | X_i) &= f_X(X_i; \\theta) \\\\\n&= \\theta \\exp (-\\theta X_i) \\\\\n\\ell_i(\\theta) &= \\ln f_X(X_i; \\theta) \\\\\n&=\\ln \\theta -\\theta X_i \\\\\nS_i(\\theta) &= \\frac{\\partial \\ell_i(\\theta)}{\\partial \\theta} \\\\\n&= \\frac{1}{\\theta} - X_i \\\\\n\\text{To obtain the } &\\text{MLE, we solve} \\\\\n\\frac{1}{n} \\sum_{i = 1}^n S_i(\\theta) &= 0 \\\\\n\\frac{1}{n} \\sum_{i = 1}^n \\frac{1}{\\theta} - X_i &= 0 \\\\\n\\frac{1}{n} \\sum_{i = 1}^n \\frac{1}{\\theta} &= \\frac{1}{n} \\sum_{i = 1}^n X_i \\\\\n\\frac{1}{n} \\cdot \\frac{n}{\\theta}  &=\\frac{1}{n} \\sum_{i = 1}^n X_i \\\\\n\\frac{1}{\\theta}&=\\frac{1}{n} \\sum_{i = 1}^n X_i \\\\ \n\\hat \\theta &= \\frac{1}{\\bar X_n}\n\\end{split}\n$$\n\n\n### Example: Score vector for Poisson distribution\n\n\n$$\n\\begin{split}\nX &\\sim \\text{Po}(\\theta)\\\\\nf_X(x) &= \\frac{\\theta^x}{x!} e^{-\\theta}\\\\\n\\text{ For a } &\\text{single subject } X_i, \\\\\nL_i(\\theta | X_i) &= f_X(X_i; \\theta) \\\\\n&= \\frac{\\theta^{X_i}}{X_i!} e^{-\\theta} \\\\\n\\ell_i(\\theta) &= \\ln f_X(X_i; \\theta) \\\\\n&= X_i\\ln\\theta  -\\theta - \\ln X_i!  \\\\\nS_i(\\theta) &= \\frac{\\partial \\ell_i(\\theta)}{\\partial \\theta} \\\\\n&= \\frac{X_i}{\\theta} -1\\\\\n\\text{To obtain} &\\text{ the MLE,  solve} \\\\\n\\frac{1}{n} \\sum_{i = 1}^n S_i(\\theta) &= 0 \\\\\n\\frac{1}{n} \\sum_{i = 1}^n  \\frac{X_i}{\\theta} -1 &= 0 \\\\\n\\frac{1}{n} \\sum_{i = 1}^n  \\frac{X_i}{\\theta} - \\frac{1}{n} \\sum_{i = 1}^n 1 &= 0 \\\\\n\\frac{1}{n} \\sum_{i = 1}^n  \\frac{X_i}{\\theta} &= \\frac{1}{n} \\sum_{i = 1}^n 1 \\\\\n\\frac{1}{\\theta} \\frac{1}{n} \\sum_{i = 1}^n X_i &= \\frac{n}{n} \\\\\n\\frac{1}{\\theta} \\frac{1}{n} \\sum_{i = 1}^n X_i &= 1 \\\\\n\\hat \\theta &=  \\frac{1}{n} \\sum_{i = 1}^n X_i \\\\\n&= \\bar X_n\n\\end{split}\n$$\n\n\n### Example: Score vector for Normal distribution\n\n\n$$\n\\begin{split}\nX &\\sim N(\\mu, \\sigma^2)\\\\\n\\text{ For a } &\\text{single subject } X_i, \\\\\nL_i(\\theta | X_i) &= f_X(X_i; \\theta) \\\\\n&= \\dfrac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\Bigg(-\\dfrac{(X_i - \\mu)^2}{2\\sigma^2}\\Bigg) \\\\\n\\ell_i(\\theta) &= \\ln f_X(X_i; \\theta) \\\\\n&= -\\ln\\sigma-\\ln\\sqrt{2\\pi} - \\frac{(X_i - \\mu)^2}{2\\sigma^2}\\\\\nS_i(\\mu) &= \\frac{\\partial \\ell_i(\\theta)}{\\partial \\mu} \\\\\n&= -\\frac{1}{2\\sigma^2} \\cdot 2(X_i-\\mu) \\cdot -1 \\\\\n&= \\frac{2(X_i - \\mu)}{2\\sigma^2}\\\\\n&= \\frac{X_i - \\mu}{\\sigma^2}\\\\\n\\text{To obtain} &\\text{ the MLE for } \\mu, \\text{ solve} \\\\\n\\frac{1}{n} \\sum_{i = 1}^n S_i(\\mu) &= 0 \\\\\n\\frac{1}{n} \\sum_{i = 1}^n \\frac{X_i - \\mu}{\\sigma^2} &= 0   \\\\\n\\frac{1}{\\sigma^2} \\frac{1}{n} \\sum_{i = 1}^n X_i - \\mu &= 0 \\\\\n\\frac{1}{n} \\sum_{i = 1}^n X_i - \\mu &= 0 \\\\\n\\frac{1}{n} \\sum_{i = 1}^n X_i &= \\frac{1}{n} \\sum_{i = 1}^n \\mu \\\\\n\\frac{1}{n} \\sum_{i = 1}^n X_i &= \\frac{n\\mu}{\\mu} = \\mu \\\\\n\\Rightarrow \\hat \\mu &= \\bar X_n \\\\\n\\end{split}\n$$\n\n$$\n\\begin{split}\n\\text{Rewrite } \\ell_i(\\theta) &= -\\ln\\sigma-\\ln\\sqrt{2\\pi} - \\frac{(X_i - \\mu)^2}{2} \\cdot \\frac{1}{\\sigma^2} \\\\\n&=  -\\ln\\sigma-\\ln\\sqrt{2\\pi} - \\frac{(X_i - \\mu)^2}{2} \\cdot \\sigma^{-2} \\\\\nS_i(\\sigma) &= \\frac{\\partial \\ell_i(\\theta)}{\\partial \\sigma} \\\\\n&= -\\frac{1}{\\sigma} - \\frac{(X_i - \\mu)^2}{2} \\cdot -2\\sigma^{-3} \\\\\n&= -\\frac{1}{\\sigma} + \\frac{(X_i - \\mu)^2}{\\sigma^3} \\\\\n&= \\frac{1}{\\sigma} \\Bigg(-1 +  \\frac{(X_i - \\mu)^2}{\\sigma^2} \\Bigg) \\\\\n\\text{Equivalently, } S_i(\\sigma^2) &= -\\frac{1}{2\\sigma^2} + \\frac{(X_i - \\mu)^2}{2\\sigma^4}  \\\\\n\\text{To obtain} &\\text{ the MLE for } \\sigma \\text{ and } \\sigma^2, \\text{ solve} \\\\\n\\frac{1}{n} \\sum_{i = 1}^n S_i(\\sigma) &= 0 \\\\\n\\frac{1}{n} \\sum_{i = 1}^n \\frac{1}{\\sigma} \\Bigg(-1 +  \\frac{(X_i - \\mu)^2}{\\sigma^2} \\Bigg) &= 0 \\\\\n\\frac{1}{\\sigma}  \\frac{1}{n} \\sum_{i = 1}^n -1 +  \\frac{(X_i - \\mu)^2}{\\sigma^2} &= 0 \\\\\n\\frac{1}{n} \\sum_{i = 1}^n -1 +  \\frac{(X_i - \\mu)^2}{\\sigma^2} &= 0 \\\\\n\\frac{1}{n} \\sum_{i = 1}^n \\frac{(X_i - \\mu)^2}{\\sigma^2} &=  \\frac{1}{n} \\sum_{i = 1}^n 1 \\\\\n&= \\frac{n}{n} = 1\\\\\n\\frac{1}{\\sigma^2} \\frac{1}{n} \\sum_{i = 1}^n  (X_i - \\mu)^2 &= 1 \\\\\n\\hat{\\sigma^2} &=\\frac{1}{n} \\sum_{i = 1}^n  (X_i - \\mu)^2\n\\end{split}\n$$\n\nThus, we have obtained the score vector for a single individual and the (total) score:\n\n$$\n\\begin{split}\nS_i(\\mu,\\sigma) &= \n\\begin{Bmatrix}\n\\dfrac{X_i - \\mu}{\\sigma^2}  \\\\\n -\\dfrac{1}{\\sigma} + \\dfrac{(X_i - \\mu)^2}{\\sigma^3} \\\\\n\\end{Bmatrix} \\\\\nS(\\mu,\\sigma) &= \n\\begin{Bmatrix}\n\\displaystyle \\sum_{i = 1}^n \\dfrac{X_i - \\mu}{\\sigma^2}  \\\\\n\\displaystyle \\sum_{i = 1}^n  -\\dfrac{1}{\\sigma} + \\dfrac{(X_i - \\mu)^2}{\\sigma^3} \\\\\n\\end{Bmatrix} \\\\\n\\end{split}\n$$\n\n\n\n\n## Fisher information matrix and large-sample variance\n\nThere are two equivalent definitions/ways of obtaining the Fisher information matrix\n\n\n$$\n\\begin{split}\nI_1(\\theta) &= - E\\Bigg[\\frac{\\partial}{\\partial \\theta} S_i(\\theta) \\Bigg] \\\\\n&= - E\\Bigg[\\frac{\\partial^2}{\\partial \\theta^2}  \\ln f_X(X_i;\\theta) \\Bigg] \\\\\nI_1(\\theta) &= E\\Bigg[ \\Big\\{S_i(\\theta)\\Big\\}^2 \\Bigg] \\\\\n&=E\\Bigg[ \\Big\\{  \\frac{\\partial}{\\partial \\theta} \\ln f_X(X_i;\\theta)\\Big\\}^2 \\Bigg]\n\\end{split}\n$$\n\n\nWhen the model is correct, assuming regularity conditions that allow interchanging the derivatives and integral, then\n\n$$\n\\text{Var}_\\theta \\Big[ S_i(\\theta) \\Big] = - E_\\theta\\Bigg[\\frac{\\partial}{\\partial \\theta} S_i (\\theta) \\Bigg] \\equiv I(\\theta)\n$$\n\n\n\n**Distribution of the MLE in large samples**\n\nIn large samples, the MLE is normal with large-sample variance (in the univariate case) or large-sample covariance matrix (in the multivariate case)\n\n$$\n\\text{Var}(\\hat \\theta) = \\frac{1}{nI(\\theta)}\n$$\n\nThe **total Fisher information** equals $\\displaystyle nI(\\theta)$.\n\n### Multivariate Fisher information\n\nIn the multivariate parameter case, the Fisher information matrix $I(\\theta)$ is the matrix of second derivatives of the log likelihood for a single individual, with elements\n\n\n$$\nI_{jk}(\\theta) = - E_\\theta \\Bigg[\\frac{\\partial^2}{\\partial\\theta_j\\partial\\theta_k} \\ell_i(\\theta) \\Bigg]\n$$\n\n\nThe large sample variance of the MLE for a parameter $\\theta$ when the other parameter $\\eta$ is known equals $\\displaystyle \\Big(n I_{\\theta\\theta}\\Big)^{-1}$.\n\nWhen the other parameter $\\eta$ is unknown, we have the Fisher information\n\n$$\\displaystyle I^*_{\\theta\\theta} = I_{\\theta\\theta} - \\Big[ I_{\\theta\\eta} \\cdot I^{-1}_{\\eta\\eta} \\cdot I_{\\eta\\theta} \\Big] = I_{\\theta\\theta} - \\dfrac{I_{\\theta\\eta}^2}{I_{\\eta\\eta}}$$\n\n\nThen, we have the large sample variance of the MLE for $\\theta$ with the other parameter unknown: $\\displaystyle \\Big(n I^*_{\\theta\\theta}\\Big)^{-1}$. And we have the total information $nI^*_{\\theta\\theta}$.\n\nNote that, when the off-diagonal elements ($I_{\\theta\\eta}$ and $I_{\\eta\\theta}$) equal zero, then the large sample variance of the MLe for $\\theta$ and $\\eta$ with the other parameter unknown will necessarily equal the large sample variance of the MLE for $\\theta$ and $\\eta$ with the other parameter known. That is: \n\n\n$$\\displaystyle I^*_{\\theta\\theta} = I_{\\theta\\theta} - \\dfrac{I_{\\theta\\eta}^2}{I_{\\eta\\eta}} = I_{\\theta\\theta} - \\dfrac{0}{I_{\\eta\\eta}} = I_{\\theta\\theta} $$\n\n\n\n### Example: Fisher information matrix for Exponential distribution\n\n#### Approach 1 \n\n\n$$\n\\begin{split}\nX &\\sim \\text{Exp}(\\theta) \\\\\nS_i(\\theta) &= \\frac{1}{\\theta} - X_i \\\\\n&= \\theta^{-1} - X_i \\\\\nI_\\theta &= - E_\\theta\\Bigg[\\frac{\\partial}{\\partial \\theta} S_i (\\theta) \\Bigg] \\\\\n&= - E_\\theta \\Big[-\\theta^{-2} \\Big]\\\\\n&= -E_\\theta \\Big[-\\frac{1}{\\theta^2}\\Big] \\\\\n&= E_\\theta \\Big[\\frac{1}{\\theta^2}\\Big] \\\\\n&= \\frac{1}{\\theta^2} \\\\\nnI_\\theta &= \\frac{n}{\\theta^2}, \\text{the total Fisher information }\\\\ \n\\text{We have the MLE } \\hat \\theta &= \\frac{1}{\\bar X_n} \\text{ and } \\\\\n\\text{Var}(\\hat \\theta) &= \\frac{1}{nI(\\theta)}, \\text{the large-sample variance} \\\\\n&= \\frac{1}{n} \\cdot \\theta^2 \\\\\n&= \\frac{\\theta^2}{n}\n\\end{split}\n$$\n\n\n#### Approach 2\n\nWe have\n\n$$\n\\begin{split}\nE[X] &= \\frac{1}{\\theta} \\\\\n\\text{Var}[X] &= \\frac{1}{\\theta^2} \\\\\nI_1(\\theta) &= E\\Big[ \\{S_i(\\theta)\\}^2 \\Big] \\\\\n&= E\\Big[\\Big(\\frac{1}{\\theta} - X \\Big)^2 \\Big] \\\\\n&= E\\Big[\\frac{1}{\\theta^2} - 2\\frac{X}{\\theta} + X^2 \\Big] \\\\\n&= \\frac{1}{\\theta^2} - \\frac{2}{\\theta}E[X] + E[X^2] \\\\\n&= \\frac{1}{\\theta^2} - \\frac{2}{\\theta^2} + \\text{Var}[X] + (E[X])^2\\\\\n&= \\frac{1}{\\theta^2} - \\frac{2}{\\theta^2} + \\frac{1}{\\theta^2} + \\frac{1}{\\theta^2} \\\\\n&= \\frac{1}{\\theta^2}\\\\\n\\end{split}\n$$\n\n\n\n\\newpage\n\n### Example: Fisher information matrix for Poisson distribution\n\n#### Approach 1 \n\n\n$$\n\\begin{split}\nX &\\sim \\text{Poisson}(\\theta) \\\\\nS_i(\\theta) &=  \\frac{X_i}{\\theta} -1\\\\\nI_\\theta &= - E_\\theta\\Bigg[\\frac{\\partial}{\\partial \\theta} S_i (\\theta) \\Bigg] \\\\\n&= - E_\\theta\\Bigg[\\frac{\\partial}{\\partial \\theta} \\frac{X_i}{\\theta} -1\\Bigg] \\\\\n&= - E_\\theta\\Bigg[\\frac{\\partial}{\\partial \\theta} X_i \\cdot \\theta^{-1} -1\\Bigg] \\\\\n&= - E_\\theta \\Big[X_i \\cdot -\\theta^{-2} \\Big]\\\\\n&= -E_\\theta \\Big[-\\frac{X_i}{\\theta^2}\\Big] \\\\\n&= E_\\theta \\Big[\\frac{X_i}{\\theta^2}\\Big] \\\\\n&= \\frac{E_\\theta[X_i]}{\\theta^2} \\\\\n&= \\frac{\\theta}{\\theta^2}\\\\\n&= \\frac{1}{\\theta} \\\\\nnI(\\theta) &= \\frac{n}{\\theta}, \\text{ the total Fisher information}\\\\\n\\text{We have the MLE } \\hat \\theta &= \\bar X_n \\text{ and } \\\\\n\\text{Var}(\\hat \\theta) &= \\frac{1}{nI(\\theta)}, \\text{ the large-sample variance} \\\\\n&= \\frac{\\theta}{n}\n\\end{split}\n$$\n\n\n\n#### Approach 2\n\n\n$$\n\\begin{split}\nE[X] &= \\text{Var}[X] = \\theta \\\\\nI_1(\\theta) &= E\\Big[ \\{S_i(\\theta)\\}^2 \\Big] \\\\ \n&= E\\Big[ \\Big(\\frac{X}{\\theta} - 1 \\Big)^2 \\Big] \\\\\n&= E\\Big[\\frac{X^2}{\\theta^2} - 2\\frac{X}{\\theta} + 1 \\Big] \\\\\n&= \\frac{1}{\\theta^2}E[X^2] -\\frac{2}{\\theta}E[X] + 1\\\\\n&= \\frac{1}{\\theta^2} \\Big(\\text{Var}[X] + \\{E[X]\\}^2 \\Big) -\\frac{2\\theta}{\\theta} + 1 \\\\\n&= \\frac{1}{\\theta^2} \\Big(\\theta + \\theta^2 \\Big) -2+1 \\\\\n&=\\frac{1}{\\theta} + 1- 1 \\\\\n&= \\frac{1}{\\theta}\n\\end{split}\n$$\n\n\n### Example: Fisher information for Normal Distribution\n\n\n$$\n\\begin{split}\nS_i(\\mu,\\sigma) &= \n\\begin{Bmatrix}\n\\displaystyle\\frac{\\partial}{\\partial \\mu} \\ell_i(\\mu, \\sigma)  \\\\\n\\displaystyle\\frac{\\partial}{\\partial \\sigma} \\ell_i(\\mu, \\sigma) \\\\\n\\end{Bmatrix} \\\\\n&= \n\\begin{Bmatrix}\n\\dfrac{X_i - \\mu}{\\sigma^2}  \\\\\n -\\dfrac{1}{\\sigma} + \\dfrac{(X_i - \\mu)^2}{\\sigma^3} \\\\\n\\end{Bmatrix} \\\\\nI_{jk}(\\theta) &= - E_\\theta \\Bigg[\\frac{\\partial^2}{\\partial\\theta_j\\partial\\theta_k} \\ell_i(\\theta) \\Bigg] \\\\\nI_{11}(\\theta) &= - E_\\theta \\Bigg[\\frac{\\partial}{\\partial\\mu}\\frac{\\partial}{\\partial\\mu} \\ell_i(\\theta) \\Bigg]\\\\\n&=- E_\\theta \\Bigg[\\frac{\\partial}{\\partial\\mu} \\dfrac{X_i - \\mu}{\\sigma^2} \\Bigg]\\\\\n&=- E_\\theta \\Bigg[\\frac{\\partial}{\\partial\\mu} \\dfrac{X_i}{\\sigma^2} - \\dfrac{\\mu}{\\sigma^2}  \\Bigg]\\\\\n&=- E_\\theta \\Bigg[-\\frac{1}{\\sigma^2}  \\Bigg]  \\\\\n&= E_\\theta \\Bigg[\\frac{1}{\\sigma^2}  \\Bigg]  \\\\\n&= \\frac{1}{\\sigma^2} \\\\\nI_{21}(\\theta) = I_{12}(\\theta) &= - E_\\theta \\Bigg[\\frac{\\partial}{\\partial\\sigma}\\frac{\\partial}{\\partial\\mu} \\ell_i(\\theta) \\Bigg]\\\\\n&= - E_\\theta \\Bigg[\\frac{\\partial}{\\partial\\sigma}\\dfrac{X_i - \\mu}{\\sigma^2} \\Bigg] \\\\\n&= - E_\\theta \\Bigg[\\frac{\\partial}{\\partial\\sigma}(X_i - \\mu) \\cdot \\sigma^{-2} \\Bigg] \\\\\n&= - E_\\theta \\Bigg[(X_i - \\mu) \\cdot -2\\sigma^{-3} \\Bigg] \\\\\n&= - E_\\theta \\Bigg[-\\frac{2(X_i - \\mu)}{\\sigma^3}  \\Bigg] \\\\\n&=  E_\\theta \\Bigg[\\frac{2(X_i - \\mu)}{\\sigma^3}  \\Bigg] \\\\\n&=  \\frac{2}{\\sigma^3}E_\\theta \\Big[X_i - \\mu \\Big] \\\\\n&= \\frac{2}{\\sigma^3} \\Big[E_\\theta[X_i] - \\mu \\Big] \\\\\n&= \\frac{2}{\\sigma^3} \\Big[\\mu - \\mu \\Big] = 0 \\\\\nI_{22}(\\theta) &= - E_\\theta \\Bigg[\\frac{\\partial}{\\partial\\sigma}\\frac{\\partial}{\\partial\\sigma} \\ell_i(\\theta) \\Bigg]\\\\\n&= - E_\\theta \\Bigg[\\frac{\\partial}{\\partial\\sigma}-\\dfrac{1}{\\sigma} + \\dfrac{(X_i - \\mu)^2}{\\sigma^3} \\Bigg]\\\\\n&= - E_\\theta \\Bigg[\\frac{\\partial}{\\partial\\sigma}-\\sigma^{-1} + (X_i - \\mu)^2 \\cdot \\sigma^{-3} \\Bigg]\\\\\n&= - E_\\theta \\Bigg[\\frac{1}{\\sigma^2} + (X_i - \\mu)^2 \\cdot -3\\sigma^{-4} \\Bigg]\\\\\n&= - E_\\theta \\Bigg[\\frac{1}{\\sigma^2} + (X_i - \\mu)^2 \\cdot \\frac{-3}{\\sigma^4}  \\Bigg]\\\\\n&= -\\frac{1}{\\sigma^2} - E_\\theta \\Bigg[ (X_i - \\mu)^2 \\cdot \\frac{-3}{\\sigma^4}  \\Bigg]\\\\\n&= -\\frac{1}{\\sigma^2} + \\frac{3}{\\sigma^4}E_\\theta \\Big[ (X_i - \\mu)^2   \\Big]\\\\\n&= -\\frac{1}{\\sigma^2} + \\frac{3}{\\sigma^4}E_\\theta \\Bigg[ \\Big(X_i - E[X_i]\\Big)^2   \\Bigg]\\\\\n&= -\\frac{1}{\\sigma^2} + \\frac{3}{\\sigma^4}\\text{Var}[X]\\\\\n&= -\\frac{1}{\\sigma^2} + \\frac{3\\sigma^2}{\\sigma^4}\\\\\n&= \\frac{3}{\\sigma^2} - \\frac{1}{\\sigma^2} \\\\\n&= \\frac{2}{\\sigma^2} \\\\\n\\end{split}\n$$\n\nThus, we have obtained  \n\n$$\n\\begin{split}\n\\text{The Fisher information matrix } I(\\theta) &=\n\\begin{bmatrix}\n\\displaystyle \\frac{1}{\\sigma^2} & 0 \\\\\n 0 & \\displaystyle \\frac{2}{\\sigma^2}\\\\\n\\end{bmatrix} \\\\\n\\text{The total Fisher information matrix } nI(\\theta) &=\n\\begin{bmatrix}\n\\displaystyle \\frac{n}{\\sigma^2} & 0 \\\\\n 0 & \\displaystyle \\frac{2n}{\\sigma^2}\\\\\n\\end{bmatrix} \\\\\n\\text{The large-sample covariance matrix } \\frac{1}{nI(\\theta)} &=\n\\begin{bmatrix}\n\\displaystyle \\frac{\\sigma^2}{n} & 0 \\\\\n 0 & \\displaystyle \\frac{\\sigma^2}{2n}\\\\\n\\end{bmatrix} \\\\\n\\end{split}\n$$\n\n\nBecause the off-diagonal elements of the Fisher information matrix equal $0$, we can immediately infer that the large sample variance of $\\hat \\mu$ equals $\\dfrac{\\sigma^2}{n}$ regardless of whether $\\sigma$ is known; the large sample variance of $\\hat \\sigma^2$ equals $\\dfrac{\\sigma^2}{2n}$ regardless of whether $\\sigma$ is known. \n\n### Example: Fisher information for Poisson regression\n\n\n$$\n\\begin{split}\nS_i(\\beta_0,\\beta_1) &= \n\\begin{Bmatrix}\n\\displaystyle\\frac{\\partial}{\\partial \\beta_0} \\ell_i(\\mu, \\sigma)  \\\\\n\\displaystyle\\frac{\\partial}{\\partial \\beta_1} \\ell_i(\\mu, \\sigma) \\\\\n\\end{Bmatrix} \\\\\n&= \n\\begin{Bmatrix}\nY - \\exp(\\beta_0 + \\beta_1X)  \\\\\nXY - X\\exp(\\beta_0 + \\beta_1X) \\\\\n\\end{Bmatrix} \\\\\nI_{11}(\\theta)&= -E\\Bigg[ \\frac{\\partial}{\\partial \\beta_0} Y - \\exp(\\beta_0 + \\beta_1X) \\Bigg] \\\\\n&= - E\\Big[-\\exp(\\beta_0 + \\beta_1X) \\Big] \\\\\n&= E\\Big[\\exp(\\beta_0 + \\beta_1X) \\Big] \\\\\nI_{22}(\\theta)&= -E\\Bigg[ \\frac{\\partial}{\\partial \\beta_1} XY - X\\exp(\\beta_0 + \\beta_1X) \\Bigg] \\\\\n&= - E\\Big[-X^2\\exp(\\beta_0 + \\beta_1X) \\Big] \\\\\n&= E\\Big[X^2\\exp(\\beta_0 + \\beta_1X) \\Big] \\\\\nI_{12}(\\theta) = I_{21}(\\theta)&= -E\\Bigg[ \\frac{\\partial}{\\partial \\beta_1} Y - \\exp(\\beta_0 + \\beta_1X) \\Bigg] \\\\\n&= -E\\Big[-X\\exp(\\beta_0 + \\beta_1X) \\Big] \\\\\n&= E\\Big[X\\exp(\\beta_0 + \\beta_1X) \\Big] \\\\\n\\end{split}\n$$\n\n\nWe have obtained the Fisher information matrix,\n\n\n$$\n\\begin{split}\nI(\\theta) &=\n\\begin{bmatrix}\nE\\Big[\\exp(\\beta_0 + \\beta_1X) \\Big] & E\\Big[X\\exp(\\beta_0 + \\beta_1X) \\Big] \\\\\nE\\Big[X\\exp(\\beta_0 + \\beta_1X) \\Big] & E\\Big[X^2\\exp(\\beta_0 + \\beta_1X) \\Big]\\\\\n\\end{bmatrix} \\\\\n\\text{if } \\beta_0 \\text{ is known, } I_{\\beta_1\\beta_1} &=E\\Big[X^2\\exp(\\beta_0 + \\beta_1X) \\Big]\\\\\n\\text{Var}(\\hat \\beta_1) &= \\frac{1}{nE\\Big[X^2\\exp(\\beta_0 + \\beta_1X) \\Big]} \\\\\n\\text{if } \\beta_0 \\text{ is unknown, } I^*_{\\beta_1\\beta_1} &= I_{\\beta_1\\beta_1} - \\frac{(I_{\\beta_0\\beta_1})^2}{I_{\\beta_0\\beta_0}}\\\\\n&= E\\Big[X^2\\exp(\\beta_0 + \\beta_1X) \\Big] - \\frac{E[X\\exp(\\beta_0 + \\beta_1X)]^2}{E[\\exp(\\beta_0 + \\beta_1X)]}\n\\\\\n\\text{Var}(\\hat \\beta_1) &= \\frac{1}{n} \\Bigg(E\\Big[X^2\\exp(\\beta_0 + \\beta_1X) \\Big] - \\frac{E[X\\exp(\\beta_0 + \\beta_1X)]^2}{E[\\exp(\\beta_0 + \\beta_1X)]}\\Bigg)^{-1}\n\\end{split}\n$$\n\n\n\n\n\\newpage\n\n### Example: Fisher information for Gamma distribution\n\nIf both the shape $\\alpha$ and rate $\\lambda$ are unknown for iid gamma variates $X_i \\sim$ Ga($\\alpha, \\lambda$), the Fisher information for $\\theta = (\\alpha, \\lambda)$ is\n\n\n$$\n\\begin{split}\nI(\\theta) &= \\begin{bmatrix}\n\\psi'(\\alpha) & -\\lambda^{-1} \\\\\n-\\lambda^{-1} & \\alpha\\lambda^{-2}\n\\end{bmatrix} \\text{ where }\\\\ \n\\ell_i(\\theta) &= \\alpha \\ln(\\lambda) + (\\alpha - 1)\\ln X-\\lambda X -\\ln \\Gamma(\\alpha)\n\\end{split}\n$$\n\n\nNB: $\\psi'(\\alpha)$ is the trigamma function (the second derivative of the log of $\\Gamma(\\alpha)$). \n\nAbove, we used the shape-rate form of the Gamma distribution. If, instead, we use the shape-scale form($\\alpha, \\beta$) with $\\alpha$ known, we have\n\n\n$$\n\\begin{split}\n\\ell(\\beta) &= -n\\ln\\Gamma(\\alpha) -n\\alpha\\ln\\beta + (a - 1)\\sum_{i = 1}^n \\ln x_i - \\frac{\\sum_{i = 1}^n x_i}{\\beta} \\\\\nS(\\beta) &= \\frac{n\\alpha}{\\beta^2}\\Bigg(\\frac{\\sum_{i = 1}^n x_i}{n\\alpha} -\\beta \\Bigg)\n\\end{split}\n$$\n\n\n\n\\newpage\n\n## Cramer-Rao inequality\n\n**Minimum Variance Unbiased Estimators (MVUE) for** $\\theta$: An unbiased estimator (for every $\\theta$) whose variance is no larger than that of any other unbiased estimator. \n\n**Cramer-Rao information inequality for univariate parameters**\n\nLet $W(\\pmb X)$ be an unbiased estimator of a scalar parameter $\\tau(\\theta)$. Then\n\n$$\n\\begin{split}\n\\text{Var}\\{W(\\pmb X)\\} &\\ge \\frac{\\{\\frac{\\partial}{\\partial\\theta} \\tau(\\theta)\\}^2}{nI(\\theta)} \\\\\n\\text{Var}\\{W(\\pmb X)\\} &\\ge {\\Big\\{\\frac{\\partial}{\\partial\\theta} \\tau(\\theta)\\Big\\}^2} \\frac{1}{nI(\\theta)}\n\\end{split}\n$$\n\n\n**Cramer-Rao information inequality for multivariate parameters**\n\nLet $W(\\pmb X)$ be an unbiased estimator of a multivariate parameter $\\tau(\\theta)$.\n\n\n$$\n\\text{Var}\\{W(\\pmb X)\\} \\ge {\\Big\\{\\frac{\\partial}{\\partial\\theta} \\tau(\\theta)\\Big\\}^t} {\\Big\\{\\frac{\\partial}{\\partial\\theta} \\tau(\\theta)\\Big\\}}  \\frac{1}{nI(\\theta)}\n$$\n\n\nFor unbiased estimators of $\\tau(\\theta)$, the Cramer-Rao inequality provides **a lower bound for the variance**.\n\nFor unbiased estimators $W(\\pmb X)$ of $\\tau(\\theta)$ (i.e., a **function of** $\\theta$), we have\n\n\n$$\n\\text{Var}\\{W(\\pmb X)\\} \\ge \\underbrace{{\\Big\\{\\frac{\\partial}{\\partial\\theta} \\tau(\\theta)\\Big\\}^2} \\frac{1}{nI(\\theta)}}_{\\text{Cramer-Rao Lower Bound}}\n$$\n\n\nFor unbiased estimators $W(\\pmb X)$ of $\\theta$, we have \n\n\n$$\\text{Var}\\{W(\\pmb X)\\} \\ge \\underbrace{\\frac{1}{nI(\\theta)}}_{\\text{CRLB}}$$\n\nMLEs are Best Asymptotically Normal (i.e., asymptotically efficient): The MLE has a limiting normal distribution with the Cramer-Rao lower bound as variance. That is, \n\n$$\\text{Var}(\\hat \\theta)= \\frac{1}{nI(\\theta)}$$ \n\n\n### Example: Cramer-Rao Lower bound for Poisson Distribution\n\nGiven a sample $X_1, ..., X_n$ from the Poisson distribution with density \n\n$$\nf(x; \\theta) = e^{-\\theta} \\frac{\\theta^x}{x!}\n$$\n\nwith $\\theta$ unknown and $x \\in \\R$. We know that $E[X] = \\text{Var}[X] = \\theta$. Find the MLE of $\\theta$, the Fisher information and the corresponding large-sample variance of the MLE, and the Cramer-Rao lower bound for any unbiased estimator of $e^{-\\theta}$. \n\n\n$$\n\\begin{split}\n\\hat \\theta &= \\bar X_n \\\\\nI(\\theta) &= \\frac{1}{\\theta}  \\\\\nnI(\\theta) &= \\frac{n}{\\theta}  \\\\\n\\frac{1}{nI(\\theta)} &= \\frac{\\theta}{n} = \\text{Var}(\\hat \\theta) \\\\\n\\text{Var}\\{W(\\pmb X)\\} &\\ge {\\Big\\{\\frac{\\partial}{\\partial\\theta} \\tau(\\theta)\\Big\\}^2} \\frac{1}{nI(\\theta)} \\\\\n\\text{Var}\\{W(\\pmb X)\\} &\\ge {\\Big\\{\\frac{\\partial}{\\partial\\theta} e^{-\\theta}\\Big\\}^2} \\frac{\\theta}{n} \\\\\n\\text{Var}\\{W(\\pmb X)\\} &\\ge {\\Big\\{-e^{-\\theta}\\Big\\}^2} \\frac{\\theta}{n} \\\\\n\\text{Var}\\{W(\\pmb X)\\} &\\ge e^{-2\\theta} \\frac{\\theta}{n} \\\\\n\\text{Var}\\{W(\\pmb X)\\} &\\ge \\frac{\\theta e^{-2\\theta}}{n} \\\\\n\\end{split}\n$$\n\n\n\n\\newpage\n\n## Newton-Raphson method \n\nThe NR method is an iterative application of\n\n$$x_{n + 1} = x_n - f(x_n)/f'(x_n)$$\n\n\nFor example, let $f(x) = x^2$. Our goal is to solve $f(x) = 0$. Then,\n\n$$\n\\begin{split}\nf(x) &= x^2 \\\\\nf'x &= 2x\\\\\nx_{n + 1} &= x_n - f(x_n)/f'(x_n) \\\\\n&= x_n - \\frac{x_n^2}{2x_n} \\\\\n&= x_n - \\frac{1}{2}x_n \\\\\n&= \\frac{x_n}{2}\n\\end{split}\n$$\n\nThus, we see that the iteration converges towards 0, the solution to $f(x) = x^2 = 0$. \n\nOur goal is to find the solution to $\\displaystyle \\frac{1}{n} \\sum_{i = 1}^n S_i(\\theta) = 0$. We have\n\n$$\n\\begin{split}\nf(\\theta) &= \\frac{1}{n} \\sum_{i = 1}^n S_i(\\theta) \\\\\nf'(\\theta) &= \\frac{1}{n} \\sum_{i = 1}^n \\frac{\\partial}{\\partial \\theta} S_i(\\theta)\\\\\n\\theta_{n + 1} &= \\theta_n - f(\\theta_n)/f'(\\theta_n) \\\\\n&= \\theta_n - \\dfrac{\\frac{1}{n} \\sum_{i = 1}^n S_i(\\theta_n)}{\\frac{1}{n} \\sum_{i = 1}^n \\frac{\\partial}{\\partial \\theta} S_i(\\theta_n)}\n\\end{split}\n$$\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}