{
  "hash": "adaebfe84b67b5f063cfccd45ef7abe0",
  "result": {
    "markdown": "# Hypothesis testing\n\n## Wald, Score, and LR test statistic: Simple null \n  \nSuppose we want to test $H_0: \\theta = \\theta_0$ vs. $H_a: \\theta \\ne \\theta_0$. We can use the following test statistics (which are asymptotically equivalent):\n\n$$\n\\begin{split}\n\\text{Wald: }T_W &= \\frac{(\\hat \\theta_{\\text{MLE}} -\\theta_0)^2}{\\Big\\{nI_1(\\hat \\theta_{\\text MLE})\\Big\\}^{-1}} \\\\\n&= \\frac{(\\hat \\theta_{\\text{MLE}} -\\theta_0)^2}{\\text{Var}(\\hat \\theta_{\\text{MLE}})} \\\\\n&= nI_1(\\hat \\theta_{\\text{MLE}})(\\hat \\theta_{\\text{MLE}} -\\theta_0)^2   \\\\\n\\text{Score: } T_S &= \\frac{\\Big[S_n(\\theta_0)\\Big]^2}{nI_1(\\theta_0)}\\\\\n\\text{Likelihood ratio: } T_{LR} &= -2\\Big\\{\\ell(\\theta_0) - \\ell(\\hat \\theta_{\\text{MLE}}) \\Big\\} \\\\\n&=  2\\Big\\{\\ell(\\hat \\theta_{\\text{MLE}}) - \\ell(\\theta_0)  \\Big\\} \\\\\n&= -2\\ln\\Bigg\\{\\frac{L(\\theta_0)}{L(\\hat \\theta_{\\text{MLE}})} \\Bigg\\} \\\\\n&=  2\\ln\\Bigg\\{\\frac{ L(\\hat \\theta_{\\text{MLE}})}{L(\\theta_0)} \\Bigg\\}\n\\end{split}\n$$\n\n\n\n\\newpage\n\n### Example: Test statistics for Normal($\\theta,1$)\n\nLet iid $Y_i \\sim N(\\theta,1)$ and $H_0: \\theta = \\theta_0$. Obtain the three test statistics. \n\n$$\n\\begin{split}\nL(\\theta) &= \\prod_{i = 1}^n \\frac{1}{\\sqrt{2\\pi}} \\exp\\Bigg(-\\frac{1}{2}(Y_i - \\theta)^2 \\Bigg) \\\\\n&= (2\\pi)^{-n/2} \\exp\\Big(-\\frac{1}{2} \\sum_{i = 1}^n (Y_i - \\theta)^2\\Big) \\\\\n\\ell(\\theta) &= -n/2\\ln(2\\pi)  -\\frac{1}{2} \\sum_{i = 1}^n (Y_i - \\theta)^2 \\\\\nS_n(\\theta) &= \\frac{d}{d\\theta} \\ell(\\theta) \\\\ \n&= \\sum_{i = 1}^n Y_i - \\theta = 0 \\\\\n\\Rightarrow n\\theta &= \\sum_{i = 1}^n Y_i \\\\\n\\hat \\theta_{\\text{MLE}} &= \\frac{1}{n} \\sum_{i = 1}^n Y_i = \\bar Y \\\\\nI_1(\\theta) &= - E\\Big[\\frac{\\partial}{\\partial\\theta}  S_i(\\theta) \\Big] \\\\\n&= - E\\Big[\\frac{\\partial}{\\partial\\theta}  Y_i - \\theta \\Big] \\\\\n&= -E[-1] \\\\\n&=1 \\\\\nT_W &= (\\hat \\theta_{\\text{MLE}} - \\theta_0)^2 \\cdot nI_1(\\hat \\theta_{\\text{MLE}}) \\\\\n&= n(\\bar Y - \\theta_0)^2\n\\end{split}\n$$\n\n$$\n\\begin{split}\n\\ell(\\theta_0) &=  -n/2\\ln(2\\pi)  -\\frac{1}{2} \\sum_{i = 1}^n (Y_i - \\theta_0)^2 \\\\ \n\\ell(\\hat \\theta_{\\text{MLE}}) &=  -n/2\\ln(2\\pi)  -\\frac{1}{2} \\sum_{i = 1}^n (Y_i - \\bar Y)^2 \\\\ \nT_{LR} &= 2\\Big\\{\\ell(\\hat \\theta_{\\text{MLE}}) - \\ell(\\theta_0)  \\Big\\} \\\\\n&= 2\\Big\\{-n/2\\ln(2\\pi)  -\\frac{1}{2} \\sum_{i = 1}^n (Y_i - \\bar Y)^2 - \\Big( -n/2\\ln(2\\pi)  -\\frac{1}{2} \\sum_{i = 1}^n (Y_i - \\theta_0)^2\\Big)  \\Big\\} \\\\\n&= 2\\Big\\{-\\frac{1}{2} \\sum_{i = 1}^n (Y_i - \\bar Y)^2 + \\frac{1}{2} \\sum_{i = 1}^n (Y_i - \\theta_0)^2 \\Big\\} \\\\ \n&= 2\\Big\\{\\frac{1}{2} \\sum_{i = 1}^n (Y_i - \\theta_0)^2 -\\frac{1}{2} \\sum_{i = 1}^n (Y_i - \\bar Y)^2 \\Big\\} \\\\ \n&= 2\\Big\\{\\frac{1}{2} \\sum_{i = 1}^n (Y_i - \\theta_0)^2 -(Y_i - \\bar Y)^2 \\Big\\} \\\\ \n&=  \\sum_{i = 1}^n (Y_i - \\theta_0)^2 -(Y_i - \\bar Y)^2 \\\\\n&= \\sum_{i = 1}^n Y_i^2 - 2\\theta_0Y_i + \\theta_0^2 - (Y_i^2 - 2\\bar YY_i + \\bar Y^2) \\\\\n&=\\sum_{i = 1}^n - 2\\theta_0Y_i + \\theta_0^2 + 2\\bar YY_i - \\bar Y^2 \\\\\n&= -2\\theta_0\\sum_{i = 1}^n Y_i +n\\theta_0^2 + 2\\bar Y\\sum_{i=1}^n Y_i -n\\bar Y ^2 \\\\\n&= -2\\theta_0n\\bar Y +n\\theta_0^2 + 2\\bar Yn\\bar Y -n\\bar Y ^2 \\\\\n&= n\\Big(-2\\theta_0\\bar Y +\\theta_0^2 + 2\\bar Y^2 -\\bar Y ^2 \\Big) \\\\\n&= n\\Big( \\bar Y ^2 -2\\theta_0\\bar Y +\\theta_0^2  \\Big) \\\\\n&= n\\Big(\\bar Y - \\theta_0 \\Big)^2\n\\end{split}\n$$\n\n$$\n\\begin{split} \nT_S &= \\frac{\\Big[S_n(\\theta_0)\\Big]^2}{nI_1(\\theta_0)}\\\\\n&= \\frac{\\Big[S_n(\\theta_0)\\Big]^2}{n} \\\\\n&= \\frac{1}{n} \\Bigg({\\sum_{i = 1}^n Y_i - \\theta_0}\\Bigg)^2 \\\\\n&= \\frac{1}{n} \\Big(n\\bar Y - n\\theta_0 \\Big)^2 \\\\\n&= \\frac{1}{n} \\Big(n(\\bar Y - \\theta_0) \\Big)^2 \\\\\n&= \\frac{n^2}{n} (\\bar Y - \\theta_0)^2 \\\\\n&= n(\\bar Y - \\theta_0)^2\n\\end{split}\n$$\n\n### Example: Test statistics for Normal($\\mu, \\sigma^2$) with $\\sigma^2$ known\n\nNow, let iid $Y_i \\sim N(\\mu,\\sigma^2)$ with $\\sigma^2$ known and $H_0: \\mu = \\mu_0$. Obtain the three test statistics. \n\n$$\n\\begin{split}\n\\ell(\\mu) &= -n/2\\ln(2\\pi\\sigma^2)  -\\frac{1}{2\\sigma^2} \\sum_{i = 1}^n (Y_i - \\mu)^2 \\\\\nS_n(\\mu) &= \\frac{1}{\\sigma^2}\\sum_{i = 1}^n Y_i - \\mu = 0 \\\\\n\\hat \\mu_{\\text{MLE}} &= \\frac{1}{n} \\sum_{i = 1}^n Y_i = \\bar Y \\\\\nI_1(\\mu) &= - E\\Big[\\frac{\\partial}{\\partial\\mu}  S_i(\\mu) \\Big] \\\\\n&= - E\\Big[\\frac{\\partial}{\\partial\\mu}  \\frac{1}{\\sigma^2} \\Big(Y_i - \\mu \\Big) \\Big] \\\\\n&= -E\\Big[-\\frac{1}{\\sigma^2}\\Big] \\\\\n&=\\frac{1}{\\sigma^2}\\\\\nT_W &= (\\hat \\mu_{\\text{MLE}} - \\mu_0)^2 \\cdot nI_1(\\hat \\mu_{\\text{MLE}}) \\\\\n&= \\frac{n(\\bar Y - \\mu_0)^2}{\\sigma^2} \\\\\n\\end{split}\n$$\n\n$$\n\\begin{split}\n\\ell(\\mu_0) &= -n/2\\ln(2\\pi\\sigma^2)  -\\frac{1}{2\\sigma^2} \\sum_{i = 1}^n (Y_i - \\mu_0)^2 \\\\\n\\ell(\\hat \\mu_{\\text{MLE}}) &= -n/2\\ln(2\\pi\\sigma^2)  -\\frac{1}{2\\sigma^2} \\sum_{i = 1}^n (Y_i - \\bar Y)^2 \\\\\nT_{LR} &= 2\\Big\\{\\ell(\\hat \\mu_{\\text{MLE}}) - \\ell(\\mu_0)  \\Big\\} \\\\\n&= 2\\Big\\{-n/2\\ln(2\\pi\\sigma^2)  -\\frac{1}{2\\sigma^2} \\sum_{i = 1}^n (Y_i - \\bar Y)^2 - \\Big(-n/2\\ln(2\\pi\\sigma^2)  -\\frac{1}{2\\sigma^2} \\sum_{i = 1}^n (Y_i - \\mu_0)^2 \\Big)  \\Big\\} \\\\\n&= 2\\Big\\{\\frac{1}{2\\sigma^2} \\sum_{i = 1}^n (Y_i - \\mu_0)^2 -(Y_i - \\bar Y)^2 \\Big\\} \\\\ \n&=  \\frac{1}{\\sigma^2}\\sum_{i = 1}^n (Y_i - \\mu_0)^2 -(Y_i - \\bar Y)^2 \\\\\n&= \\frac{n(\\bar Y - \\mu_0)^2}{\\sigma^2} \\\\\nT_S &= \\frac{\\Big[S_n(\\mu_0)\\Big]^2}{nI_1(\\mu_0)}\\\\\n&=  \\Big(\\frac{1}{\\sigma^2}\\sum_{i = 1}^n Y_i - \\mu_0 \\Big)^2 \\frac{1}{nI_1(\\mu_0)} \\\\\n&=  \\Big(\\frac{1}{\\sigma^2}\\sum_{i = 1}^n Y_i - \\mu_0 \\Big)^2 \\frac{\\sigma^2}{n} \\\\\n&=  \\frac{\\sigma^2}{n\\sigma^4}\\Big(\\sum_{i = 1}^n Y_i - \\mu_0 \\Big)^2 \\\\\n&=  \\frac{1}{n\\sigma^2}\\Big(\\sum_{i = 1}^n Y_i - \\mu_0 \\Big)^2 \\\\\n&=  \\frac{n^2}{n\\sigma^2}\\Big(Y_i - \\mu_0 \\Big)^2 \\\\\n&= \\frac{n(\\bar Y - \\mu_0)^2}{\\sigma^2} \\\\\n\\end{split}\n$$\n\n\n\\newpage \n\nNow, say we want to test $H_0: \\mu \\le \\mu_0$ vs. $H_a: \\mu > \\mu_0$. We have obtained the T-statistic $$T_W = T_S = T_{LR} =  \\frac{n(\\bar Y - \\mu_0)^2}{\\sigma^2}$$\n\nWe reject $H_0$ if $T > \\chi^2_{1,\\alpha}$. For example, when $\\alpha = 0.05, \\chi^2_{1,0.05} \\approx 3.84$. This is equivalent to rejecting $H_0$ if $Z > z_{\\alpha}$, where $z_\\alpha$ satisfies $P(Z \\ge z_\\alpha) = \\alpha$ and $$Z = \\sqrt T = \\frac{\\bar Y - \\mu_0}{\\sigma/\\sqrt n}$$\n\n## A note on notation ($z_\\alpha$, $t_\\alpha$, $\\chi^2_\\alpha$)\n\nA note on notation: $z_{\\alpha} = \\Phi^{-1}(1-\\alpha) = Q(1 - \\alpha)$. For example, when $\\alpha = 0.05, z_{\\alpha} \\approx 1.64$. We have $P(Z \\le z_\\alpha) = 1- \\alpha$ and $P(Z > z_\\alpha) = \\alpha$.  \n\n- $z_\\alpha$ satisfies $P(Z > z_\\alpha) = \\alpha$ where $Z \\sim N(0,1)$\n- $t_{n-1,\\alpha}$ satisfies $P(T_{n-1} >t_{n-1,\\alpha})$ where $T_{n-1} \\sim t_{n=1}$\n- $\\chi^2_{r, \\alpha}$ satisfies $P(\\chi^2_r > \\chi^2_{r,\\alpha})$, where $\\chi^2_p$ is a chi-squared random variable with *p* degrees of freedom. \n\n\n\n\n\n\\newpage\n\n## Wald test: Simple null ($H_0: \\theta = \\theta_0$)\n\nLet $Y_i$ be iid with density $f(y | \\theta)$. Consider a simple null hypothesis $H_0: \\theta = \\theta_0$. Suppose that $\\hat \\theta_{\\text{MLE}}$ is a consistent root of the likelihood equation. Then,\n$$\n\\begin{split}\n\\sqrt{n}(\\hat \\theta_{\\text{MLE}} - \\theta) &\\xrightarrow{D} N\\Bigg(0, \\frac{1}{I_1(\\theta)}\\Bigg) \\\\\nZ_W =\\sqrt{nI_1(\\theta)}(\\hat \\theta_{\\text{MLE}} - \\theta) &\\xrightarrow{D} N(0, 1)\n\\end{split}\n$$\n\nIf $nI_1(\\hat \\theta_{\\text{MLE}})/nI_1(\\theta_0)$ converges in probability to 1 as $n \\rightarrow \\infty$, then $$T_W = Z^2_W =nI_1(\\theta)(\\hat \\theta_{\\text{MLE}} -\\theta)^2 \\xrightarrow{D} \\chi_1^2 \\text{ under } H_0,$$ and we reject the null hypothesis is $T_W > \\chi^2_{1,\\alpha}$, where $\\chi^2_{1,\\alpha} = Q(1 - \\alpha)$ and $Q$ is the quantile function associated with the $\\chi^2$ distribution. For example, when $\\alpha = 0.05, \\chi^2_{1,0.05} \\approx 3.84$..\n\n### Multi-parameter formulation: Simple null ($H_0: \\pmb \\theta = \\pmb \\theta_0$)\n\nThe more general vector-formulation of the Wald statistic (including a possible alternative formulation) is given by \n\n$$\n\\begin{split}\n&\\hat{\\pmb\\theta}_{\\text{MLE}} \\xrightarrow{D} N\\Big({\\pmb\\theta}_0, \\Big[\\pmb I_T(\\pmb \\theta_0) \\Big]^{-1}\\Big) \\\\\n&\\sqrt{\\pmb I_T(\\pmb \\theta_0)} (\\hat{\\pmb\\theta}_{\\text{MLE}} - \\pmb \\theta_0) \\xrightarrow{D} N\\Big({\\pmb\\theta}_0,  \\mathbb{1}_b \\Big) \\\\\n&(1) ~T_W = (\\hat{\\pmb\\theta}_{\\text{MLE}} - {\\pmb\\theta}_0)^T \\{\\pmb I_T(\\hat {\\pmb \\theta}_{\\text{MLE}})\\}(\\hat{\\pmb\\theta}_{\\text{MLE}} - {\\pmb\\theta}_0) \\xrightarrow{D} \\chi^2_b \\\\\n&\\text{Reject if }   T_W > \\chi^2_{b,\\alpha} \\\\\n&(2) ~T_W' = (\\hat{\\pmb\\theta}_{\\text{MLE}} - {\\pmb\\theta}_0)^T \\{\\pmb I_T({\\pmb \\theta}_0)\\}(\\hat{\\pmb\\theta}_{\\text{MLE}} - {\\pmb\\theta}_0) \\xrightarrow{D} \\chi^2_b \\\\\n&\\text{Reject if }   T'_W > \\chi^2_{b,\\alpha}\n\\end{split}\n$$\n\n\\newpage\n\n### $H_0: \\theta = \\theta_0$ vs. $H_a: \\theta \\ne \\theta_0$\n\nThere are two possible forms for the Wald test, depending on whether we plug $\\theta_0$ or $\\hat \\theta_{\\text{MLE}}$ into the Fisher information matrix. If $nI_1(\\hat \\theta_{\\text{MLE}})/nI_1(\\theta_0)$ converges in probability to 1 as $n \\rightarrow \\infty$, we have the following rejection rules at significance level $\\alpha$ for a two-sided test against $Ha: \\theta \\ne \\theta_0$:\n$$\n\\begin{split}\n&\\text{First approach (plug in } \\hat\\theta_{\\text{MLE}}): \\text{Under } H_0,\\\\\n&T_W = nI_1(\\hat \\theta_{\\text{MLE}})(\\hat \\theta_{\\text{MLE}} -\\theta_0)^2 \\xrightarrow{D} \\chi_1^2  \\\\\n&Z_W = \\sqrt{nI_1(\\hat \\theta_{\\text{MLE}})}(\\hat \\theta_{\\text{MLE}} - \\theta_0) \\xrightarrow{D} N(0, 1) \\\\\n&\\text{Reject } H_0 \\text{ if, equivalently, } \\\\\n&(1) ~Z_W > z_{\\alpha/2}, \\text{ or } Z_W < -z_{\\alpha/2}~; \\\\\n&(2)~\\hat \\theta_{\\text{MLE}} < \\theta_0 -\\frac{z_{\\alpha/2}}{\\sqrt{nI_1(\\hat \\theta_{\\text{MLE}})}}, \\text{ or }  \\hat \\theta_{\\text{MLE}} > \\theta_0 +\\frac{z_{\\alpha/2}}{\\sqrt{nI_1(\\hat \\theta_{\\text{MLE}})}} \\\\\\\\\n&\\text{Second approach (plug in } \\theta_0): \\text{Under } H_0,\\\\\n&T_W' = nI_1( \\theta_0)(\\hat \\theta_{\\text{MLE}} -\\theta_0)^2 \\xrightarrow{D} \\chi_1^2  \\\\\n&Z_W' = \\sqrt{nI_1( \\theta_0)}(\\hat \\theta_{\\text{MLE}} - \\theta_0) \\xrightarrow{D} N(0, 1) \\\\\n&\\text{Reject } H_0 \\text{ if, equivalently, } \\\\\n&(1) ~Z'_W > z_{\\alpha/2}, \\text{ or } Z'_W < -z_{\\alpha/2}~; \\\\\n&(2)~\\hat \\theta_{\\text{MLE}} < \\theta_0 -\\frac{z_{\\alpha/2}}{\\sqrt{nI_1( \\theta_0)}}, \\text{ or }  \\hat \\theta_{\\text{MLE}} > \\theta_0 +\\frac{z_{\\alpha/2}}{\\sqrt{nI_1( \\theta_0)}} \\\\\n\\end{split}\n$$\n\n### $H_0: \\theta = \\theta_0$ vs. $H_a: \\theta < \\theta_0$\n\nIf $nI_1(\\hat \\theta_{\\text{MLE}})/nI_1(\\theta_0)$ converges in probability to 1 as $n \\rightarrow \\infty$, we have, for a one-sided test against $H_a: \\theta < \\theta_0$,\n$$\n\\begin{split}\n&\\text{First approach (plug in } \\hat\\theta_{\\text{MLE}}): \\text{Under } H_0,\\\\\n&T_W = nI_1(\\hat \\theta_{\\text{MLE}})(\\hat \\theta_{\\text{MLE}} -\\theta_0)^2 \\xrightarrow{D} \\chi_1^2  \\\\\n&Z_W = \\sqrt{nI_1(\\hat \\theta_{\\text{MLE}})}(\\hat \\theta_{\\text{MLE}} - \\theta_0) \\xrightarrow{D} N(0, 1) \\\\\n&\\text{Reject } H_0 \\text{ if, equivalently, } \\\\\n&Z_W  = \\sqrt{nI_1(\\hat \\theta_{\\text{MLE}})}(\\hat \\theta_{\\text{MLE}} - \\theta_0) <- z_{\\alpha}, \\text{ or }\\\\\n&\\hat \\theta_{\\text{MLE}} < \\theta_0 -\\frac{z_{\\alpha}}{\\sqrt{nI_1(\\hat \\theta_{\\text{MLE}})}}\\\\\\\\\n&\\text{Second approach (plug in } \\theta_0): \\text{Under } H_0,\\\\\n&T_W' = nI_1( \\theta_0)(\\hat \\theta_{\\text{MLE}} -\\theta_0)^2 \\xrightarrow{D} \\chi_1^2  \\\\\n&Z_W' = \\sqrt{nI_1( \\theta_0)}(\\hat \\theta_{\\text{MLE}} - \\theta_0) \\xrightarrow{D} N(0, 1) \\\\\n&\\text{Reject } H_0 \\text{ if, equivalently, } \\\\\n&Z'_W  =  \\sqrt{nI_1( \\theta_0)}(\\hat \\theta_{\\text{MLE}} - \\theta_0) <- z_{\\alpha}, \\text{ or }\\\\\n&\\hat \\theta_{\\text{MLE}} < \\theta_0 -\\frac{z_{\\alpha}}{\\sqrt{nI_1( \\theta_0)}}\\\\\n\\end{split}\n$$\n\n### $H_0: \\theta = \\theta_0$ vs. $H_a: \\theta > \\theta_0$\n\nIf $nI_1(\\hat \\theta_{\\text{MLE}})/nI_1(\\theta_0)$ converges in probability to 1 as $n \\rightarrow \\infty$, we have, for a one-sided test against $H_a: \\theta > \\theta_0$,\n$$\n\\begin{split}\n&\\text{First approach (plug in } \\hat\\theta_{\\text{MLE}}): \\text{Under } H_0,\\\\\n&T_W = nI_1(\\hat \\theta_{\\text{MLE}})(\\hat \\theta_{\\text{MLE}} -\\theta_0)^2 \\xrightarrow{D} \\chi_1^2  \\\\\n&Z_W = \\sqrt{nI_1(\\hat \\theta_{\\text{MLE}})}(\\hat \\theta_{\\text{MLE}} - \\theta_0) \\xrightarrow{D} N(0, 1) \\\\\n&\\text{Reject } H_0 \\text{ if, equivalently, } \\\\\n&Z_W  = \\sqrt{nI_1(\\hat \\theta_{\\text{MLE}})}(\\hat \\theta_{\\text{MLE}} - \\theta_0) > z_{\\alpha}, \\text{ or }\\\\\n&\\hat \\theta_{\\text{MLE}} > \\theta_0 +\\frac{z_{\\alpha}}{\\sqrt{nI_1(\\hat \\theta_{\\text{MLE}})}}\\\\\\\\\n&\\text{Second approach (plug in } \\theta_0): \\text{Under } H_0,\\\\\n&T_W' = nI_1( \\theta_0)(\\hat \\theta_{\\text{MLE}} -\\theta_0)^2 \\xrightarrow{D} \\chi_1^2  \\\\\n&Z_W' = \\sqrt{nI_1( \\theta_0)}(\\hat \\theta_{\\text{MLE}} - \\theta_0) \\xrightarrow{D} N(0, 1) \\\\\n&\\text{Reject } H_0 \\text{ if, equivalently, } \\\\\n&Z'_W  =  \\sqrt{nI_1( \\theta_0)}(\\hat \\theta_{\\text{MLE}} - \\theta_0) > z_{\\alpha}, \\text{ or }\\\\\n&\\hat \\theta_{\\text{MLE}} > \\theta_0 +\\frac{z_{\\alpha}}{\\sqrt{nI_1( \\theta_0)}}\\\\\n\\end{split}\n$$\n\n\\newpage\n\n### Example: Bernoulli test of $H_0: p = p_0$ vs. $H_a: p > p_0$\n\nWe have iid $Y_i \\sim$ Bernoulli($p$). Consider a simple null hypothesis $H_0: p = p_0$ vs. $H_a: p > p_0$. Derive $\\ell(p), S_n(p), \\hat p_{\\text{MLE}}$, and $I_T(p)$. From this, derive two expressions for the Wald test.\n\n$$\n\\begin{split}\nE[Y_i] &= p \\\\\nL(p) &= \\prod_{i = 1}^n p^{Y_i} (1-p)^{1 - Y_i} \\\\\n\\ell(p) &= \\sum_{i = 1}^n Y_i\\ln p + (1-Y_i)\\ln(1-p) \\\\\nS_n(p) = \\frac{d\\ell(p)}{dp} &= \\sum_{i = 1}^n \\frac{Y_i}{p} - \\frac{1-Y_i}{1 - p} = 0 \\\\\n \\sum_{i = 1}^n \\frac{Y_i}{p} &= \\sum_{i = 1}^n \\frac{1-Y_i}{1 - p} \\\\\n\\frac{1}{p}\\sum_{i = 1}^n {Y_i} &= \\frac{1}{{1 - p}}\\sum_{i = 1}^n {1-Y_i} \\\\\n\\frac{1 - p}{p} &= \\frac{\\sum_{i = 1}^n {1-Y_i}}{\\sum_{i = 1}^n {Y_i}} \\\\\n\\frac{1}{p} - 1 &= \\frac{n}{\\sum_{i = 1}^n Y_i} - 1 \\\\\n\\frac{1}{p} &= \\frac{n}{\\sum_{i = 1}^n Y_i} \\\\\n\\hat p_{\\text{MLE}} &= \\frac{1}{n} \\sum_{i = 1}^n Y_i = \\bar Y \\\\\n\\end{split}\n$$\n\n$$\n\\begin{split}\nI_1(p) &= - E\\Big[\\frac{\\partial}{\\partial p} S_i(p) \\Big] \\\\\n&= - E\\Big[\\frac{\\partial}{\\partial p} {Y_i} \\cdot{p}^{-1} - (1-Y_i) \\cdot(1 - p)^{-1} \\Big]\\\\\n&= - E\\Big[ {Y_i} \\cdot-{p}^{-2} - (1-Y_i) \\cdot-(1 - p)^{-2} \\cdot -1 \\Big]\\\\\n&= - E\\Big[ -\\frac{Y_i}{{p}^{2}} - \\frac{1-Y_i}{(1 - p)^{2}}  \\Big]\\\\\n&= E\\Big[ \\frac{Y_i}{{p}^{2}} + \\frac{1-Y_i}{(1 - p)^{2}}  \\Big]\\\\\n&= \\frac{1}{p^2}E[ {Y_i}] + \\frac{1}{(1-p)^2} E[{1-Y_i}]  \\\\\n&= \\frac{p}{p^2} + \\frac{1-p}{(1-p)^2} \\\\\n&= \\frac{1}{p} + \\frac{1}{1-p} \\\\\n&= \\frac{1 -p}{p(1-p)} + \\frac{p}{p(1-p)} \\\\\n&= \\frac{1}{p(1 - p)} \\\\\nI_T(p) = nI_1(p) &= \\frac{n}{p(1 - p)} \\\\\\\\\n\\text{Under } H_0&, \\sqrt{nI_1(p)}(\\hat p_{\\text{MLE}} - p) \\xrightarrow{D} N(0, 1) \\\\\n(1) \\text{ Based on } Z_W &=  \\sqrt{nI_1(\\hat p_{\\text{MLE}})}(\\hat p_{\\text{MLE}} - p_0) \\xrightarrow{D} N(0, 1), \\\\\n\\text{Reject } H_0 \\text{ if } Z_W &= \\frac{\\sqrt n (\\hat p_{\\text{MLE}} - p_0)}{\\sqrt{\\hat p_{\\text{MLE}}(1 - \\hat p_{\\text{MLE}})}}  > z_{\\alpha} \\\\\n(2) \\text{ Based on } Z'_W &=  \\sqrt{nI_1( p_0)}(\\hat p_{\\text{MLE}} - p_0) \\xrightarrow{D} N(0, 1), \\\\\n\\text{Reject } H_0 \\text{ if } Z'_W &= \\frac{\\sqrt n (\\hat p_{\\text{MLE}} - p_0)}{\\sqrt{ p_0(1 -  p_0)}}  > z_{\\alpha} \\\\\n\\end{split}\n$$\n\n## Score (Lagrange Multiplier) test: Simple null ($H_0: \\theta = \\theta_0$)\n\n$$\n\\begin{split}\nT_S = \\frac{[S_n(\\theta_0)]^2}{nI_1(\\theta_0)}~ &\\xrightarrow{D} \\chi^2_1 \\\\\n\\text{Reject } H_0 \\text{ if } T_S &> \\chi^2_{1, \\alpha} \\\\\nZ_S = \\frac{S_n(\\theta_0)}{\\sqrt{nI_1(\\theta_0)}}~ &\\xrightarrow{D} N(0,1) \\\\\n\\text{Reject } H_0 \\text{ if } Z_S &> z_{\\alpha} \\\\\n\\end{split}\n$$\n\n### Multi-parameter formulation: Simple null ($H_0: \\pmb \\theta = \\pmb \\theta_0$)\n\nUnder $H_0$,\n\n$$\n\\begin{split}\nT_S = \\pmb S_n(\\pmb \\theta_0)^T [\\pmb I_T(\\pmb \\theta_0)]^{-1} \\pmb S_n(\\pmb \\theta_0) &\\xrightarrow {D} \\chi^2_b \\\\\n[\\pmb I_T(\\pmb \\theta_0)]^{-1/2} \\pmb S_n(\\pmb \\theta_0) &\\xrightarrow {D} N(\\pmb 0, \\mathbb{1}_b)\n\\end{split}\n$$\n\n\\newpage\n\n### Example: Bernoulli test of $H_0: p = p_0$ vs. $H_a: p > p_0$\n\nReturn to the example above. What is the Score test statistic?\n\n$$\n\\begin{split}\nT_S &= \\frac{[S_n(\\theta_0)]^2}{nI_1(\\theta_0)} \\\\\n&= \\Bigg(\\sum_{i = 1}^n \\frac{Y_i}{p_0} - \\frac{1-Y_i}{1 - p_0}\\Bigg)^2 \\Bigg(\\frac{p_0(1 - p_0)}{n} \\Bigg) \\\\\n&= \\Bigg(\\sum_{i = 1}^n \\frac{Y_i(1 - p_0)}{p_0(1 - p_0)} - \\frac{p_0(1-Y_i)}{p_0(1 - p_0)}\\Bigg)^2 \\Bigg(\\frac{p_0(1 - p_0)}{n} \\Bigg) \\\\\n&= \\Bigg(\\sum_{i = 1}^n \\frac{Y_i - Y_ip_0}{p_0(1 - p_0)} - \\frac{p_0 -Y_ip_0}{p_0(1 - p_0)}\\Bigg)^2 \\Bigg(\\frac{p_0(1 - p_0)}{n} \\Bigg) \\\\\n&= \\Bigg(\\sum_{i = 1}^n \\frac{Y_i - p_0}{p_0(1 - p_0)} \\Bigg)^2 \\Bigg(\\frac{p_0(1 - p_0)}{n} \\Bigg) \\\\\n&=  \\Bigg(\\frac{n\\bar Y - np_0}{p_0(1 - p_0)} \\Bigg)^2 \\Bigg(\\frac{p_0(1 - p_0)}{n} \\Bigg) \\\\\n&= \\Bigg(n \\frac{(\\bar Y - p_0)}{p_0(1 - p_0)} \\Bigg)^2 \\Bigg(\\frac{p_0(1 - p_0)}{n} \\Bigg) \\\\\n&= \\frac{n^2}{n} \\frac{(\\bar Y - p_0)^2}{p_0^2(1 - p_0)^2} \\Big(p_0(1 - p_0)\\Big) \\\\\n&= \\frac{n(\\bar Y - p_0)^2}{p_0(1 - p_0)} \\\\\nZ_S &= \\sqrt{T_S} \\\\ &=   \\frac{\\sqrt n(\\bar Y - p_0)}{\\sqrt{p_0(1 - p_0)}} \\\\\n&= Z'_W\n\\end{split}\n$$\n\n\\newpage\n\n## Likelihood ratio test: Simple null ($H_0: \\theta = \\theta_0$)\n\n$$\n\\begin{split}\nT_{LR} = 2\\Big\\{ \\ell(\\hat \\theta_{MLE}) - \\ell(\\theta_0) \\Big\\} &\\xrightarrow{D} \\chi^2_1 \\text{ under } H_0, \\text{ as } n \\rightarrow \\infty \\\\\n\\text{Reject } H_0 \\text{ if } T_{LR} &> \\chi^2_{1, \\alpha}\n\\end{split}\n$$\n\nWe have the following equivalent definitions:\n$$\n\\begin{split}\nT_{LR} &= -2\\Big\\{\\ell(\\theta_0) - \\ell(\\hat \\theta_{\\text{MLE}})\\Big\\} \\\\\n&= 2\\Big\\{\\ell(\\hat \\theta_{\\text{MLE}}) -\\ell(\\theta_0) \\Big\\} \\\\\n&= -2\\ln\\Bigg\\{\\frac{L(\\theta_0)}{L(\\hat \\theta_{\\text{MLE}})}\\Bigg\\} \\\\\n&= 2\\ln\\Bigg\\{\\frac{L(\\hat \\theta_{\\text{MLE}})}{L(\\theta_0)}\\Bigg\\} \n\\end{split}\n$$\n\n\n### Multi-parameter formulation: Simple null ($H_0: \\pmb \\theta = \\pmb \\theta_0$)\n\n$$T_{LR} = 2\\Big\\{ \\ell(\\hat {\\pmb \\theta}_{MLE}) - \\ell(\\pmb\\theta_0) \\Big\\} \\xrightarrow{D} \\chi^2_b \\text{ under } H_0, \\text{ as } n \\rightarrow \\infty$$\n\nThe distribution of $T_{LR}$ converges to a $\\chi^2_r$ distribution as $n \\rightarrow \\infty$, where the degrees of freedom $r$, are given by the difference between the number of free parameters specified by $\\pmb \\theta \\in \\Theta_0$ (the $H_0$-restricted parameter space) and the number of free parameters specified by $\\pmb \\theta \\in \\Theta$ (the entire parameter space).\n\nWe reject $H_0$ iff $T_{LR} > \\chi^2_{r, \\alpha}$.\n\n\\newpage\n\n### Example: Bernoulli test of $H_0: p = p_0$ vs. $H_a: p > p_0$\n\nReturn to the example above. What is the Likelihood ratio test statistic?\n\n$$\n\\begin{split}\n\\ell(p) &= \\sum_{i = 1}^n Y_i \\ln p + (1 - Y_i)\\ln(1-p) \\\\\nT_{LR} &= -2\\Big\\{\\ell(p_0) - \\ell(\\hat p_{\\text{MLE}})\\Big\\} \\\\\n&= -2\\Big\\{ \\sum_{i = 1}^n Y_i \\ln p_0 + (1 - Y_i)\\ln(1-p_0)  - \\Big [ \\sum_{i = 1}^n Y_i \\ln \\hat p_{\\text{MLE}} + (1 - Y_i)\\ln(1-\\hat p_{\\text{MLE}})  \\Big]\\Big\\} \\\\\n&= -2 \\Big\\{\\ln\\Big( \\frac{p_0}{\\hat p_{\\text{MLE}}} \\Big)  \\sum_{i = 1}^n Y_i + \\ln\\Big( \\frac{1 - p_0}{1 - \\hat p_{\\text{MLE}}} \\Big) \\sum_{i = 1}^n 1 - Y_i \\Big\\} \\\\\n\\text{Reject } &H_0 \\text{ if } T_{LR} > \\chi^2_{1, \\alpha}\n\\end{split}\n$$\n\n\\newpage\n\n\n\n### Example: Poisson distribution\n\nLet $X_1, ..., X_n$ be independent random samples from a Poisson distribution with parameter $\\lambda$ and define the sum $Y = \\sum_{i = 1}^n X_i$. Construct a confidence interval for $\\lambda$ by inverting a LR test statistic testing $H_0: \\lambda = \\lambda_0$ vs. $H_a: \\lambda \\ne \\lambda_0$. \n\n**Approach 1: Using** $T_{LR} = -2\\Big\\{\\ell(\\lambda_0) - \\ell(\\hat \\lambda_{\\text{MLE}})\\Big\\}$\n$$\n\\begin{split}\nL(\\lambda | \\pmb x) &= \\frac{e^{-n\\lambda}\\lambda^{\\sum_ix_i}}{\\prod_i x_i!}, \\text{with } \\hat \\lambda_{\\text{MLE}} = \\bar x \\\\\n\\ell(\\lambda | \\pmb x) &= -n\\lambda + \\sum_i x_i\\ln(\\lambda) - \\sum_i \\ln(x_i!)\\\\\n\\ell(\\lambda_0 | \\pmb x) &= -n\\lambda_0 + \\sum_i x_i\\ln(\\lambda_0) - \\sum_i \\ln(x_i!) \\\\\n\\ell(\\lambda_{\\text{MLE}} | \\pmb x) &=-n\\bar x + \\sum_i x_i\\ln(\\bar x) - \\sum_i \\ln(x_i!) \\\\\nT_{LR} &= -2\\Big\\{\\ell(\\lambda_0) - \\ell(\\hat \\lambda_{\\text{MLE}})\\Big\\} \\\\\n&= -2\\Big\\{-n\\lambda_0 + \\sum_i x_i\\ln(\\lambda_0) - \\sum_i \\ln(x_i!)  - \\Big[ -n\\bar x + \\sum_i x_i\\ln(\\bar x) - \\sum_i \\ln(x_i!)\\Big] \\Big\\} \\\\\n&= -2\\Big\\{-n\\lambda_0 + \\sum_i x_i\\ln(\\lambda_0) - \\sum_i \\ln(x_i!) + n\\bar x -\\sum_i x_i\\ln(\\bar x) + \\sum_i \\ln(x_i!)\\Big\\} \\\\\n&= -2\\Big\\{-n\\lambda_0 + \\sum_i x_i\\ln(\\lambda_0)  + n\\bar x -\\sum_i x_i\\ln(\\bar x)\\Big\\} \\\\\n\\text{Using } \\sum_i x_i &= n\\bar x,  \\\\\nT_{LR}&= -2\\Big\\{n(\\bar x -\\lambda_0) + n\\bar x\\Big[\\ln(\\lambda_0) - \\ln(\\bar x)\\Big]\\Big\\} \\\\\n&=2\\Big\\{n(\\lambda_0 - \\bar x) + n\\bar x\\Big[ \\ln(\\bar x) - \\ln(\\lambda_0) \\Big]\\Big\\} \\\\\n&=2n\\Big\\{\\lambda_0 - \\bar x + \\bar x\\ln\\Big[\\frac{\\bar x}{\\lambda_0}\\Big] \\Big\\} \\\\\n\\text{By }& \\text{Wilk's theorem}, \\\\\n&2n\\Big\\{\\lambda_0 - \\bar X + \\bar X\\ln\\Big[\\frac{\\bar X}{\\lambda_0}\\Big] \\Big\\} \\sim \\chi_1^2\n\\end{split}\n$$\n\n**Approach 2: Using** $T_{LR} = -2\\ln\\Big\\{\\frac{L(\\lambda_0)}{L(\\hat \\lambda_{\\text{MLE}})}\\Big\\}$\n$$\n\\begin{split}\nL(\\lambda | \\pmb x) &= \\frac{e^{-n\\lambda}\\lambda^{\\sum_ix_i}}{\\prod_i x_i!}, \\text{with } \\hat \\lambda_{\\text{MLE}} = \\bar x \\\\\nL(\\lambda_0) &=\\frac{e^{-n\\lambda_0}\\lambda_0^{\\sum_ix_i}}{\\prod_i x_i!} \\\\\nL(\\hat \\lambda_{\\text{MLE}}) &= \\frac{e^{-n\\bar x}\\bar x^{\\sum_ix_i}}{\\prod_i x_i!} \\\\\nT_{LR} &= -2\\ln\\Big\\{\\frac{L(\\lambda_0)}{L(\\hat \\lambda_{\\text{MLE}})}\\Big\\} \\\\\n&= -2\\ln\\Big\\{ \\frac{e^{-n\\lambda_0}\\lambda_0^{\\sum_ix_i}}{\\prod_i x_i!} \\cdot \\frac{\\prod_i x_i!}{e^{-n\\bar x}\\bar x^{\\sum_ix_i}}\\Big\\} \\\\\n&=-2\\ln\\Big\\{ \\frac{e^{-n\\lambda_0}\\lambda_0^{\\sum_ix_i}}{e^{-n\\bar x}\\bar x^{\\sum_ix_i}}\\Big\\} \\\\\n&=-2\\ln\\Big\\{e^{-n\\lambda_0 + n\\bar x} \\Big(\\frac{\\lambda_0}{\\bar x}\\Big)^{\\sum_i x_i}\\Big\\} \\\\\n&= -2\\Big\\{-n\\lambda_0 + n\\bar x + \\sum_i x_i\\ln\\Big[\\frac{\\lambda_0}{\\bar x}\\Big]\\Big\\} \\\\\n&=-2n\\Big\\{-\\lambda_0 + \\bar x + \\bar x\\ln\\Big[\\frac{\\lambda_0}{\\bar x}\\Big]\\Big\\} \\\\\n&=2n\\Big\\{\\lambda_0 - \\bar x + \\bar x\\ln\\Big[\\frac{\\bar x}{\\lambda_0}\\Big]\\Big\\} \\\\\n\\text{By }& \\text{Wilk's theorem}, \\\\\n&2n\\Big\\{\\lambda_0 - \\bar X + \\bar X\\ln\\Big[\\frac{\\bar X}{\\lambda_0}\\Big] \\Big\\} \\sim \\chi_1^2\n\\end{split}\n$$\n\n\n\\newpage\n\n## Composite null hypotheses ($H_0: \\pmb \\theta_1 = \\pmb \\theta_{10}$)\n\nEarlier, we defined the test statistics when the null hypothesis is of the form $H_0: \\pmb \\theta = \\pmb\\theta_0$. Note that this restricts the entire parameter vector $\\pmb \\theta$ (for example, for a normal distribution, we would restrict both $\\mu$ and $\\sigma^2$ to $\\pmb\\theta_0$ under the null). However, we are often interested only in certain components of $\\pmb \\theta$. For example, we wish to test $\\theta_1 =\\mu$, while leaving $\\theta_2 =\\sigma^2$ unrestricted.\n\nIn other words, under a simple null hypothesis, all parameters in $\\theta$ are specified, i.e. assumed to be known. Under a composite null hypothesis, we can account for parameters that are unknown and leave them unrestricted.\n\n### Partitioning the information matrix\n\n$$\\hat{\\pmb I} = \\pmb I_T(\\hat{\\pmb \\theta}_{\\text{MLE}}) = n \\pmb I_1(\\hat{\\pmb \\theta}_{\\text{MLE}})$$\n\n$$\n\\hat{\\pmb I} = \n\\begin{bmatrix}\n\\hat{\\pmb I}_{11} & \\hat{\\pmb I}_{12} \\\\\n\\hat{\\pmb I}_{21} & \\hat{\\pmb I}_{22} \\\\\n\\end{bmatrix}\n$$\n\n\n### Wald test: Composite null ($H_0: \\pmb \\theta_1 = \\pmb \\theta_{10}$) with unknown parameter(s)\n\nConsider testing $H_0: \\pmb \\theta_1 = \\pmb \\theta_{10}$ versus $H_a: \\pmb \\theta_1 \\ne \\pmb \\theta_{10}$.\n\nRecall that, in the simple null hypothesis ($H_0: \\pmb \\theta = \\pmb \\theta_0$) case, \n$$\nT_W = (\\hat{\\pmb\\theta}_{\\text{MLE}} - {\\pmb\\theta}_0)^T \\{\\pmb I_T(\\hat {\\pmb \\theta}_{\\text{MLE}})\\}(\\hat{\\pmb\\theta}_{\\text{MLE}} - {\\pmb\\theta}_0) \\xrightarrow{D} \\chi^2_b \n$$\n\nInstead, when we have a composite null hypothesis of the form $H_0: \\pmb \\theta_1 = \\pmb \\theta_{10}$ (and thus, we leave at least one parameter $\\in \\pmb \\theta$ unknown under the null), we replace $\\{\\pmb I_T(\\hat {\\pmb \\theta}_{\\text{MLE}})\\}$ with \n$$\n\\begin{split}\n\\Big(\\hat{\\pmb I}_{11}- \\hat{\\pmb I}_{12}\\Big[\\hat{\\pmb I}_{22}\\Big]^{-1}\\hat{\\pmb I}_{21} \\Big)  &= \\Bigg( \\hat{\\pmb I}_{11}- \\frac{\\hat{\\pmb I}_{12}\\hat{\\pmb I}_{21}}{\\hat{\\pmb I}_{22}} \\Bigg) \\\\\n\\text{If } \\hat{\\pmb I}_{12} &\\equiv \\hat{\\pmb I}_{21},\\\\\n\\Big(\\hat{\\pmb I}_{11}- \\hat{\\pmb I}_{12}(\\hat{\\pmb I}_{22})^{-1}\\hat{\\pmb I}_{21} \\Big)  &= \\Bigg( \\hat{\\pmb I}_{11}- \\frac{(\\hat{\\pmb I}_{12})^2}{\\hat{\\pmb I}_{22}} \\Bigg)\n\\end{split}\n$$\n\nNotice the similarity with the Fisher information matrix we formulated earlier, when the other parameter is unknown. In particular, when the other parameter $\\eta$ is unknown, we have the Fisher information\n\n$$I^*_{\\theta\\theta} = I_{\\theta\\theta} - \\Big[ I_{\\theta\\eta} \\cdot I^{-1}_{\\eta\\eta} \\cdot I_{\\eta\\theta} \\Big] = I_{\\theta\\theta} - \\dfrac{I_{\\theta\\eta}^2}{I_{\\eta\\eta}}$$\n\nPutting this all together, we obtain the Wald test statistic for testing the composite null hypothesis $H_0: \\pmb \\theta_1 = \\pmb \\theta_{10}$ versus $H_a: \\pmb \\theta_1 \\ne \\pmb \\theta_{10}$:\n\n$$\n\\begin{split}\n&\\text{Under } H_0, \\text{ when } n \\rightarrow \\infty,  \\\\\nT_W &= (\\hat{\\pmb\\theta}_1 - {\\pmb\\theta}_{10})^T \\Big(\\hat{\\pmb I}_{11}- \\hat{\\pmb I}_{12}\\Big[\\hat{\\pmb I}_{22}\\Big]^{-1}\\hat{\\pmb I}_{21} \\Big)(\\hat{\\pmb\\theta}_1 - {\\pmb\\theta}_{10}) \\\\\n&= (\\hat{\\pmb\\theta}_1 - {\\pmb\\theta}_{10})^T \\Bigg( \\hat{\\pmb I}_{11}- \\frac{(\\hat{\\pmb I}_{12})^2}{\\hat{\\pmb I}_{22}} \\Bigg)(\\hat{\\pmb\\theta}_1 - {\\pmb\\theta}_{10}) \\xrightarrow{D} \\chi^2_r \n\\end{split}\n$$\n\n### Score test: Composite null ($H_0: \\pmb \\theta_1 = \\pmb \\theta_{10}$) with unknown parameter(s)\n\nRecall that, in the simple null hypothesis ($H_0: \\pmb \\theta = \\pmb \\theta_0$) case, \n$$\nT_S = \\pmb S_n(\\pmb \\theta_0)^T [\\pmb I_T(\\pmb \\theta_0)]^{-1} \\pmb S_n(\\pmb \\theta_0) \\xrightarrow {D} \\chi^2_b \\\\ \n$$\n\nThe Score statistic $T_S$ for the composite null hypothesis requires calculating the MLE of $\\pmb \\theta$ under the restriction imposed by $H_0$. We denote this restricted MLE by $\\tilde{\\pmb \\theta}$, and obtain the total Fisher information matrix evaluated at the restricted MLE: $\\tilde{\\pmb I} = \\pmb I_T(\\tilde{\\pmb \\theta})$. \n\nPutting this all together, we obtain the Score test statistic for testing the composite null hypothesis $H_0: \\pmb \\theta_1 = \\pmb \\theta_{10}$ versus $H_a: \\pmb \\theta_1 \\ne \\pmb \\theta_{10}$:\n\n$$\n\\begin{split}\n&\\text{Under } H_0, \\text{ when } n \\rightarrow \\infty,  \\\\\nT_S &= \\pmb S_{1n}(\\pmb \\theta_{10})^T \\Big(\\tilde{\\pmb I}_{11}- \\tilde{\\pmb I}_{12}\\Big[\\tilde{\\pmb I}_{22}\\Big]^{-1}\\tilde{\\pmb I}_{21} \\Big)^{-1} \\pmb S_{1n}(\\pmb \\theta_{10}) \\\\\n&= \\pmb S_{1n}(\\pmb \\theta_{10})^T \\Bigg( \\tilde{\\pmb I}_{11}- \\frac{(\\tilde{\\pmb I}_{12})^2}{\\tilde{\\pmb I}_{22}} \\Bigg)^{-1} \\pmb S_{1n}(\\pmb \\theta_{10}) \n\\xrightarrow {D} \\chi^2_r \\\\ \n\\end{split}\n$$\n\n\n### Likelihood ratio test: Composite null ($H_0: \\pmb \\theta_1 = \\pmb \\theta_{10}$) with unknown parameter(s)\n\nWe can measure the relative plausibility of $H_1$ to $H_0$ by the log likelihood ratio. To generalize the case of simple hypotheses, assume that $H_0$ specifies $\\theta \\in \\Theta_0$ and $H_1$ specifies $\\theta \\in \\Theta_1$. Let $\\Theta = \\Theta_0 \\cup \\Theta_1$. We have the simple ($\\Lambda$) and the generalized ($\\Lambda^*$) log-likelihood ratio:\n\n$$\n\\begin{split}\n\\text{In the simple case, } \\Lambda &=\n\\ln\\frac{f(X_1, ..., X_n | H_1)}{f(X_1, ..., X_n | H_0)} \\\\\n\\text{In the general case, } \\Lambda^*&=\n\\ln\\frac{\\sup_{\\theta \\in \\Theta_1} f(X_1, ..., X_n | \\pmb\\theta)}{\\sup_{\\theta \\in \\Theta_0}f(X_1, ..., X_n | \\pmb\\theta)} \\\\  \n&= \\ln\\frac{\\sup_{\\theta \\in \\Theta_1} L(\\pmb \\theta | \\pmb X)}{\\sup_{\\theta \\in \\Theta_0} L(\\pmb \\theta | \\pmb X)} \\\\  \n\\end{split}\n$$\n\nWe can again use the likelihood ratio statistic  \n$$\n\\begin{split}\nT_{LR} &=  2\\ln\\frac{\\sup_{\\theta \\in \\Theta} f(X_1, ..., X_n | \\pmb \\theta)}{\\sup_{\\theta \\in \\Theta_0}f(X_1, ..., X_n | \\pmb \\theta)}  \\\\\n&= 2\\ln\\frac{\\sup_{\\theta \\in \\Theta} L(\\pmb\\theta | \\pmb X)}{\\sup_{\\theta \\in \\Theta_0}L(\\pmb\\theta | \\pmb X)} \\\\\n&= -2\\ln \\frac{\\sup_{\\theta \\in \\Theta_0}L(\\pmb\\theta | \\pmb X)}{\\sup_{\\theta \\in \\Theta} L(\\pmb\\theta | \\pmb X)}   \\\\\n&= -2\\Big\\{ \\ell(\\tilde{\\pmb \\theta}) - \\ell(\\hat{\\pmb \\theta}) \\Big\\},\n\\end{split}\n$$\n\nwhere $\\tilde{\\pmb \\theta}$ is the restricted MLE under $H_0$ and $\\hat{\\pmb \\theta}$ is the unrestricted MLE.  \n\nLarge values of $T_{LR}$ provide stronger evidence against $H_0$. According to **Wilks Theorem**, when the joint distribution of $X_1, ..., X_n$ depends on $p$ unknown parameters and $p_0$ unknown parameters under $H_0$, under regularity conditions and assuming $H_0$ is true, the distribution of $T_{LR}$ tends to a $\\chi^2$ distribution with degrees of freedom $r = p - p_0$ as $n \\rightarrow \\infty$. For *n* large, we compare the value of $T_{LR}$ to the expected values from a $\\chi^2_{r}$. \n$$\n\\begin{split}\n\\text{Under } H_0, \\text{ as } n \\rightarrow \\infty, \\\\\nT_{LR} = -2\\Big\\{ \\ell(\\tilde{\\pmb \\theta}) - \\ell(\\hat{\\pmb \\theta}) \\Big\\} &\\xrightarrow{D} \\chi^2_{r}\n\\end{split}\n$$\nThe critical region for a test with approximate significance level $\\alpha$ is given by \n$$\nR = \\{\\pmb X: T_{LR} \\ge \\chi^2_{r,\\alpha}\\} \n$$\n\nIn a simple case when we are testing $H_0: \\theta = 0$ against $H_a: \\theta \\ne 0$, we have $p = 1$, $p_0 = 0$, and $\\nu = p - p_0 = 1$. In this case, $\\sup_{\\theta \\in \\Omega} L(\\theta | \\pmb X)$ simplifies to $L(\\hat\\theta_{\\text{MLE}})$; $\\sup_{\\theta \\in \\Theta_0}L(\\theta | \\pmb X)$ simplifies to $L(\\theta_0)$.\n\n## Example: Normality with unknown variance (t-test)\n\nWe have iid $Y_i \\sim N(\\mu, \\sigma^2)$ with $\\sigma^2$ unknown. We want to test $H_0: \\mu = \\mu_0$ vs. $H_a: \\mu \\ne \\mu_0$. Notice that this is a composite null: we leave $\\sigma^2$ unspecified under the null. \n\nFor the Wald statistic, recall that \n$$\nT_W = (\\hat{\\pmb\\theta}_1 - {\\pmb\\theta}_{10})^T \\Bigg( \\hat{\\pmb I}_{11}- \\frac{(\\hat{\\pmb I}_{12})^2}{\\hat{\\pmb I}_{22}} \\Bigg)(\\hat{\\pmb\\theta}_1 - {\\pmb\\theta}_{10}) \\xrightarrow{D} \\chi^2_r\n$$\n\nWe have \n$$\n\\begin{split}\n{\\pmb\\theta}_{10} &= \\mu_{0} \\\\\n\\hat{\\pmb\\theta}_1 &= \\hat \\mu = \\bar Y \\\\\n\\text{In the simple case, we } &\\text{have the total Fisher information matrix  } \\\\\n\\text{} I_T(\\theta) &=\n\\begin{bmatrix}\n\\displaystyle \\frac{n}{\\sigma^2} & 0 \\\\\n 0 & \\displaystyle \\frac{2n}{\\sigma^2}\\\\\n\\end{bmatrix} \\\\\n\\text{In the composite case, } &\\text{we have } \\\\\n\\pmb I_T(\\hat  {\\pmb \\theta}_{\\text{MLE}}) &=\n\\begin{bmatrix}\n\\displaystyle \\frac{n}{s^2} & 0 \\\\\n 0 & \\displaystyle \\frac{2n}{s^2}\\\\\n\\end{bmatrix}, \\text{ where }  \\\\\ns^2 = \\hat \\sigma^2 &= \\frac{1}{n} \\sum_{i = 1}^n (Y_i - \\bar Y)^2. \\text{ Thus, }\\\\\n\\Bigg( \\hat{\\pmb I}_{11}- \\frac{(\\hat{\\pmb I}_{12})^2}{\\hat{\\pmb I}_{22}} \\Bigg) &= \\frac{n}{s^2} \\\\\nT_W &= (\\hat{\\pmb\\theta}_1 - {\\pmb\\theta}_{10})^T \\Bigg( \\hat{\\pmb I}_{11}- \\frac{(\\hat{\\pmb I}_{12})^2}{\\hat{\\pmb I}_{22}} \\Bigg)(\\hat{\\pmb\\theta}_1 - {\\pmb\\theta}_{10}) \\\\\n&= (\\hat \\mu - \\mu_0)^T(\\frac{n}{s^2})(\\hat \\mu - \\mu_0) \\\\\n&= (\\bar Y - \\mu_0)^T(\\frac{n}{s^2})(\\bar Y - \\mu_0) \\\\\n&= \\frac{n(\\bar Y - \\mu_0)^2}{s^2}\n\\end{split}\n$$\n\nFor the Score statistic, recall that \n$$\nT_S =\\pmb S_{1n}(\\pmb \\theta_{10})^T \\Bigg( \\tilde{\\pmb I}_{11}- \\frac{(\\tilde{\\pmb I}_{12})^2}{\\tilde{\\pmb I}_{22}} \\Bigg)^{-1} \\pmb S_{1n}(\\pmb \\theta_{10})\n$$\nWe know\n$$\n\\begin{split}\n\\pmb I_T(  {\\tilde{\\pmb \\theta}}) &=\n\\begin{bmatrix}\n\\displaystyle \\frac{n}{\\tilde \\sigma^2} & 0 \\\\\n 0 & \\displaystyle \\frac{2n}{\\tilde \\sigma^2}\\\\\n\\end{bmatrix} \\\\\n\\text{Recall that } S_{\\mu n}(\\mu,\\sigma^2) &= \\frac{1}{\\sigma^2} \\sum_{i = 1}^n (Y_i - \\mu) \\\\\n\\text{ and }S_{\\sigma^2 n}(\\mu,\\sigma^2) &= \\frac{-n}{2\\sigma^2} + \\frac{1}{2\\sigma^4} \\sum_{i = 1}^n (Y_i - \\mu)^2 \\\\\n\\pmb S_{1n}(\\pmb \\theta_{10}) &= S_{\\mu n}(\\mu_0,\\tilde{\\sigma}^2) \\\\\n&=\\frac{1}{\\tilde\\sigma^2} \\sum_{i = 1}^n (Y_i - \\mu_0) \\\\\n\\end{split}\n$$\n\nFirst, we fix $\\mu$ to $\\mu_0$ in order to solve $S_{\\sigma^2 n}(\\mu_0,\\sigma^2) = 0$ and obtain $\\tilde\\sigma^2$:\n$$\n\\begin{split}\nS_{\\sigma^2 n}(\\mu_0,\\sigma^2) &= \\frac{-n}{2\\sigma^2} + \\frac{1}{2\\sigma^4} \\sum_{i = 1}^n (Y_i - \\mu_0)^2 = 0 \\\\\n&\\Rightarrow  \\frac{1}{2\\sigma^4} \\sum_{i = 1}^n (Y_i - \\mu_0)^2 = \\frac{n}{2\\sigma^2} \\\\\n&\\Rightarrow \\frac{2\\sigma^2}{2\\sigma^4} = \\frac{n}{ \\sum_{i = 1}^n (Y_i - \\mu_0)^2} \\\\\n\\tilde{\\sigma}^2 &= \\frac{1}{n}\\sum_{i = 1}^n (Y_i - \\mu_0)^2 \\\\\n\\text{Using the identity } \\sum_{i = 1}^n (Y_i - \\mu_0)^2 &= \\sum_{i = 1}^n \\Big [(Y_i - \\bar Y)^2 + (\\bar Y - \\mu_0)^2 \\Big] \\\\\n&= \\sum_{i = 1}^n (Y_i - \\bar Y)^2 +  \\sum_{i = 1}^n (\\bar Y - \\mu_0)^2 \\\\\n&= n(\\bar Y - \\mu_0)^2  + \\sum_{i = 1}^n (Y_i - \\bar Y)^2, \\text{ we have} \\\\    \n\\tilde{\\sigma}^2 &= \\frac{1}{n}\\sum_{i = 1}^n (Y_i - \\mu_0)^2 \\\\\n&= \\frac{1}{n} \\Big[ n(\\bar Y - \\mu_0)^2  + \\sum_{i = 1}^n (Y_i - \\bar Y)^2\\Big] \\\\\n&= s^2 + (\\bar Y - \\mu_0)^2\n\\end{split}\n$$\n\n\\newpage\n\nNow,\n\n$$\n\\begin{split}\n\\pmb I_T(  {\\tilde{\\pmb \\theta}}) &=\n\\begin{bmatrix}\n\\displaystyle \\frac{n}{\\tilde \\sigma^2} & 0 \\\\\n 0 & \\displaystyle \\frac{2n}{\\tilde \\sigma^2}\\\\\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n\\displaystyle \\frac{n}{s^2 + (\\bar Y - \\mu_0)^2} & 0 \\\\\n 0 & \\displaystyle \\frac{2n}{s^2 + (\\bar Y - \\mu_0)^2}\\\\\n\\end{bmatrix} \\\\\n\\Bigg( \\tilde{\\pmb I}_{11}- \\frac{(\\tilde{\\pmb I}_{12})^2}{\\tilde{\\pmb I}_{22}} \\Bigg)^{-1} &= \\Bigg(\\frac{n}{s^2 + (\\bar Y - \\mu_0)^2}\\Bigg)^{-1} \\\\\n&= \\frac{s^2 + (\\bar Y - \\mu_0)^2}{n}\n\\\\\n\\pmb S_{1n}(\\pmb \\theta_{10}) &= S_{\\mu n}(\\mu_0,\\tilde{\\sigma}^2) \\\\\n&=\\frac{1}{\\tilde\\sigma^2} \\sum_{i = 1}^n (Y_i - \\mu_0) \\\\\n&= \\frac{1}{s^2 + (\\bar Y - \\mu_0)^2}  \\sum_{i = 1}^n (Y_i - \\mu_0) \\\\\n&= \\frac{1}{s^2 + (\\bar Y - \\mu_0)^2} \\sum_{i = 1}^n Y_i - \\sum_{i = 1}^n \\mu_0 \\\\\n&= \\frac{n\\bar Y - n\\mu_0}{s^2 + (\\bar Y - \\mu_0)^2} \\\\\n&= \\frac{n(\\bar Y - \\mu_0)}{s^2 + (\\bar Y - \\mu_0)^2} \\\\\nT_S &=\\pmb S_{1n}(\\pmb \\theta_{10})^T \\Bigg( \\tilde{\\pmb I}_{11}- \\frac{(\\tilde{\\pmb I}_{12})^2}{\\tilde{\\pmb I}_{22}} \\Bigg)^{-1} \\pmb S_{1n}(\\pmb \\theta_{10}) \\\\\n&=  \\frac{n(\\bar Y - \\mu_0)}{s^2 + (\\bar Y - \\mu_0)^2} \\Bigg(\\frac{s^2 + (\\bar Y - \\mu_0)^2}{n} \\Bigg) \\frac{n(\\bar Y - \\mu_0)}{s^2 + (\\bar Y - \\mu_0)^2} \\\\\n&= \\frac{n\\Big(s^2 + (\\bar Y - \\mu_0)^2 \\Big) n(\\bar Y - \\mu_0)^2}{n\\Big(s^2 + (\\bar Y - \\mu_0)^2 \\Big)s^2 + (\\bar Y - \\mu_0)^2}  \\\\\n&= \\frac{n(\\bar Y - \\mu_0)^2}{s^2 + (\\bar Y - \\mu_0)^2}\n\\end{split}\n$$\n\\newpage\n\nFor the Likelihood Ratio test statistic, recall that\n$$\nT_{LR} = -2\\Big\\{ \\ell(\\tilde{\\pmb \\theta}) - \\ell(\\hat{\\pmb \\theta}) \\Big\\}\n$$\n\nWe have\n$$\n\\begin{split}\n\\ell(\\mu, \\sigma^2) &= -n/2\\ln(2\\pi) - n/2\\ln[\\sigma^2] - \\frac{1}{2\\sigma^2} \\sum_{i = 1}^n (Y_i - \\mu)^2 \\\\\n\\ell(\\tilde{\\pmb \\theta}) &= \\ell(\\mu_0, \\tilde \\sigma^2) \\\\ \n&=  -n/2\\ln(2\\pi) - n/2\\ln[\\tilde\\sigma^2] - \\frac{1}{2\\tilde\\sigma^2} \\sum_{i = 1}^n (Y_i - \\mu_0)^2 \\\\\n\\text{Recall that } \\tilde \\sigma^2 &=  \\frac{1}{n}\\sum_{i = 1}^n (Y_i - \\mu_0)^2 \\\\\n\\text{Thus, } -\\frac{1}{2\\tilde\\sigma^2} &= -\\frac{n}{2\\sum_{i = 1}^n (Y_i - \\mu_0)^2} \\text{ and } \\\\\n\\ell(\\tilde{\\pmb \\theta}) &= -n/2\\ln(2\\pi) - n/2\\ln[\\tilde\\sigma^2] -\\frac{n\\sum_{i = 1}^n (Y_i - \\mu_0)^2}{2\\sum_{i = 1}^n (Y_i - \\mu_0)^2}  \\\\\n&=-n/2\\ln(2\\pi) - n/2\\ln[\\tilde\\sigma^2] -{n}/{2}\\\\\n\\ell(\\hat{\\pmb \\theta}) &= -n/2\\ln(2\\pi) - n/2\\ln[s^2] - \\frac{1}{2s^2} \\sum_{i = 1}^n (Y_i - \\bar Y)^2  \\\\\n&= -n/2\\ln(2\\pi) - n/2\\ln[s^2] - \\frac{1}{2s^2}  ns^2  \\\\\n&= -n/2\\ln(2\\pi) - n/2\\ln[s^2] - n/2  \\\\\nT_{LR} &= -2\\Big\\{ \\ell(\\tilde{\\pmb \\theta}) - \\ell(\\hat{\\pmb \\theta}) \\Big\\} \\\\\n&= -2\\Big[ - n/2\\ln[\\tilde \\sigma^2]  + n/2\\ln[s^2] \\Big] \\\\\n&= 2 \\Big[n/2\\ln[\\tilde \\sigma^2]  - n/2\\ln[s^2] \\Big] \\\\\n&= 2\\Big[n/2 \\Big(\\ln[\\tilde \\sigma^2]  - \\ln[s^2] \\Big)  \\Big] \\\\\n&= n \\ln \\Big(\\frac{\\tilde \\sigma^2}{s^2} \\Big) \\\\\n&= n \\ln \\Bigg(\\frac{s^2 + (\\bar Y - \\mu_0)^2}{s^2} \\Bigg) \\\\\n&= n \\ln \\Bigg(1 + \\frac{(\\bar Y - \\mu_0)^2}{s^2} \\Bigg)\n\\end{split}\n$$\n\n\n## Power function and consistency of tests\n\nThe power function of a test with rejection region $R$ based on a sample of size $n$ is given by \n$$\\beta_n(\\pmb \\theta) = P_{\\pmb \\theta}(\\pmb Y \\in R)$$\nIf $\\pmb \\theta \\notin \\Theta_0$, $\\beta_n(\\pmb \\theta)$ is the probability of detecting the alternative: $\\beta_n(\\pmb \\theta) =1 - \\beta(R)$.  \n\nIf $\\pmb \\theta \\in \\Theta_0$, $\\beta_n(\\pmb \\theta)$ is the Type I error probability: $\\beta_n(\\pmb \\theta) = \\alpha(R)$.  \n\nA sequence of tests is called consistent against a specific alternative $\\pmb \\theta_1$ if $\\beta_n(\\pmb \\theta_1) \\xrightarrow{n \\rightarrow \\infty} 1$. \n\nFor example, for a test of $H_0: \\theta \\le \\theta_0$ versus $H_a: \\theta > \\theta_0$, say we use a test statistic $T_n$ with rejection region \n$$\n\\frac{\\sqrt n(T_n - \\theta_0)}{\\sigma(\\theta_0)} > z_{\\alpha}\n$$\nThen, the test is consistent against all alternatives $\\theta_1 > \\theta_0$ if, for all $\\theta$,\n$$\n\\begin{split}\n\\sqrt{n}(T_n - \\theta) &\\xrightarrow D N\\Big(0, \\sigma^2(\\theta)\\Big) \\text{ or, equivalently,} \\\\\n\\frac{\\sqrt{n}(T_n - \\theta)}{\\sigma(\\theta)} &\\xrightarrow D N(0, 1) \n\\end{split}\n$$\n\n### Power example: $N(\\mu, \\sigma^2)$ with $\\sigma^2$ known\n\nRecall the example in which we obtained the test statistics for $Y_i \\sim N(\\mu, \\sigma^2)$ with $\\sigma^2$ known and a simple null hypothesis $H_0: \\mu = \\mu_0$. We obtained\n\n$$\n\\begin{split}\nT_W = T_S = T_{LR} &= \\frac{n(\\bar Y -\\mu_0)^2}{\\sigma^2} \\\\\n\\text{We reject } H_0 \\text{ if } T_W &> \\chi^2_{1,\\alpha}\\\\\nZ_W = Z_S = Z_{LR} &= \\frac{\\sqrt n(\\bar Y -\\mu_0)}{\\sigma} \\\\\n& = \\frac{\\bar Y - \\mu_0}{\\sigma/\\sqrt{n}}\\\\\n\\text{We reject } H_0 \\text{ if } Z_W &> z_{\\alpha}\\\\\n\\end{split}\n$$\n\n\n\\newpage\n\nThe corresponding power function is \n\n$$\n\\begin{split}\n\\beta_n(\\mu) &= P_\\mu (\\pmb Y \\in R) \\\\\n&= P_\\mu\\Bigg(\\frac{\\bar Y - \\mu_0}{\\sigma/\\sqrt n} >  z_{\\alpha}\\Bigg) \\\\\n&= P_\\mu\\Bigg(\\frac{\\bar Y - \\mu + \\mu - \\mu_0}{\\sigma/\\sqrt n} >  z_{\\alpha}\\Bigg) \\\\\n&= P_\\mu\\Bigg(\\frac{\\bar Y - \\mu}{\\sigma/\\sqrt n} >  z_{\\alpha} - \\frac{\\mu - \\mu_0}{\\sigma/\\sqrt n}\\Bigg) \\\\\n\\text{Recall that } E[\\bar Y] &= \\mu; ~\\text{Var}[\\bar Y] = \\sigma^2/n; ~\\text{SD}[\\bar Y] =\\sigma/\\sqrt n. \\text{ Thus}, \\\\\n\\beta_n(\\mu) &= P_\\mu \\Bigg(Z > z_{\\alpha} - \\frac{\\mu - \\mu_0}{\\sigma/\\sqrt n} \\Bigg)  \\\\\n&= 1 - P_\\mu \\Bigg(Z \\le z_{\\alpha} - \\frac{\\mu - \\mu_0}{\\sigma/\\sqrt n} \\Bigg) \\\\\n&= 1 - \\Phi\\Bigg( z_{\\alpha} - \\frac{\\mu - \\mu_0}{\\sigma/\\sqrt n}\\Bigg)\\\\\n\\text{Recall that } 1-\\Phi(x) &= \\Phi(-x). \\text{ Thus, } \\\\\n\\beta_n(\\mu) &= \\Phi\\Bigg(\\frac{\\mu - \\mu_0}{\\sigma/\\sqrt n} - z_{\\alpha} \\Bigg) \\\\\n\\end{split}\n$$\n\n\\newpage\n\nSuppose we require a 5% level test with a minimum power of 80% to reject $H_0: \\mu = \\mu_0$ in favor of $H_a: \\mu > \\mu_0$ if $\\mu \\ge \\mu_0 + \\sigma$. What is the required sample size? \n\nSince $\\beta_n(\\mu_1)$ is increasing in $\\mu_1$, we need to determine *n* such that $\\beta_n(\\mu_0 + \\sigma) \\ge 0.8$.\n\n$$\n\\begin{split}\n\\beta_n(\\mu_1) &= \\Phi\\Bigg(\\frac{\\mu_1 - \\mu_0}{\\sigma/\\sqrt n} - z_{\\alpha} \\Bigg) \\\\\n\\beta_n(\\mu_0 +\\sigma) &= \\Phi\\Bigg(\\frac{\\mu_0 +\\sigma - \\mu_0}{\\sigma/\\sqrt n} - z_{\\alpha} \\Bigg) \\\\\n&= \\Phi(\\sqrt n - z_{\\alpha}) \\\\\n&\\ge 0.8 \\\\\n\\sqrt n - z_{0.05} &\\ge \\Phi^{-1}(0.8) \\\\\n\\sqrt n &\\ge \\Phi^{-1}(0.8)  + \\Phi^{-1}(0.95) \\\\\nn &\\ge \\Big(\\Phi^{-1}(0.8)  + \\Phi^{-1}(0.95)\\Big)^2 \\\\\n&\\approx 6.18\n\\end{split}\n$$\nThus, we need $n = 7$ datapoints.   \nNote, in in the TI-82 Stats calculator, type in `(invNorm(0.8) + invNorm(0.95))^2`. \n\n### Power example: $N(\\mu, \\sigma^2)$ with $\\mu$ and $\\sigma^2$ unknown (t-test)\n\nEarlier, we obtained the Wald statistic for a *t*-test \n$$\n\\begin{split}\nT_W &= \\frac{n(\\bar Y - \\mu_0)^2}{S^2} \\\\\nZ_W &= \\frac{\\sqrt n(\\bar Y - \\mu_0)}{S} \\\\\n\\end{split}\n$$ \nWe know, by the CLT, \n\n$$\n\\begin{split}\n&\\sqrt n(\\bar Y - \\mu) \\xrightarrow D N(0, \\sigma^2) \\\\\n&\\frac{\\sqrt n(\\bar Y - \\mu_0)}{\\sigma}  \\xrightarrow D N(0, 1) \\\\\n&\\text{We also know } S_n \\text{ is a } \\text{consistent estimator of } \\sigma: \\\\\n&S_n \\xrightarrow P \\sigma \\text{ for } n \\rightarrow \\infty \\\\\n&\\text{Using } \\text{Slutsky's lemma, } \\\\ \n&\\frac{\\sqrt n(\\bar Y - \\mu_0)}{S} \\xrightarrow{D} N(0,1)\n\\end{split}\n$$\n\nThus, $\\displaystyle Z_W = \\frac{\\sqrt n(\\bar Y - \\mu_0)}{S} > z_{\\alpha}$ is consistent against all alternatives.\n\n### Power example: Consistency for asymptotically normal test statistics\n\nSay we have rejection region\n$$\n\\frac{\\sqrt n(T_n - \\theta_0)}{\\sigma(\\theta_0)} > z_{\\alpha}\n$$\nand we know \n$$\n\\sqrt n(T_n - \\theta) \\xrightarrow D N\\Big(0, \\sigma^2(\\theta)\\Big) \\text{ for all } \\theta\n$$\n\nThen, for some alternative $\\theta_1$, we have the power function\n\n$$\n\\begin{split}\n\\beta_n(\\theta_1) &= P_\\theta \\Bigg(\\frac{\\sqrt n(T_n - \\theta_0)}{\\sigma(\\theta_0)} > z_{\\alpha} \\Bigg)\\\\\n&= P_\\theta\\Big({\\sqrt n(T_n - \\theta_1 + \\theta_1- \\theta_0)} > z_{\\alpha}{\\sigma(\\theta_0)} \\Big)\\\\\n&= P_\\theta\\Big({\\sqrt n(T_n - \\theta_1)} > z_{\\alpha}{\\sigma(\\theta_0) - \\sqrt n( \\theta_1- \\theta_0)} \\Big)\\\\\n\\text{If } \\theta_1 &> \\theta_0, \\text{ then} \\\\\n&\\lim_{n \\rightarrow \\infty} z_{\\alpha}{\\sigma(\\theta_0) - \\sqrt n( \\theta_1- \\theta_0)} = -\\infty \\\\\n&\\text{Because }\\sqrt n(T_n - \\theta) \\xrightarrow D N\\Big(0, \\sigma^2(\\theta)\\Big) \\text{ for all } \\theta, \\\\\n&\\lim_{n \\rightarrow \\infty} P_\\theta\\Big({\\sqrt n(T_n - \\theta_1)} > z_{\\alpha}{\\sigma(\\theta_0) - \\sqrt n( \\theta_1- \\theta_0)} \\Big) = 1. \\text{ Hence, } \\\\\n\\beta_n(\\theta_1) &\\xrightarrow{n \\rightarrow \\infty} 1 \\text{ for all } \\theta_1 > \\theta_0.\n\\end{split}\n$$\n\n\n### Power example: Consistency for asymptotically normal test statistics with nuisance parameters\n\nSay we have rejection region\n$$\n\\frac{\\sqrt n (T_n - \\theta_0)}{S_n} > z_{\\alpha}\n$$\nWe know \n$$\n\\begin{split}\n\\sqrt n(T_n - \\theta) &\\xrightarrow D N\\Big(0, \\sigma^2(\\theta,\\eta) \\Big) \\text{ for all } \\theta \\text{ and } \\eta\\\\\nS^2_n &\\text{ is a consistent estimator of } \\sigma^2(\\theta,\\eta)\n\\end{split}\n$$\n\nThen, for some alternative $\\theta_1$, we have the power function\n$$\n\\begin{split}\n\\beta_n(\\theta_1) &= P\\Bigg(\\frac{\\sqrt n (T_n - \\theta_0)}{S_n} > z_{\\alpha} \\Bigg) \\\\\n&= P\\Big({\\sqrt n (T_n -\\theta_1 + \\theta_1 - \\theta_0)} > z_{\\alpha}{S_n} \\Big) \\\\\n&= P\\Big({\\sqrt n (T_n -\\theta_1)} > z_{\\alpha}{S_n} -\\sqrt n(\\theta_1 - \\theta_0) \\Big) \\\\\n\\text{If } \\theta_1 &> \\theta_0, \\text{ then} \\\\\n&\\lim_{n \\rightarrow \\infty} z_{\\alpha}{S_n} -\\sqrt n(\\theta_1 - \\theta_0) = -\\infty \\\\\n&\\text{Because } \\sqrt n(T_n - \\theta) \\xrightarrow D N\\Big(0, \\sigma^2(\\theta,\\eta) \\Big) \\text{ for all } \\theta \\text{ and } \\eta, \\\\\n&\\lim_{n \\rightarrow \\infty} P\\Big({\\sqrt n (T_n -\\theta_1)} > z_{\\alpha}{S_n} -\\sqrt n(\\theta_1 - \\theta_0) \\Big) = 1.  \\text{ Hence, } \\\\\n\\beta_n(\\theta_1) &\\xrightarrow{n \\rightarrow \\infty} 1 \\text{ for all } \\theta_1 > \\theta_0.\n\\end{split}\n$$\n\n\n## Asymptotic power approximation and sample size\n\nFor a given alternative $\\theta_1$ and sample size $n$, we can write \n$$\n\\begin{split}\n\\theta_1 &= \\theta_0 + \\frac{\\Delta}{\\sqrt n} \\\\\n\\Delta &= \\sqrt n(\\theta_1 - \\theta_0)\n\\end{split}\n$$\n\nAn approximation of the power for the alternative $\\theta_1$ is then given by\n\n$$\n\\begin{split}\n\\beta_n(\\theta_1) &\\approx \\Phi \\Bigg(\\frac{\\sqrt n(\\theta_1 - \\theta_1)}{\\sigma(\\theta_0)} - z_{\\alpha} \\Bigg) \\\\\n&= \\Phi \\Bigg(\\frac{\\Delta}{\\sigma(\\theta_0)} - z_{\\alpha} \\Bigg) \\\\\n\\end{split}\n$$\n\nAnd the minimum required sample size is given by\n\n$$\nn \\ge \\frac{(z_{\\alpha} + z_{1 -\\beta})^2}{(\\theta_1 - \\theta_0)^2}\\sigma^2(\\theta_0)\n$$\n\n### Asymptotic power approximation: Derivation and example\n\nSay we have rejection region \n$$\n\\begin{split}\n&\\frac{\\sqrt n(T_n - \\theta_0)}{\\sigma(\\theta_0)} > z_{\\alpha} \\\\\n\\beta_n(\\theta) &= P_\\theta\\Bigg(\\frac{\\sqrt n(T_n - \\theta_0)}{\\sigma(\\theta_0) } > z_{\\alpha}  \\Bigg) \\\\\n&\\text{Define a local alternative } \\theta_1 \\\\\n\\theta_1 &= \\theta_0 + \\frac{\\Delta}{\\sqrt n} \\Leftrightarrow \\Delta =\\sqrt{n}(\\theta_1 - \\theta_0) \\\\\n\\beta_n(\\theta) &= P_\\theta\\Bigg(\\frac{\\sqrt n(T_n - \\theta_1 + \\theta_1 - \\theta_0)}{\\sigma(\\theta_0) } > z_{\\alpha}  \\Bigg) \\\\\n&= P_\\theta\\Bigg(\\frac{\\sqrt n(T_n - \\theta_1)}{\\sigma(\\theta_0) } > z_{\\alpha} - \\frac{\\sqrt n(\\theta_1 - \\theta_0)}{\\sigma(\\theta_0)}  \\Bigg) \\\\\n&\\text{We know}\\\\\n&\\sqrt n(T_n - \\theta) \\xrightarrow D N\\Big(0, \\sigma^2(\\theta) \\Big) \\text{ for all } \\theta \\text{ and } \\\\\n&\\frac{\\sqrt n(T_n - \\theta)}{\\sigma(\\theta)} \\xrightarrow D N\\Big(0, 1 \\Big) \\text{ for all } \\theta \\\\\n&\\text{We can also see that that} \\\\\n\\lim_{n \\rightarrow \\infty } \\theta_1 &= \\lim_{n \\rightarrow \\infty }\\theta_0 + \\frac{\\Delta}{\\sqrt n} \\\\\n&= \\theta_0 \\\\\n&\\text{If } \\sigma^2(\\theta) \\text{ is continous, then} \\\\\n&\\sigma^2(\\theta_1) \\rightarrow \\sigma^2(\\theta_0) \\text{ when } \\theta_1 \\rightarrow \\theta_0\\\\\n&\\text{Thus, when } \\theta_1 \\xrightarrow{n \\rightarrow \\infty} \\theta_0, \\\\\n&\\frac{\\sqrt n(T_n - \\theta_1)}{\\sigma(\\theta_0) } \\xrightarrow D N(0,1) \\\\\n\\end{split}\n$$\n\nWe have obtained the asymptotic power of the test,\n$$\n\\begin{split}\n\\beta_n(\\theta_1) &\\xrightarrow{n \\rightarrow \\infty} P_\\theta\\Bigg(\\frac{\\sqrt n(T_n - \\theta_1)}{\\sigma(\\theta_0) } > z_{\\alpha} - \\frac{\\sqrt n(\\theta_1 - \\theta_0)}{\\sigma(\\theta_0)}  \\Bigg) \\\\\n&= P_\\theta\\Bigg(Z > z_{\\alpha} - \\frac{\\sqrt n(\\theta_1 - \\theta_0)}{\\sigma(\\theta_0)}  \\Bigg) \\\\\n&= 1 - P_\\theta\\Bigg(Z \\le z_{\\alpha} - \\frac{\\sqrt n(\\theta_1 - \\theta_0)}{\\sigma(\\theta_0)}   \\Bigg) \\\\\n&= 1 - \\Phi\\Bigg(z_{\\alpha} - \\frac{\\sqrt n(\\theta_1 - \\theta_0)}{\\sigma(\\theta_0)}   \\Bigg) \\\\\n&= \\Phi\\Bigg( \\frac{\\sqrt n(\\theta_1 - \\theta_0)}{\\sigma(\\theta_0)} - z_{\\alpha}   \\Bigg) \\\\\n&= \\Phi\\Bigg( \\frac{\\Delta}{\\sigma(\\theta_0)} - z_{\\alpha}   \\Bigg)\n\\end{split}\n$$\n\n### Asymptotic power approximation: Bernoulli example\n\nSuppose iid $Y_i \\sim$ Bernoulli($p$). For $T_n = \\bar Y$, we have, by the CLT,\n$$\n\\begin{split}\n\\frac{\\sqrt n(T_n - p)}{\\sqrt{p(1 - p)}} &\\xrightarrow D N(0,1)\\\\\n\\sigma^2(p) = p(1 - p) &\\text{ is continuous}\n\\end{split}\n$$\n\nThen, the approximate power for the test $H_0: p \\le p_0$ versus $H_a: p > p_0$ against a fixed alternative $p_1$ equals\n$$\n\\begin{split}\n\\beta_n(p_1) &\\approx \\Phi\\Bigg( \\frac{\\sqrt n(p_1 - p_0)}{\\sigma(p_0)} - z_{\\alpha}   \\Bigg) \\\\\n&=\\Phi\\Bigg( \\frac{\\sqrt n(p_1 - p_0)}{\\sqrt{p_0(1 - p_0)}} - z_{\\alpha}   \\Bigg) \\\\\n\\end{split}\n$$\n\n## Asymptotic equivalence \n\nIf the rejection regions of two tests correspond to $V_n > u_{\\alpha}$ and $V'_n > u_{\\alpha}$, then the tests will be asymptotically equivalent if\n$$\nP(V_n > u_{\\alpha}, V'n \\le u_{\\alpha}) +P(V_n \\le u_{\\alpha}, V'n > u_{\\alpha}) \\rightarrow 0 \\text{ when } n \\rightarrow \\infty\n$$\nThat is, as $n \\rightarrow \\infty$, the probability of the two tests giving discordant results goes to zero. ",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}