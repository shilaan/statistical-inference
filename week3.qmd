# Asymptotics of maximum likelihood estimators

## Asymptotic normality of the MLE

As $n \rightarrow \infty$, $$\sqrt{n}(\hat \theta - \theta_0) \xrightarrow{D} N\Big(0, I_1(\theta_0)^{-1}\Big)$$ 

## Asymptotic variance

If a sequence of estimators $\pmb W_n$ of $\pmb \tau(\pmb \theta)$ satisfies
$$\sqrt{n}(\pmb W_n - \pmb\tau(\pmb \theta)) \xrightarrow{D} N(\pmb 0, \pmb V(\pmb \theta)),$$

then $\pmb V(\pmb\theta)$ is called the asymptotic variance of the sequence $\pmb W_n$ in $\pmb\tau(\pmb \theta)$

$$
\text{Var}(\pmb W_n) \approx \pmb V(\pmb \theta)/n
$$
For scalar $W_n$ and $\tau$ this becomes

$$
\begin{split}
\text{if } \sqrt{n}( W_n - \tau(\pmb \theta)) \xrightarrow{D} N( 0,  V(\pmb \theta)), &\text{ then, for any } \epsilon > 0 \\
\lim_{n \rightarrow \infty} P\Bigg( \sqrt{n}\Big( W_n - \tau(\pmb \theta)\Big) \le \epsilon \Bigg) &= \Phi\Bigg(\frac{\epsilon}{\sqrt{V(\pmb \theta)}}\Bigg)
\end{split}
$$

## Asymptotic efficiency

A sequence of estimators $\pmb W_n$ is called asymptotically efficient for a parameter $\pmb \tau(\pmb \theta)$ if the asymptotic variance of $\pmb W_n$ achieves the Cramer-Rao bound. That is,

$$
\begin{split}
\sqrt{n}(\pmb W_n - \pmb\tau(\pmb \theta)) &\xrightarrow{D} N(\pmb 0, \pmb V(\pmb \theta)) \\
\text{ with } \pmb V(\pmb \theta) = \Big(\pmb \tau'&(\pmb \theta)\Big)^T \Big( \pmb I_1(\pmb\theta) \Big)^{-1} \pmb \tau'(\pmb \theta)
\end{split}
$$

Through the Delta method,
$$
\sqrt{n} \Big(\tau(\hat \theta_n) - \tau(\theta) \Big) \xrightarrow{D} N\Big(0, I_1(\theta)^{-1} [\tau'(\theta)]^2 \Big)
$$

## Asymptotic relative efficiency

Let $\tau(\pmb \theta)$ be a scalar function of $\pmb \theta$. If two sequences of estimators $W_n$ and $U_n$ satisfy
$$
\begin{split}
\sqrt{n}( W_n - \tau(\pmb \theta)) \xrightarrow{D} N( 0,  V_W(\pmb \theta)) \\
\sqrt{n}( U_n - \tau(\pmb \theta)) \xrightarrow{D} N( 0,  V_U(\pmb \theta)),
\end{split}
$$

then the asymptotic relative efficiency (ARE) of $U_n$ with respect to $W_n$ is

$$
\text{ARE}(U_n, W_n) = \frac{V_W(\pmb \theta)}{V_U(\pmb \theta)}
$$

\newpage

## Example: N($\theta, \theta$)

The MLE of $\theta$ based on a set of $n$ iid $X_i \sim N(\theta,\theta)$ measurements is given by 
$$\displaystyle \hat \theta_{\text{MLE}} = \frac{1}{2}\Big(-1 + \sqrt{1 + 4\bar Y_n}\Big), \text{ with } Y_i = X_i^2$$

Show that $\hat \theta_{\text{MLE}}$ is consistent for $\theta$. By the weak law of large numbers,
$$
\begin{split}
\bar Y_n &\xrightarrow{P} E[Y_i] \\
E[Y_i] &= E[X_i^2] \\
&= \text{Var}[X_i] + (E[X_i])^2 \\
&= \theta + \theta^2. \text{ Thus, } \\
\bar Y_n &\xrightarrow{P} \theta + \theta^2 \\ 
h(\bar Y_n) &\xrightarrow{P} h(\theta + \theta^2) \\
\frac{1}{2}\Big(-1 + \sqrt{1 + 4\bar Y_n}\Big)  &\xrightarrow{P} \frac{1}{2}\Big(-1 + \sqrt{1 + 4(\theta + \theta^2)}\Big) \\
&= \frac{1}{2}\Big(-1 + \sqrt{4\theta^2 + 4\theta + 1}\Big) \\
&= \frac{1}{2}\Bigg(-1 + \sqrt{4\Big(\theta^2 + \theta + \frac{1}{4}\Big)}\Bigg) \\
&= \frac{1}{2}\Bigg(-1 + \sqrt{4\Big(\theta + \frac{1}{2}\Big)^2}\Bigg) \\
&= \frac{1}{2}\Big(-1 + 2 \Big[\theta + \frac{1}{2}\Big]\Big) \\
&=\frac{1}{2} (-1 + 2\theta + 1)\\
&= \theta \\
\text{Thus, } \hat \theta_{\text{MLE}}  &\xrightarrow{P}  \theta
\end{split}
$$


Show that $\hat \theta_{\text{MLE}}$ is asymptotically normal. Use the following fact: 
$$
\begin{split}
\text{When } X \sim N(\mu, \sigma^2), E[X^4] &= \mu^4 + 6\mu^2\sigma^2 + 3\sigma^4. \text{ Thus, } \\
\text{When } X \sim N(\theta, \theta), E[X^4] &= \theta^4 + 6\theta^2\theta + 3\theta^2 \\
&= \theta^4 + 6\theta^3 + 3\theta^2 
\end{split}
$$

$$
\begin{split}
\text{Var}[Y_i] 
&=E[Y_i^2] - \Big(E[Y_i]\Big)^2 \\
&= E[X_i^4] - (\theta + \theta^2)^2 \\
&=\theta^4 + 6\theta^2\theta + 3\theta^2 - (\theta + \theta^2)^2\\
&= \theta^4 + 6\theta^3 + 3\theta^2 - [\theta^2 +2\theta^3 + \theta^4 ] \\
&= 4\theta^3+2\theta^2 \\
&= 2\theta^2(2\theta + 1) \\
&\text{Using the CLT, } \\
\sqrt{n}(\bar Y_n - \mu) &\xrightarrow{D} N\Big(0, 2\theta^2(2\theta + 1)\Big) \\
&\text{Using the Delta method, } \\
\sqrt{n}\Big(g(\bar Y_n) - g(\mu)\Big) &\xrightarrow{D} N\Big(0, 2\theta^2(2\theta + 1)\Big[g'(\mu) \Big]^2 \Big) \\
\text{Let } g(\mu) &=\frac{1}{2}\Big(-1 + \sqrt{1 + 4\mu}\Big). \text{ Then, } \\
g'(\mu) &=\frac{1}{2} \cdot \frac{1}{2}(1 + 4\mu)^{-1/2} \cdot 4 \\
&=(1 + 4\mu)^{-1/2} \\
&= \frac{1}{\sqrt{1 + 4\mu}} \\
&=\frac{1}{\sqrt{1 + 4 (\theta + \theta^2)}} \\
&=\frac{1}{\sqrt{4\theta^2 + 4\theta + 1}}  \\
&=\frac{1}{\sqrt{4 (\theta^2+ \theta + \frac{1}{4}) }} \\
&=\frac{1}{\sqrt{4 (\theta + \frac{1}{2})^2 }} \\
&=\frac{1}{{2 (\theta + \frac{1}{2}) }} \\
&=\frac{1}{{2\theta + 1 }} \\
[g'(\mu)]^2 &=\frac{1}{{(2\theta + 1)^2 }}. \text{ Thus, } \\
\sqrt{n}\Big(g(\bar Y_n) - g(\mu)\Big) &\xrightarrow{D} N\Bigg(0, \frac{2\theta^2(2\theta + 1)}{(2\theta + 1)^2} \Bigg)  \\
&= N\Bigg(0, \frac{2\theta^2}{2\theta + 1} \Bigg)
\end{split}
$$

Confirm the asymptotic variance by obtaining the Fisher information matrix. We have
$$
\begin{split}
X_i &\sim N(\theta,\theta)\\
f_{X_i}(x_i) &= \frac{1}{\sqrt{2\pi}} \theta^{-1/2} \exp\Bigg( - \frac{(x_i - \theta)^2}{2\theta} \Bigg) \\
\ln f_{X_i}(X_i;\theta) &= \ln\frac{1}{\sqrt{2\pi}} -\frac{1}{2}\ln\theta - \frac{1}{2}\Bigg[\frac{(X_i - \theta)^2}{\theta}\Bigg]  \\
&=\ln\frac{1}{\sqrt{2\pi}} -\frac{1}{2}\ln\theta - \frac{1}{2}\Bigg[\frac{X_i^2 - 2X_i\theta + \theta^2}{\theta}\Bigg]\\
&=\ln\frac{1}{\sqrt{2\pi}} -\frac{1}{2}\ln\theta - \frac{1}{2}\Bigg[\frac{X_i^2 }{\theta} - 2X_i+ \theta \Bigg]  \\
S_i(\theta) = \frac{\partial}{\partial \theta} \ln f_{X_i}(X_i;\theta) &= -\frac{1}{2\theta} - \frac{1}{2}\Big[ -\frac{X_i^2}{\theta^2} +1\Big] \\
&=-\frac{1}{2\theta} +  \frac{X_i^2}{2\theta^2} -\frac{1}{2}\\
I_1(\theta) &= -E\Big[\frac{\partial}{\partial \theta}S_i(\theta)\Big] \\
&= -E\Big[\frac{\partial}{\partial \theta} -\frac{1}{2}\theta^{-1} + \frac{1}{2}X_i^2 \cdot\theta^{-2} - \frac{1}{2} \Big] \\
&= -E\Big[-\frac{1}{2} \cdot-\theta^{-2} +\frac{1}{2}X_i^2 \cdot -2\theta^{-3}\Big] \\
&=-E\Big[\frac{1}{2\theta^2} - \frac{X_i^2}{\theta^3} \Big] \\
&=E\Big[ \frac{X_i^2}{\theta^3} -\frac{1}{2\theta^2} \Big] \\
&=\frac{1}{\theta^3}E[{X_i^2}] -\frac{1}{2\theta^2}  \\
&= \frac{\theta + \theta^2}{\theta^3} -\frac{1}{2\theta^2}  \\
&= \frac{1}{\theta^2} + \frac{1}{\theta} -\frac{1}{2\theta^2}  \\
&= \frac{1}{2\theta^2}  + \frac{1}{\theta} \\
&= \frac{1}{2\theta^2}  + \frac{2\theta}{\theta \cdot 2\theta} \\
&= \frac{1 + 2\theta}{2\theta^2}  \\
\text{Var}[\hat \theta_{\text{MLE}}] &\approx \frac{1}{n} \cdot \frac{2\theta^2}{1 + 2\theta} \\
\end{split}
$$


$$
\begin{split}
\text{Earlier, } &\text{we obtained} \\
\sqrt{n}(\hat \theta_{\text{MLE}} - \theta_0) &\xrightarrow{D} N\Bigg(0, \frac{2\theta^2}{1 + 2\theta}\Bigg) \\
\text{Which is } &\text{equivalent to} \\
\hat \theta_{\text{MLE}} - \theta_0 &\xrightarrow{D} \frac{1}{\sqrt n} N\Bigg(0, \frac{2\theta^2}{1 + 2\theta}\Bigg) \\
\hat \theta_{\text{MLE}} - \theta_0 &\xrightarrow{D}N\Bigg(0, \frac{1}{n} \cdot \frac{2\theta^2}{1 + 2\theta}\Bigg) \\
\hat \theta_{\text{MLE}} &\xrightarrow{D}N\Bigg(\theta_0 , \frac{1}{n} \cdot \frac{2\theta^2}{1 + 2\theta}\Bigg) \\
\end{split}
$$


\newpage

## Example: Convergence of a transformation of the Uniform

Let $U_1, U_2, ...$ be iid observations from a uniform distribution over the unit interval $[0,1]$ and define $\displaystyle Y_n = \Bigg(\prod_{i = 1}^n U_i \Bigg)^{-1/n}$. Recall that $\displaystyle \prod_{i = 1}^n x_i = \exp\Bigg(\sum_{i = 1}^n \ln x_i\Bigg)$ and show that $Y_n$ converges in probability to $e$. We have 

$$
\begin{split}
f_U(u) &= 1 \\
Y_n &= \Bigg(\prod_{i = 1}^n U_i \Bigg)^{-1/n} \\
&= e^{\Big(\displaystyle \sum_{i = 1}^n \ln U_i\Big)^{-1/n}} \\
&= e^{\Big(\displaystyle -\frac{1}{n} \sum_{i = 1}^n \ln U_i\Big)} \\
\ln U_i &\xrightarrow{P} E\Big[\ln U_i\Big] \\
E\Big[ \ln U_i\Big] &= \int_0^1 \ln U_i ~f_U(u) ~du \\
 &= \int_0^1 \ln U_i  ~du\\
 &= u\ln(u) - u \Bigg|_0^1 \\
 &=\ln(1) - 1 \\
 &=-1 \\
\ln U_i &\xrightarrow{P} -1 \\
\text{By continuous} &\text{ mapping, } \\
h(\ln U_i) &\xrightarrow{P} h(-1) \\
\exp{\Big(\displaystyle -\frac{1}{n} \sum_{i = 1}^n \ln U_i\Big)} &\xrightarrow{P} \exp{\Big(\displaystyle -\frac{1}{n} \sum_{i = 1}^n -1\Big)} \\
&= e^{n/n} \\
&= e
\end{split}
$$

## Example: Average relative efficiency

Suppose iid $X_i \sim \text{Poisson}(\lambda)$ and we are interested in estimating $P(X = 0) = e^{-\lambda}$. Derive two estimators; (1) based on the Poisson process and (2) by setting $Y_i = I(X_i = 0)$, in which case we have iid $Y_i \sim \text{Bernoulli}(e^{-\lambda})$. Compare their efficiencies. We have

$$
\begin{split}
E[X_i] &= \text{Var}[X_i]  =\lambda \\
\text{By } &\text{WLLN, }  \\ 
\bar X_n &\xrightarrow{P} \lambda\\
\text{By  } &\text{continuous mapping, } \\
h(\bar X_n) &\xrightarrow{P} h(\lambda) \\
e^{-\bar X_n} &\xrightarrow{P} e^{- \lambda} \\
\text{By } &\text{CLT}, \\
\sqrt n(\bar X_n - \lambda) &\xrightarrow{D} N(0,\lambda)\\
\text{By } &\text{Delta method}, \\
\sqrt n\Big(h(\bar X_n) - h(\lambda)\Big) &\xrightarrow{D} N(0,\lambda\Big[h'(\lambda)\Big]^2) \\
\text{Let } h(\lambda) &= e^{-\lambda}. \text{Then, } \\
h'(\lambda) &= -e^{-\lambda} \\
[h'(\lambda)\Big]^2 &= e^{-2\lambda}. \text{Thus, } \\
\sqrt n\Big(h(\bar X_n) - h(\lambda)\Big) &\xrightarrow{D} N(0,\lambda e^{-2\lambda}) \\
\end{split}
$$

If, instead, we set $Y_i = I(X_i = 0)$, then we have

$$
\begin{split} 
\text{iid } Y_i &\sim \text{Bernoulli}(e^{-\lambda}) \\
E[Y_i] &= e^{-\lambda} \\
\text{Var}[Y_i] &= e^{-\lambda}(1-e^{-\lambda}) \\
\text{By the} &\text{ WLLN,} \\
\bar Y_n &\xrightarrow{P} e^{-\lambda} \\
\text{By the} &\text{ CLT}, \\
\sqrt n(\bar Y_n - e^{-\lambda}) &\xrightarrow{D} N\Big(0,e^{-\lambda}(1-e^{-\lambda})\Big)\\
\end{split}
$$
The average relative efficiency of $\bar Y_n$ vs. $e^{-\bar X}$ is given by
$$
\begin{split}
\text{ARE}(\bar Y, e^{-\bar X}) &= \frac{\lambda e^{-2\lambda}}{e^{-\lambda}(1-e^{-\lambda})} \\
&= \frac{\lambda e^{-\lambda}}{1 - e^{-\lambda}}
\end{split}
$$

This is a decreasing function. Thus, the Poisson estimator $e^{-\bar X_n}$ is more efficient than the Bernoulli estimator $\bar Y_n$. 
